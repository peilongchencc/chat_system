# LangChain

æœ¬é¡¹ç›®åŸºäºLangChainå®˜ç½‘ç¿»è¯‘ã€æ·»åŠ è‡ªå·±çš„ç†è§£ï¼Œä¸»è¦ç”¨äºä¸ªäººç ”ç©¶ã€å­¦ä¹ ã€‚<br>

- [LangChain](#langchain)
  - [Quickstart(å¿«é€Ÿå…¥é—¨):](#quickstartå¿«é€Ÿå…¥é—¨)
    - [Setup(å®‰è£…):](#setupå®‰è£…)
      - [Jupyter Notebook:](#jupyter-notebook)
      - [Installation:](#installation)
    - [LangSmith:](#langsmith)
    - [Building with LangChain(ä½¿ç”¨LangChainæ„å»ºåº”ç”¨):](#building-with-langchainä½¿ç”¨langchainæ„å»ºåº”ç”¨)
    - [LLM Chain:](#llm-chain)
      - [OpenAI:](#openai)
      - [Local:](#local)
    - [æ‹“å±•-pythonæ“ä½œç¬¦| çš„ç”¨æ³•:](#æ‹“å±•-pythonæ“ä½œç¬¦-çš„ç”¨æ³•)
    - [LangChainä½¿ç”¨ | æ“ä½œç¬¦çš„åŸç†:](#langchainä½¿ç”¨--æ“ä½œç¬¦çš„åŸç†)
    - [Chain Debug:](#chain-debug)
    - [Retrieval Chain(æ£€ç´¢é“¾--æŸ¥æ‰¾å¹¶æå–çš„è¿‡ç¨‹):](#retrieval-chainæ£€ç´¢é“¾--æŸ¥æ‰¾å¹¶æå–çš„è¿‡ç¨‹)
      - [ä½•æ—¶ä½¿ç”¨æ£€ç´¢é“¾:](#ä½•æ—¶ä½¿ç”¨æ£€ç´¢é“¾)
      - [æ£€ç´¢é“¾å®Œæ•´ä»£ç :](#æ£€ç´¢é“¾å®Œæ•´ä»£ç )


## Quickstart(å¿«é€Ÿå…¥é—¨):

In this quickstart we'll show you how to:<br>

åœ¨è¿™ä¸ªå¿«é€Ÿå…¥é—¨ä¸­ï¼Œæˆ‘ä»¬å°†å‘ä½ å±•ç¤ºå¦‚ä½•ï¼š<br>

- Get setup with LangChain, LangSmith and LangServe(å¼€å§‹å®‰è£…å’Œé…ç½® LangChainã€LangSmith å’Œ LangServe)

- Use the most basic and common components(ç»„ä»¶) of LangChain: prompt templates, models, and output parsers(ä½¿ç”¨ LangChain æœ€åŸºç¡€å’Œå¸¸è§çš„ç»„ä»¶ï¼šæç¤ºæ¨¡æ¿ã€æ¨¡å‹å’Œè¾“å‡ºè§£æå™¨)

- Use LangChain Expression Language(æ¨æµ‹åº”è¯¥æŒ‡çš„æ˜¯langchainè¯­è¨€è§„åˆ™), the protocol(åè®®) that LangChain is built on and which facilitates(ä¿ƒè¿›ï¼›ä½¿å˜å¾—æ›´ç®€æ˜“) component chaining(é“¾æ¥ï¼›è¿æ¥)(ä½¿ç”¨LangChainè¡¨è¾¾å¼è¯­è¨€ï¼Œè¿™æ˜¯LangChainæ„å»ºçš„åè®®ï¼Œæœ‰åŠ©äºç»„ä»¶é“¾æ¥)

- Build a simple application with LangChain(ä½¿ç”¨ LangChain æ„å»ºä¸€ä¸ªç®€å•çš„åº”ç”¨ç¨‹åº)

- Trace your application with LangSmith(ç”¨ LangSmith è·Ÿè¸ªä½ çš„åº”ç”¨ç¨‹åº)

- Serve your application with LangServe(ä½¿ç”¨ LangServe ä¸ºä½ çš„åº”ç”¨ç¨‹åºæä¾›æœåŠ¡ï¼ˆæ”¯æ’‘ï¼‰ï¼Œæˆ–è€…ç¿»è¯‘ä¸º "åˆ©ç”¨ LangServe éƒ¨ç½²å¹¶è¿è¡Œä½ çš„åº”ç”¨ç¨‹åºã€‚")

That's a fair amount to cover! Let's dive in.<br>

å†…å®¹ç›¸å½“å¤šï¼è®©æˆ‘ä»¬å¼€å§‹å§ã€‚<br>

### Setup(å®‰è£…):

#### Jupyter Notebook:

This guide (and most of the other guides in the documentation) use Jupyter notebooks and assume(å‡è®¾) the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because often times things can go wrong (unexpected output, API down, etc) and going through guides in an interactive(äº¤äº’çš„) environment is a great way to better understand them.<br>

æœ¬æŒ‡å—ï¼ˆä»¥åŠæ–‡æ¡£ä¸­çš„å¤§å¤šæ•°å…¶ä»–æŒ‡å—ï¼‰éƒ½ä½¿ç”¨ Jupyter ç¬”è®°æœ¬ï¼Œå¹¶å‡è®¾è¯»è€…ä¹Ÿåœ¨ä½¿ç”¨ã€‚Jupyter ç¬”è®°æœ¬éå¸¸é€‚åˆå­¦ä¹ å¦‚ä½•ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç³»ç»Ÿï¼Œå› ä¸ºåœ¨æ“ä½œè¿‡ç¨‹ä¸­ç»å¸¸ä¼šå‡ºç°é—®é¢˜ï¼ˆæ„å¤–è¾“å‡ºã€API æ•…éšœç­‰ï¼‰ï¼Œè€Œåœ¨äº¤äº’å¼ç¯å¢ƒä¸­æµè§ˆæŒ‡å—æ˜¯æ›´å¥½åœ°ç†è§£è¿™äº›é—®é¢˜çš„ç»ä½³æ–¹å¼ã€‚<br>

> "åœ¨äº¤äº’å¼ç¯å¢ƒä¸­æµè§ˆæŒ‡å—æ˜¯æ›´å¥½åœ°ç†è§£è¿™äº›é—®é¢˜çš„ç»ä½³æ–¹å¼" åº”è¯¥æŒ‡çš„æ˜¯äº¤äº’å¼ç¯å¢ƒèƒ½æ›´å¥½è§‚å¯Ÿè¾“å‡ºã€‚æ¯•ç«ŸIDEèƒ½DEBUGçš„ä¼˜åŠ¿å¤ªå¤§hhhã€‚

#### Installation:

åˆ©ç”¨condaåˆ›å»ºè™šæ‹Ÿç¯å¢ƒ:<br>

```bash
conda create --name langchain python=3.10.11
```

æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ:<br>

```bash
conda activate langchain
```

å®‰è£…`langchain`:<br>

```bash
pip install langchain
# or
conda install langchain -c conda-forge
```

> ç¬”è€…ä½¿ç”¨çš„æ˜¯condaå®‰è£…æ–¹å¼ã€‚

### LangSmith:

Many of the applications you build with LangChain will contain multiple steps with multiple invocations(è°ƒç”¨) of LLM calls(è¿™é‡Œåº”è¯¥æŒ‡çš„æ˜¯è°ƒç”¨å¤§æ¨¡å‹çš„æ¥å£). As these applications get more and more complex, it becomes crucial(è‡³å…³é‡è¦çš„) to be able to inspect(æ£€æŸ¥ï¼›ä»”ç»†è§‚å¯Ÿ) what exactly is going on inside your chain(é“¾) or agent(ä»£ç†). The best way to do this is with LangSmith.<br>

ä½¿ç”¨LangChainæ„å»ºåº”ç”¨ç¨‹åºåŒ…å«å¤šä¸ªæ­¥éª¤ï¼Œæ¶‰åŠå¤šæ¬¡è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å£ã€‚éšç€è¿™äº›åº”ç”¨ç¨‹åºå˜å¾—è¶Šæ¥è¶Šå¤æ‚ï¼Œèƒ½å¤Ÿæ£€æŸ¥é“¾æ¡æˆ–ä»£ç†ä¸­ç©¶ç«Ÿå‘ç”Ÿäº†ä»€ä¹ˆå˜å¾—è‡³å…³é‡è¦ã€‚åšè¿™äº‹æœ€ä½³çš„æ–¹æ³•å°±æ˜¯ä½¿ç”¨LangSmithã€‚<br>

> multiple invocations(è°ƒç”¨) of è¡¨ç¤ºå¤šæ¬¡è°ƒç”¨ã€‚

Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:<br>

è¯·æ³¨æ„ï¼Œè™½ç„¶LangSmithå¹¶éå¿…éœ€ï¼Œä½†å®ƒéå¸¸æœ‰å¸®åŠ©ã€‚å¦‚æœæ‚¨æƒ³ä½¿ç”¨LangSmithï¼Œåœ¨é€šè¿‡ä¸Šè¿°é“¾æ¥æ³¨å†Œåï¼Œç¡®ä¿è®¾ç½®æ‚¨çš„ç¯å¢ƒå˜é‡ä»¥å¼€å§‹è®°å½•è¿½è¸ªæ—¥å¿—ï¼š<br>

```bash
export LANGCHAIN_TRACING_V2="true"
export LANGCHAIN_API_KEY="..."
```

### Building with LangChain(ä½¿ç”¨LangChainæ„å»ºåº”ç”¨):

LangChain enables building application that connect external(å¤–éƒ¨çš„) sources of data and computation(è®¡ç®—ï¼›è®¡ç®—è¿‡ç¨‹) to LLMs. <br>

LangChainæ”¯æŒæ„å»ºè¿æ¥å¤–éƒ¨æ•°æ®æºå’Œè®¡ç®—èµ„æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨ã€‚<br>

In this quickstart, we will walk through(ä¸€æ­¥ä¸€æ­¥åœ°å±•ç¤ºæˆ–è§£é‡Š) a few different ways of doing that. We will start with a simple LLM chain, which just relies on(ä¾èµ–) information in the prompt template to respond. <br>

åœ¨è¿™ä¸ªå¿«é€Ÿå…¥é—¨ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»å‡ ç§ä¸åŒçš„å®ç°æ–¹å¼ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä»ä¸€ä¸ªç®€å•çš„LLMé“¾å¼€å§‹ï¼Œå®ƒä»…ä¾èµ–äºæç¤ºæ¨¡æ¿ä¸­çš„ä¿¡æ¯æ¥è¿›è¡Œå›åº”ã€‚<br>

Next, we will build a retrieval(æ£€ç´¢) chain, which fetches data from a separate(ç‹¬ç«‹çš„) database and passes that into the prompt template. We will then add in chat history, to create a conversation retrieval chain(å¯¹è¯æ£€ç´¢é“¾). <br>

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªæ£€ç´¢é“¾ï¼Œå®ƒä¼šä»ç‹¬ç«‹çš„æ•°æ®åº“ä¸­è·å–æ•°æ®ï¼Œå¹¶å°†è¿™äº›æ•°æ®ä¼ é€’ç»™æç¤ºæ¨¡æ¿ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†æ·»åŠ èŠå¤©å†å²è®°å½•ï¼Œä»¥åˆ›å»ºä¸€ä¸ªå¯¹è¯æ£€ç´¢é“¾ã€‚<br>

This allows you interact(äº¤äº’) in a chat manner with this LLM, **so it remembers previous questions**. <br>

è¿™ä½¿æ‚¨èƒ½å¤Ÿä»¥èŠå¤©æ–¹å¼ä¸è¿™ä¸ªLLMäº¤äº’ï¼Œ**å› æ­¤å®ƒèƒ½è®°ä½ä¹‹å‰çš„é—®é¢˜**ã€‚<br>

Finally, we will build an agent - which utilizes(åˆ©ç”¨;ä½¿ç”¨;è¿ç”¨) an LLM to determine whether or not it needs to fetch data to answer questions. <br>

æœ€åï¼Œæˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªä»£ç† - å®ƒåˆ©ç”¨LLMæ¥åˆ¤æ–­æ˜¯å¦éœ€è¦è·å–æ•°æ®æ¥å›ç­”é—®é¢˜ã€‚<br>

We will cover these at a high level, but there are lot of details to all of these! We will link to relevant docs.<br>

> æ¨æµ‹å¤§æ¦‚æ˜¯æŒ‡ç«™åœ¨é«˜å¤„æ›´å®¹æ˜“çœ‹æ¸…äº‹ç‰©æœ¬è´¨ã€‚

æˆ‘ä»¬å°†ä»é«˜å±‚æ¬¡ä¸Šä»‹ç»è¿™äº›å†…å®¹ï¼Œä½†è¿™äº›éƒ½æœ‰å¾ˆå¤šç»†èŠ‚ï¼æˆ‘ä»¬å°†é“¾æ¥åˆ°ç›¸å…³æ–‡æ¡£ã€‚<br>

### LLM Chain:

For this getting started guide, we will provide two options: using OpenAI (a popular model available via API) or using a local open source model.<br>

å¯¹äºè¿™ä¸ªå…¥é—¨æŒ‡å—ï¼Œæˆ‘ä»¬å°†æä¾›ä¸¤ç§é€‰æ‹©ï¼šä½¿ç”¨ OpenAIï¼ˆä¸€ç§å¯ä»¥é€šè¿‡ API è°ƒç”¨çš„æµè¡Œæ¨¡å‹ï¼‰æˆ–ä½¿ç”¨æœ¬åœ°å¼€æºæ¨¡å‹ã€‚<br>

#### OpenAI:

First we'll need to import the LangChain x OpenAI integration(é›†æˆ) package.<br>

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å¯¼å…¥ LangChain ä¸ OpenAI çš„é›†æˆåŒ…ã€‚<br>

```bash
pip install langchain-openai
```

Accessing the API requires an API key, which you can get by creating an account(è´¦æˆ·) and heading(å‰å¾€) [here](https://platform.openai.com/api-keys). Once we have a key we'll want to set it as an environment variable by running:<br>

è¦è®¿é—® APIï¼Œä½ éœ€è¦ä¸€ä¸ª API å¯†é’¥ï¼Œå¯ä»¥é€šè¿‡åˆ›å»ºä¸€ä¸ªè´¦æˆ·å¹¶å‰å¾€è¿™é‡Œæ¥è·å–ã€‚ä¸€æ—¦æˆ‘ä»¬æ‹¥æœ‰äº†å¯†é’¥ï¼Œæˆ‘ä»¬ä¼šæƒ³è¦é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤å°†å…¶è®¾ç½®ä¸ºä¸€ä¸ªç¯å¢ƒå˜é‡ï¼š<br>

```bash
export OPENAI_API_KEY="your-api-key-here"
```

We can then initialize the model:<br>

ç„¶åæˆ‘ä»¬å¯ä»¥åˆå§‹åŒ–æ¨¡å‹ï¼š<br>

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()
```

If you'd prefer not to set an environment variable you can pass the key in directly(ç›´æ¥åœ°) via the `openai_api_key` named parameter when initiating the OpenAI LLM class:<br>

å¦‚æœä½ ä¸æƒ³è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œä½ å¯ä»¥åœ¨åˆå§‹åŒ– OpenAI LLM ç±»æ—¶ï¼Œç›´æ¥é€šè¿‡ openai_api_key è¿™ä¸ªå‘½åå‚æ•°ä¼ å…¥å¯†é’¥ï¼š<br>

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(openai_api_key="...")
```

Once you've installed and initialized the LLM of your choice, we can try using it!<br>

ä¸€æ—¦ä½ å®‰è£…å¹¶åˆå§‹åŒ–äº†ä½ é€‰æ‹©çš„ LLMï¼Œæˆ‘ä»¬å°±å¯ä»¥å°è¯•ä½¿ç”¨å®ƒäº†ï¼<br>

Let's ask it what LangSmith is - this is something that wasn't present in the training data so it shouldn't have a very good response.<br>

è®©æˆ‘ä»¬é—®é—®å®ƒ LangSmith æ˜¯ä»€ä¹ˆ - è¿™æ˜¯è®­ç»ƒæ•°æ®ä¸­æ²¡æœ‰çš„å†…å®¹ï¼Œæ‰€ä»¥å®ƒåº”è¯¥ä¸ä¼šæœ‰å¾ˆå¥½çš„å›ç­”ã€‚<br>

```python
llm.invoke("how can langsmith help with testing?")
```

> åœ¨ç¼–ç¨‹é¢†åŸŸï¼Œ"invoke a function" è¡¨ç¤º "è°ƒç”¨ä¸€ä¸ªå‡½æ•°"ï¼Œæ‰€ä»¥ä¸Šè¿°ä»£ç ä¸­ "invoke" çš„æ„æ€å¤§æ¦‚ç‡æ˜¯ "è°ƒç”¨"ã€‚

ç¬”è€…åœ¨IDEä¸­æµ‹è¯•çš„æ•ˆæœå¦‚ä¸‹:<br>

> å¿…é¡»ä¿è¯ç»ˆç«¯èƒ½å¤Ÿè¿æ¥openaiæœåŠ¡ï¼Œæ‰å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç ã€‚
> ç»æµ‹è¯•ï¼Œè¿è¡Œ`llm.invoke("...")`åç»ˆç«¯ä»¥éæµå¼è¾“å‡ºå½¢å¼è¿”å›ã€‚

```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv_path = '.env.local'
load_dotenv(dotenv_path=dotenv_path)

llm = ChatOpenAI(openai_api_key=os.getenv("OPENAI_API_KEY"))

llm_response = llm.invoke("ä½ å¥½")

print(llm_response)     # content='ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ'
print(type(llm_response))   # <class 'langchain_core.messages.ai.AIMessage'>
print(llm_response.content) # ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ
```

We can also guide(æŒ‡å¯¼ï¼›å¼•å¯¼) it's response with a prompt template(æ¨¡æ¿). Prompt templates are used to convert(è½¬æ¢) raw user input to a better input to the LLM.<br>

æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨æç¤ºæ¨¡æ¿æ¥å¼•å¯¼å…¶å›åº”ã€‚æç¤ºæ¨¡æ¿ç”¨äºå°†åŸå§‹ç”¨æˆ·è¾“å…¥è½¬æ¢ä¸ºæ›´é€‚åˆè¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å…¥ã€‚

```python
from langchain_core.prompts import ChatPromptTemplate
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are world class technical documentation writer."),  # è‹±æ–‡çš„æ„æ€æ˜¯: ä½ æ˜¯ä¸–ç•Œçº§çš„æŠ€æœ¯æ–‡æ¡£æ’°å†™ä¸“å®¶(å†™æ‰‹)ã€‚
    ("user", "{input}")
])
```

We can now combine these into a simple LLM chain:<br>

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™äº›ç»„åˆæˆä¸€ä¸ªç®€å•çš„LLMé“¾ï¼š<br>

```python
chain = prompt | llm 
```

We can now invoke(è°ƒç”¨) it and ask the same question. It still won't know the answer, but it should respond(å›åº”) in a more proper tone for a technical writer!<br>

ç°åœ¨æˆ‘ä»¬å¯ä»¥è°ƒç”¨å®ƒå¹¶è¯¢é—®ç›¸åŒçš„é—®é¢˜ã€‚å®ƒä»ç„¶ä¸ä¼šçŸ¥é“ç­”æ¡ˆï¼Œä½†åº”è¯¥ä»¥æ›´é€‚åˆæŠ€æœ¯å†™æ‰‹çš„è¯­æ°”å›åº”ï¼<br>

```python
chain.invoke({"input": "how can langsmith help with testing?"})
```

ç¬”è€…æ‰€ç”¨çš„å®Œæ•´ä»£ç å¦‚ä¸‹:<br>

```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv_path = '.env.local'
load_dotenv(dotenv_path=dotenv_path)

llm = ChatOpenAI(openai_api_key=os.getenv("OPENAI_API_KEY"))

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are world class technical documentation writer."),
    ("user", "{input}")
])

chain = prompt | llm 

chain_response = chain.invoke({"input": "how can langsmith help with testing?"})

print(chain_response.content)
```

The output of a ChatModel (and therefore, of this chain) is a message. However, it's often much more convenient to work with strings. Let's add a simple output parser to convert the chat message to a string.<br>

> "and therefore, of this chain" è¿™ä¸ªçŸ­è¯­çš„æ„æ€æ˜¯ "å› æ­¤ï¼Œå¯¹äºè¿™ä¸ªé“¾ä¹Ÿæ˜¯å¦‚æ­¤"ã€‚æä¸æ‡‚ "å› æ­¤ï¼Œå¯¹äºè¿™ä¸ªé“¾ä¹Ÿæ˜¯å¦‚æ­¤" åœ¨è¿™å¥è¯ä¸­çš„å«ä¹‰ï¼Œä½†ä¸å¦¨ç¢æ•´å¥è¯çš„ç†è§£ã€‚

ChatModelï¼ˆå› æ­¤ï¼Œå¯¹äºè¿™ä¸ªé“¾ä¹Ÿæ˜¯å¦‚æ­¤ï¼‰çš„è¾“å‡ºæ˜¯ä¸€æ¡æ¶ˆæ¯ã€‚ç„¶è€Œï¼Œä½¿ç”¨å­—ç¬¦ä¸²é€šå¸¸æ›´æ–¹ä¾¿ã€‚è®©æˆ‘ä»¬æ·»åŠ ä¸€ä¸ªç®€å•çš„è¾“å‡ºè§£æå™¨ï¼Œå°†èŠå¤©æ¶ˆæ¯è½¬æ¢ä¸ºå­—ç¬¦ä¸²ã€‚<br>

```python
from langchain_core.output_parsers import StrOutputParser

output_parser = StrOutputParser()
```

We can now add this to the previous(ä¹‹å‰çš„) chain:<br>

ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªæ·»åŠ åˆ°ä¹‹å‰çš„é“¾ä¸­ï¼š<br>

```python
chain = prompt | llm | output_parser
```

We can now invoke(è°ƒç”¨) it and ask the same question. The answer will now be a string (rather than a ChatMessage).<br>

ç°åœ¨æˆ‘ä»¬å¯ä»¥è°ƒç”¨å®ƒå¹¶æå‡ºåŒæ ·çš„é—®é¢˜ã€‚ç°åœ¨çš„ç­”æ¡ˆå°†æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ˆè€Œä¸æ˜¯ChatMessageï¼‰ã€‚<br>

```python
chain.invoke({"input": "how can langsmith help with testing?"})
```

ç¬”è€…æ‰€ç”¨å®Œæ•´ä»£ç :<br>

```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv_path = '.env.local'
load_dotenv(dotenv_path=dotenv_path)

llm = ChatOpenAI(openai_api_key=os.getenv("OPENAI_API_KEY"))

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are world class technical documentation writer."),
    ("user", "{input}")
])

output_parser = StrOutputParser()

chain = prompt | llm | output_parser

chain_response = chain.invoke({"input": "how can langsmith help with testing?"})

print(chain_response)
print(type(chain_response))     # <class 'str'>
```

è¿™æ¬¡æ·»åŠ äº†ç»“æœè§£æå™¨ï¼Œè¾“å‡ºçš„ç›´æ¥æ˜¯ `string` ç±»å‹çš„ç»“æœï¼Œå…¶å®å’Œä¹‹å‰ç¬”è€…ä½¿ç”¨çš„ `print(chain_response.content)` æ•ˆæœç›¸åŒã€‚<br>

#### Local:

Ollama allows you to run open-source large language models, such as Llama 2, locally.<br>

Ollama ä½¿ä½ èƒ½å¤Ÿåœ¨æœ¬åœ°è¿è¡Œå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ¯”å¦‚ Llama 2ã€‚<br>

First, follow these instructions(æŒ‡ç¤º) to set up and run a local Ollama instance(å®ä¾‹):<br>

é¦–å…ˆï¼ŒæŒ‰ç…§ä»¥ä¸‹æŒ‡ç¤ºæ¥è®¾ç½®å¹¶è¿è¡Œä¸€ä¸ªæœ¬åœ°çš„ Ollama å®ä¾‹ï¼š<br>

- Download
- Fetch a model via `ollama pull llama2`(é€šè¿‡ `ollama pull llama2` å‘½ä»¤è·å–æ¨¡å‹)

Then, make sure the Ollama server is running. After that, you can do:<br>

ç„¶åï¼Œç¡®ä¿ Ollama æœåŠ¡å™¨æ­£åœ¨è¿è¡Œã€‚ä¹‹åï¼Œä½ å¯ä»¥è¿›è¡Œï¼š<br>


### æ‹“å±•-pythonæ“ä½œç¬¦| çš„ç”¨æ³•:

Python ä¸­çš„ `|` æ“ä½œç¬¦ä¸»è¦æœ‰ä¸¤ç§ç”¨é€”ï¼Œå…·ä½“ç”¨æ³•å–å†³äºå®ƒè¢«ç”¨åœ¨ä»€ä¹ˆä¸Šä¸‹æ–‡ä¸­ï¼š<br>

1. **æŒ‰ä½æˆ–ï¼ˆBitwise ORï¼‰**ï¼šå½“ `|` ç”¨äºæ•´æ•°æ—¶ï¼Œå®ƒæ‰§è¡ŒæŒ‰ä½æˆ–æ“ä½œã€‚è¿™æ„å‘³ç€å¯¹ä¸¤ä¸ªæ•°çš„äºŒè¿›åˆ¶è¡¨ç¤ºè¿›è¡Œæ“ä½œï¼Œåªè¦å¯¹åº”ä½ä¸­è‡³å°‘æœ‰ä¸€ä¸ªä¸º1ï¼Œåˆ™ç»“æœçš„è¯¥ä½ä¹Ÿä¸º1ã€‚

ä¾‹å¦‚ï¼š<br>

```python
a = 0b1010  # äºŒè¿›åˆ¶è¡¨ç¤ºï¼Œç­‰äºåè¿›åˆ¶ä¸­çš„10
b = 0b0101  # äºŒè¿›åˆ¶è¡¨ç¤ºï¼Œç­‰äºåè¿›åˆ¶ä¸­çš„5
result = a | b  # ç»“æœå°†æ˜¯0b1111ï¼Œå³åè¿›åˆ¶ä¸­çš„15
```

2. **é›†åˆçš„å¹¶é›†ï¼ˆSet Unionï¼‰**ï¼šå½“ `|` ç”¨åœ¨ä¸¤ä¸ªé›†åˆä¸Šæ—¶ï¼Œå®ƒè¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰åœ¨ä¸¤ä¸ªé›†åˆä¸­çš„å…ƒç´ çš„æ–°é›†åˆï¼Œç›¸å½“äºä¸¤ä¸ªé›†åˆçš„å¹¶é›†ã€‚

ä¾‹å¦‚ï¼š<br>

```python
set1 = {1, 2, 3}
set2 = {3, 4, 5}
result = set1 | set2  # ç»“æœå°†æ˜¯{1, 2, 3, 4, 5}
```

ä» Python 3.9 å¼€å§‹ï¼Œ`|` è¿˜å¯ä»¥ç”¨äºå­—å…¸ï¼Œè¡¨ç¤ºåˆå¹¶ä¸¤ä¸ªå­—å…¸ï¼š<br>

3. **å­—å…¸çš„åˆå¹¶ï¼ˆDictionary Mergeï¼‰**ï¼šä½¿ç”¨ `|` æ“ä½œç¬¦å¯ä»¥åˆå¹¶ä¸¤ä¸ªå­—å…¸ï¼Œå¦‚æœä¸¤ä¸ªå­—å…¸æœ‰ç›¸åŒçš„é”®ï¼Œåˆ™ç¬¬äºŒä¸ªå­—å…¸ä¸­çš„é”®å€¼å¯¹ä¼šè¦†ç›–ç¬¬ä¸€ä¸ªå­—å…¸ä¸­çš„é”®å€¼å¯¹ã€‚

ä¾‹å¦‚ï¼š<br>

```python
dict1 = {'a': 1, 'b': 2}
dict2 = {'b': 3, 'c': 4}
result = dict1 | dict2  # ç»“æœå°†æ˜¯{'a': 1, 'b': 3, 'c': 4}
```

è¿™äº›æ˜¯ `|` æ“ä½œç¬¦åœ¨ Python ä¸­çš„ä¸»è¦ç”¨é€”ã€‚å®ƒçš„ç¡®åˆ‡è¡Œä¸ºå–å†³äºæ“ä½œæ•°çš„ç±»å‹ã€‚<br>

### LangChainä½¿ç”¨ | æ“ä½œç¬¦çš„åŸç†:

`LangChain` ä½¿ç”¨ `|` æ“ä½œç¬¦çš„èƒ½åŠ›ä¸æ˜¯ Python è¯­è¨€å†…å»ºçš„åŠŸèƒ½ï¼Œè€Œæ˜¯é€šè¿‡è‡ªå®šä¹‰ç±»å’Œé‡è½½è¿ç®—ç¬¦å®ç°çš„ã€‚Python å…è®¸ç±»é€šè¿‡å®šä¹‰ç‰¹æ®Šæ–¹æ³•ï¼ˆä¹Ÿç§°ä¸ºé­”æœ¯æ–¹æ³•æˆ–åŒä¸‹åˆ’çº¿æ–¹æ³•ï¼‰æ¥é‡è½½ï¼ˆoverrideï¼‰æˆ–å®ç°ç‰¹å®šçš„è¿ç®—ç¬¦è¡Œä¸ºã€‚å¯¹äº `|` æ“ä½œç¬¦ï¼Œç›¸å…³çš„é­”æœ¯æ–¹æ³•æ˜¯ `__or__`ã€‚<br>

> ç»DebugéªŒè¯ï¼ŒLangChainä½¿ç”¨çš„ç¡®å®æ˜¯ `__or__`æ–¹æ³•ã€‚

å½“ä½  `LangChain` ä½¿ç”¨ `|` æ“ä½œç¬¦ä»¥éæ ‡å‡†æ–¹å¼ï¼ˆä¾‹å¦‚ï¼Œä¸æ˜¯æŒ‰ä½æˆ–ã€é›†åˆå¹¶é›†æˆ–å­—å…¸åˆå¹¶ï¼‰ï¼Œè¿™é€šå¸¸æ„å‘³ç€è¯¥ç±»å®šä¹‰äº†ä¸€ä¸ª `__or__` æ–¹æ³•æ¥è‡ªå®šä¹‰ `|` æ“ä½œç¬¦çš„è¡Œä¸ºã€‚<br>

è¿™ç§æŠ€æœ¯å…è®¸å¼€å‘è€…åˆ›å»ºæ›´å…·è¡¨è¾¾æ€§å’Œé¢†åŸŸç‰¹å®šçš„è¯­æ³•ï¼Œæé«˜ä»£ç çš„å¯è¯»æ€§å’Œæ˜“ç”¨æ€§ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªæ•°æ®å¤„ç†åº“å¯èƒ½å…è®¸ä½ å°†æ•°æ®è½¬æ¢æˆ–è¿‡æ»¤æ“ä½œä»¥é“¾å¼æ–¹å¼è¿æ¥èµ·æ¥ï¼Œä½¿ç”¨ `|` æ“ä½œç¬¦æ¥è¡¨ç¤ºè¿™ç§è¿æ¥ï¼Œä½¿å¾—ä»£ç æ›´åŠ ç›´è§‚ã€‚<br>

è¿™é‡Œæ˜¯ä¸€ä¸ªç®€åŒ–çš„ä¾‹å­ï¼Œæ¼”ç¤ºå¦‚ä½•ä¸ºä¸€ä¸ªç±»é‡è½½ `|` æ“ä½œç¬¦ï¼š<br>

```python
class CustomClass:
    def __init__(self, value):
        self.value = value

    # å®šä¹‰ | æ“ä½œç¬¦çš„è¡Œä¸º
    def __or__(self, other):
        # è¿™é‡Œå¯ä»¥å®šä¹‰ä»»ä½•è‡ªå®šä¹‰é€»è¾‘
        # ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°åˆå¹¶ä¸¤ä¸ªå¯¹è±¡çš„å€¼
        return CustomClass(self.value + other.value)

# ä½¿ç”¨è‡ªå®šä¹‰çš„ | æ“ä½œç¬¦
a = CustomClass(5)
b = CustomClass(10)
result = a | b
print(result.value)  # è¾“å‡ºï¼š15
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ`CustomClass` çš„å®ä¾‹ `a` å’Œ `b` èƒ½å¤Ÿä½¿ç”¨ `|` æ“ä½œç¬¦ï¼Œå…¶è¡Œä¸ºæ˜¯å°†ä¸¤ä¸ªå®ä¾‹çš„ `value` å±æ€§å€¼ç›¸åŠ ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨ `CustomClass` ä¸­å®šä¹‰äº† `__or__` æ–¹æ³•æ¥å®ç°è¿™ä¸€è¡Œä¸ºã€‚<br>

æœ¬è´¨ä¸Šæ‰§è¡Œçš„æ˜¯ `a | b` æ‰§è¡Œçš„æ˜¯ `a.__or__(b)` ï¼ŒæŠŠ `b` ä½œä¸ºäº† `other` ã€‚<br>

å…·ä½“é€»è¾‘å›¾å¦‚ä¸‹:<br>

![](./materials/operator_logic.jpg)

å½“è¿è¡Œåˆ° `chain = prompt | llm | output_parser` æ—¶ï¼ŒDebugå‘ç°ï¼Œå…¶å®æ˜¯è·³è½¬åˆ°äº†LangChainä¸­ `Runnable` ç±»çš„ `__or__` æ–¹æ³•ã€‚<br>

```python
class Runnable(Generic[Input, Output], ABC):
  
  # çœç•¥å…¶ä»–å‡½æ•°

  def __or__(
      self,
      other: Union[
          Runnable[Any, Other],
          Callable[[Any], Other],
          Callable[[Iterator[Any]], Iterator[Other]],
          Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]],
      ],
  ) -> RunnableSerializable[Input, Other]:
      """Compose this runnable with another object to create a RunnableSequence.
      è¯‘: å°†æ­¤ runnable ä¸å¦ä¸€ä¸ªå¯¹è±¡ç»„åˆï¼Œä»¥åˆ›å»ºä¸€ä¸ª RunnableSequenceã€‚
      """
      return RunnableSequence(self, coerce_to_runnable(other))
```

```python
def coerce_to_runnable(thing: RunnableLike) -> Runnable[Input, Output]:
    """Coerce(å¼ºåˆ¶;å¼ºè¿«) a runnable-like object into a Runnable.

    Args:
        thing: A runnable-like object.

    Returns:
        A Runnable.
    """
    if isinstance(thing, Runnable):
        return thing
    elif inspect.isasyncgenfunction(thing) or inspect.isgeneratorfunction(thing):
        return RunnableGenerator(thing)
    elif callable(thing):
        return RunnableLambda(cast(Callable[[Input], Output], thing))
    elif isinstance(thing, dict):
        return cast(Runnable[Input, Output], RunnableParallel(thing))
    else:
        raise TypeError(
            f"Expected a Runnable, callable or dict."
            f"Instead got an unsupported type: {type(thing)}"
        )
```


```python
class RunnableSequence(RunnableSerializable[Input, Output]):
    """A sequence of runnables, where the output of each is the input of the next.

    RunnableSequence is the most important composition operator in LangChain as it is
    used in virtually every chain.

    A RunnableSequence can be instantiated directly or more commonly by using the `|`
    operator where either the left or right operands (or both) must be a Runnable.

    Any RunnableSequence automatically supports sync, async, batch.

    The default implementations of `batch` and `abatch` utilize threadpools and
    asyncio gather and will be faster than naive invocation of invoke or ainvoke
    for IO bound runnables.

    Batching is implemented by invoking the batch method on each component of the
    RunnableSequence in order.

    A RunnableSequence preserves the streaming properties of its components, so if all
    components of the sequence implement a `transform` method -- which
    is the method that implements the logic to map a streaming input to a streaming
    output -- then the sequence will be able to stream input to output!

    If any component of the sequence does not implement transform then the
    streaming will only begin after this component is run. If there are
    multiple blocking components, streaming begins after the last one.

    Please note: RunnableLambdas do not support `transform` by default! So if
        you need to use a RunnableLambdas be careful about where you place them in a
        RunnableSequence (if you need to use the .stream()/.astream() methods).

        If you need arbitrary logic and need streaming, you can subclass
        Runnable, and implement `transform` for whatever logic you need.

    Here is a simple example that uses simple functions to illustrate the use of
    RunnableSequence:

        .. code-block:: python

            from langchain_core.runnables import RunnableLambda

            def add_one(x: int) -> int:
                return x + 1

            def mul_two(x: int) -> int:
                return x * 2

            runnable_1 = RunnableLambda(add_one)
            runnable_2 = RunnableLambda(mul_two)
            sequence = runnable_1 | runnable_2
            # Or equivalently:
            # sequence = RunnableSequence(first=runnable_1, last=runnable_2)
            sequence.invoke(1)
            await sequence.ainvoke(1)

            sequence.batch([1, 2, 3])
            await sequence.abatch([1, 2, 3])

    Here's an example that uses streams JSON output generated by an LLM:

        .. code-block:: python

            from langchain_core.output_parsers.json import SimpleJsonOutputParser
            from langchain_openai import ChatOpenAI

            prompt = PromptTemplate.from_template(
                'In JSON format, give me a list of {topic} and their '
                'corresponding names in French, Spanish and in a '
                'Cat Language.'
            )

            model = ChatOpenAI()
            chain = prompt | model | SimpleJsonOutputParser()

            async for chunk in chain.astream({'topic': 'colors'}):
                print('-')
                print(chunk, sep='', flush=True)
    """

    # The steps are broken into first, middle and last, solely for type checking
    # purposes. It allows specifying the `Input` on the first type, the `Output` of
    # the last type.
    first: Runnable[Input, Any]
    """The first runnable in the sequence."""
    middle: List[Runnable[Any, Any]] = Field(default_factory=list)
    """The middle runnables in the sequence."""
    last: Runnable[Any, Output]
    """The last runnable in the sequence."""

    def __init__(
        self,
        *steps: RunnableLike,
        name: Optional[str] = None,
        first: Optional[Runnable[Any, Any]] = None,
        middle: Optional[List[Runnable[Any, Any]]] = None,
        last: Optional[Runnable[Any, Any]] = None,
    ) -> None:
        """Create a new RunnableSequence.

        Args:
            steps: The steps to include in the sequence.
        """
        steps_flat: List[Runnable] = []
        if not steps:
            if first is not None and last is not None:
                steps_flat = [first] + (middle or []) + [last]
        for step in steps:
            if isinstance(step, RunnableSequence):
                steps_flat.extend(step.steps)
            else:
                steps_flat.append(coerce_to_runnable(step))
        if len(steps_flat) < 2:
            raise ValueError(
                f"RunnableSequence must have at least 2 steps, got {len(steps_flat)}"
            )
        super().__init__(
            first=steps_flat[0],
            middle=list(steps_flat[1:-1]),
            last=steps_flat[-1],
            name=name,
        )
```

### Chain Debug:

![](./materials/chain_debug.jpg)

### Retrieval Chain(æ£€ç´¢é“¾--æŸ¥æ‰¾å¹¶æå–çš„è¿‡ç¨‹):

In order to properly(æ°å½“çš„) answer the original(åŸå§‹çš„) question ("how can langsmith help with testing?"), we need to provide additional(é¢å¤–çš„) context to the LLM.<br>

ä¸ºäº†æ°å½“å›ç­”åŸå§‹é—®é¢˜ï¼ˆ"langsmith å¦‚ä½•å¸®åŠ©è¿›è¡Œæµ‹è¯•ï¼Ÿ"ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦å‘ LLM æä¾›é¢å¤–çš„ä¸Šä¸‹æ–‡ã€‚<br>

 We can do this via retrieval. Retrieval is useful when you have too much data to pass to the LLM directly(ç›´æ¥åœ°)(too..to... å¤ª...è€Œä¸èƒ½...). You can then use a retriever to fetch only the most relevant pieces(æœ€ç›¸å…³çš„ç‰‡æ®µ) and pass those in.<br>

è¿™å¯ä»¥é€šè¿‡æ£€ç´¢æ¥å®ç°ã€‚å½“ä½ æœ‰å¤ªå¤šæ•°æ®ä¸èƒ½ç›´æ¥ä¼ é€’ç»™ LLM æ—¶ï¼Œæ£€ç´¢éå¸¸æœ‰ç”¨ã€‚ä½ å¯ä»¥ä½¿ç”¨æ£€ç´¢å™¨åªè·å–æœ€ç›¸å…³çš„æ•°æ®ç‰‡æ®µç„¶åä¼ å…¥ã€‚<br>

![](./materials/retrieval_resolve.jpg)

In this process, we will look up(æŸ¥è¯¢) relevant documents from a Retriever and then pass them into the prompt. A Retriever can be backed(å¯ä»¥ä¾æ®...) by anything - a SQL table, the internet, etc - but in this instance(å®ä¾‹) we will populate(ç”Ÿæ´»äºï¼›å¡«å……) a vector store and use that as a retriever. For more information on vectorstores, see this [documentation](https://python.langchain.com/docs/modules/data_connection/vectorstores).<br>

åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä»æ£€ç´¢å™¨ä¸­æŸ¥æ‰¾ç›¸å…³æ–‡æ¡£ï¼Œç„¶åå°†å®ƒä»¬ä¼ å…¥æç¤ºä¸­ã€‚æ£€ç´¢å™¨å¯ä»¥ç”±ä»»ä½•ä¸œè¥¿æ”¯æŒâ€”â€”ä¸€ä¸ª SQL è¡¨ï¼Œäº’è”ç½‘ç­‰â€”â€”ä½†åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†å¡«å……ä¸€ä¸ªå‘é‡å­˜å‚¨å¹¶ä½¿ç”¨å®ƒä½œä¸ºæ£€ç´¢å™¨ã€‚æœ‰å…³å‘é‡å­˜å‚¨çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤æ–‡æ¡£ã€‚<br>

First, we need to load(åŠ è½½) the data that we want to index(æ£€ç´¢). In order to do this, we will use the `WebBaseLoader`. This requires installing `BeautifulSoup`:<br>

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦åŠ è½½æˆ‘ä»¬æƒ³è¦ç´¢å¼•çš„æ•°æ®ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `WebBaseLoader` ã€‚è¿™éœ€è¦å®‰è£… `BeautifulSoup`ï¼š<br>

```bash
pip install beautifulsoup4
```

After that, we can import and use WebBaseLoader.<br>

ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥å¯¼å…¥å¹¶ä½¿ç”¨ WebBaseLoaderã€‚<br>

```python
from langchain_community.document_loaders import WebBaseLoader
loader = WebBaseLoader("https://docs.smith.langchain.com/overview")

docs = loader.load()
```

ä»£ç è§£é‡Š:<br>

ä¸Šè¿°ä»£ç ä½¿ç”¨äº†`langchain_community.document_loaders`æ¨¡å—ä¸­çš„`WebBaseLoader`ç±»ï¼Œç›®çš„æ˜¯ä»æŒ‡å®šçš„URLåŠ è½½æ–‡æ¡£ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒåˆ›å»ºäº†ä¸€ä¸ªæŒ‡å‘URL "https://docs.smith.langchain.com/overview" çš„`WebBaseLoader`å®ä¾‹ã€‚ç„¶åï¼Œè°ƒç”¨æ­¤å®ä¾‹ä¸Šçš„`load`æ–¹æ³•æ¥ä»æŒ‡å®šçš„URLè·å–å’ŒåŠ è½½å†…å®¹ã€‚<br>

ä»¥ä¸‹æ˜¯`WebBaseLoader`å·¥ä½œæ–¹å¼åŠå…¶åŠŸèƒ½çš„è¯¦ç»†è§£é‡Šï¼Œè¿™äº›ä¿¡æ¯æ ¹æ®LangChainæ–‡æ¡£æä¾›ï¼š<br>

- **åˆå§‹åŒ–**ï¼š`WebBaseLoader`åœ¨åˆå§‹åŒ–æ—¶å¯ä»¥è®¾ç½®å¤šä¸ªå‚æ•°ï¼Œå¦‚`web_path`ï¼ˆè¦ä»ä¸­åŠ è½½çš„URLsï¼‰ã€`header_template`ï¼ˆè¯·æ±‚çš„HTTPå¤´ï¼‰å’Œ`verify_ssl`ï¼ˆæ˜¯å¦éªŒè¯SSLè¯ä¹¦ï¼‰ç­‰ã€‚æ­¤ç±»æ—¨åœ¨ä½¿ç”¨`urllib`åŠ è½½HTMLé¡µé¢ï¼Œå¹¶ç”¨`BeautifulSoup`è§£æï¼Œ`BeautifulSoup`æ˜¯ä¸€ä¸ªç”¨äºä»HTMLå’ŒXMLæ–‡ä»¶ä¸­æå–æ•°æ®çš„åº“ã€7â€ æ¥æºã€‘ã€‚

- **åŠ è½½æ–¹æ³•**ï¼š`load`æ–¹æ³•ç‰¹åˆ«ç”¨äºä»åˆå§‹åŒ–æ—¶æŒ‡å®šçš„URL(s)è·å–å’Œè§£æHTMLå†…å®¹ã€‚å®ƒä½¿ç”¨`BeautifulSoup`è¿›è¡Œè§£æï¼Œå¦‚æœæä¾›äº†å¤šä¸ªURLsï¼Œä¹Ÿå¯ä»¥å¤„ç†ã€‚åŠ è½½å™¨æ”¯æŒå¼‚æ­¥æ“ä½œï¼Œè¯·æ±‚çš„é€Ÿç‡é™åˆ¶ï¼Œä»¥åŠé€šè¿‡BeautifulSoupè¿›è¡Œè‡ªå®šä¹‰è§£æé€‰é¡¹ã€‚

- **åŠŸèƒ½**ï¼šè¯¥ç±»é™¤äº†`load`æ–¹æ³•å¤–ï¼Œè¿˜æä¾›äº†å¦‚`aload`ï¼ˆå¼‚æ­¥åŠ è½½ï¼‰ã€`fetch_all`ï¼ˆå¹¶å‘å¸¦é€Ÿç‡é™åˆ¶åœ°è·å–å¤šä¸ªURLsï¼‰ã€`lazy_load`ï¼ˆæŒ‰éœ€åŠ è½½å†…å®¹ï¼‰ä»¥åŠ`scrape`æˆ–`scrape_all`ï¼ˆç”¨BeautifulSoupè·å–å’Œè§£æå†…å®¹ï¼‰ç­‰å¤šç§æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•å…è®¸åœ¨çµæ´»çš„ç½‘é¡µæŠ“å–å’Œå†…å®¹å¤„ç†åœºæ™¯ä¸­ä½¿ç”¨ã€‚

è¿™ä¸ªåŠ è½½å™¨æ˜¯LangChainæ–‡æ¡£åŠ è½½åŸºç¡€è®¾æ–½çš„ä¸€éƒ¨åˆ†ï¼Œæ—¨åœ¨ä¿ƒè¿›ç½‘é¡µå†…å®¹çš„æ£€ç´¢å’Œå¤„ç†ï¼Œä»¥ä¾›ä¸‹æ¸¸åº”ç”¨ä½¿ç”¨ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€æ•°æ®åˆ†ææˆ–è¾“å…¥åˆ°æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­ã€‚<br>

Next, we need to index it into a vectorstore. This requires a few components, namely an [embedding model](https://python.langchain.com/docs/modules/data_connection/text_embedding) and [a vectorstore](https://python.langchain.com/docs/modules/data_connection/vectorstores).<br>

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶ç´¢å¼•åˆ°ä¸€ä¸ªå‘é‡å­˜å‚¨ä¸­ã€‚è¿™éœ€è¦å‡ ä¸ªç»„ä»¶ï¼Œå³ä¸€ä¸ªåµŒå…¥æ¨¡å‹å’Œä¸€ä¸ªå‘é‡å­˜å‚¨ã€‚<br>

For embedding models, we once again provide examples for accessing via OpenAI or via local models.<br>

å¯¹äºåµŒå…¥æ¨¡å‹ï¼Œæˆ‘ä»¬å†æ¬¡æä¾›äº†é€šè¿‡ OpenAI æˆ–é€šè¿‡æœ¬åœ°æ¨¡å‹è®¿é—®çš„ä¾‹å­ã€‚<br>

Make sure you have the `langchain_openai` package installed an the appropriate environment variables set (these are the same as needed for the LLM).<br>

è¯·ç¡®ä¿ä½ å·²å®‰è£… `langchain_openai` åŒ…ï¼Œå¹¶è®¾ç½®äº†é€‚å½“çš„ç¯å¢ƒå˜é‡ï¼ˆè¿™äº›ä¸ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰€éœ€çš„ç¯å¢ƒå˜é‡ç›¸åŒï¼‰ã€‚<br>

```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()
```

Now, we can use this embedding model to ingest(æ‘„å…¥ï¼›å¯¼å…¥) documents into a vectorstore. We will use a simple local vectorstore, [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss), for simplicity's sake.<br>

> sake é€šå¸¸ç”¨äºè¡¨è¾¾åšæŸäº‹çš„åŸå› æˆ–ç›®çš„ã€‚ä¾‹å¦‚ï¼Œ"for simplicity's sake" æ„å‘³ç€ä¸ºäº†ç®€å•èµ·è§ï¼›"for the sake of clarity" æ„å‘³ç€ä¸ºäº†æ¸…æ™°èµ·è§ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªåµŒå…¥æ¨¡å‹å°†æ–‡æ¡£æ‘„å…¥åˆ°ä¸€ä¸ªå‘é‡å­˜å‚¨ä¸­ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªç®€å•çš„æœ¬åœ°å‘é‡å­˜å‚¨ï¼ŒFAISSã€‚<br>

First we need to install the required packages for that:<br>

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…æ‰€éœ€çš„åŒ…ï¼š<br>

```bash
pip install faiss-cpu
```

> faisså‘é‡åº“ç»è¿‡ `pip install faiss-cpu` åå°±å¯ä»¥ç”¨ï¼Œä¸éœ€è¦é¢å¤–å®‰è£…ã€‚

Then we can build our index:<br>

ç„¶åæˆ‘ä»¬å¯ä»¥æ„å»ºæˆ‘ä»¬çš„ç´¢å¼•ï¼š<br>

```python
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter

# "RecursiveCharacterTextSplitter"å¯ä»¥æ‹†è§£ä¸ºå‡ ä¸ªéƒ¨åˆ†æ¥ç†è§£ï¼š

# Recursiveï¼ˆé€’å½’çš„ï¼‰ã€Characterï¼ˆå­—ç¬¦ï¼‰ã€Textï¼ˆæ–‡æœ¬ï¼‰ã€Splitterï¼ˆåˆ†å‰²å™¨ï¼‰

# "RecursiveCharacterTextSplitter"å¤§è‡´æ„å‘³é€’å½’åœ°å°†æ–‡æœ¬æŒ‰å­—ç¬¦åˆ†å‰²ã€‚ä»ä¸€ä¸ªè¾ƒå¤§çš„æ–‡æœ¬å—å¼€å§‹ï¼Œå¹¶é€’å½’åœ°å°†å…¶åˆ†è§£ä¸ºå•ä¸ªå­—ç¬¦ï¼Œæ¯æ¬¡è¿­ä»£å¤„ç†æ›´å°çš„æ–‡æœ¬ç‰‡æ®µï¼Œç›´åˆ°æ•´ä¸ªæ–‡æœ¬è¢«ç»†åˆ†ä¸ºå•ç‹¬çš„å­—ç¬¦ã€‚

text_splitter = RecursiveCharacterTextSplitter()
documents = text_splitter.split_documents(docs)
vector = FAISS.from_documents(documents, embeddings)
```

ç‚¹å‡» `langchain_community.vectorstores` åå†…å®¹è¯¦è§£:<br>

**Vector store** stores embedded data and performs vector search.ï¼ˆ**å‘é‡å­˜å‚¨**ç”¨äºå­˜å‚¨åµŒå…¥æ•°æ®å¹¶æ‰§è¡Œå‘é‡æœç´¢ã€‚ï¼‰<br>

One of the most common(å¸¸è§çš„) ways to store and search over unstructured(éç»“æ„åŒ–) data is to embed it and store the resulting embedding vectors, and then query the store and retrieve(æ£€ç´¢) the data that are 'most similar'(æœ€ç›¸ä¼¼) to the embedded query.<br>

å­˜å‚¨å’Œæœç´¢éç»“æ„åŒ–æ•°æ®çš„æœ€å¸¸è§æ–¹æ³•ä¹‹ä¸€æ˜¯å°†å…¶å‘é‡åŒ–ï¼Œå­˜å‚¨è½¬åŒ–åçš„è¯å‘é‡ï¼Œç„¶åæŸ¥è¯¢å¹¶æ£€ç´¢ä¸åµŒå…¥æŸ¥è¯¢â€œæœ€ç›¸ä¼¼â€çš„æ•°æ®ã€‚<br>

**Class hierarchy(ç±»å±‚æ¬¡ç»“æ„):**

.. code-block(ä»£ç å—)::<br>

```python
VectorStore --> <name>  # Examples: Annoy, FAISS, Milvus

BaseRetriever --> VectorStoreRetriever --> <name>Retriever  # Example: VespaRetriever
```

**Main helpers(ä¸»è¦åŠ©æ‰‹):**<br>

.. code-block(ä»£ç å—)::<br>

```python
Embeddings, Document
```

Now that we have this data indexed(ç´¢å¼•åˆ°) in a vectorstore, we will create a retrieval chain. <br>

æ—¢ç„¶æˆ‘ä»¬å·²ç»å°†è¿™äº›æ•°æ®ç´¢å¼•åˆ°å‘é‡å­˜å‚¨ä¸­ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªæ£€ç´¢é“¾ã€‚<br>

This chain will take an incoming question, look up relevant documents, then pass those documents along with the original question into an LLM and ask it to answer the original question.<br>

è¯¥é“¾å°†æ¥æ”¶ä¸€ä¸ªè¾“å…¥çš„é—®é¢˜ï¼ŒæŸ¥æ‰¾ç›¸å…³æ–‡æ¡£ï¼Œç„¶åå°†è¿™äº›æ–‡æ¡£è¿åŒåŸå§‹é—®é¢˜ä¼ é€’ç»™ä¸€ä¸ªLLMï¼Œå¹¶è¦æ±‚å…¶å›ç­”åŸå§‹é—®é¢˜ã€‚<br>

First, let's set up the chain that takes a question and the retrieved documents and generates an answer.<br>

é¦–å…ˆï¼Œè®©æˆ‘ä»¬è®¾ç½®ä¸€ä¸ªé“¾ï¼Œè¯¥é“¾æ¥æ”¶ä¸€ä¸ªé—®é¢˜å’Œæ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªç­”æ¡ˆã€‚<br>

```python
from langchain.chains.combine_documents import create_stuff_documents_chain

prompt = ChatPromptTemplate.from_template("""Answer the following question based only on the provided context:

<context>
{context}
</context>

Question: {input}""")

document_chain = create_stuff_documents_chain(llm, prompt)
```

If we wanted to, we could run this ourselves by passing in documents directly:<br>

å¦‚æœæˆ‘ä»¬æ„¿æ„çš„è¯ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥é€šè¿‡ä¼ å…¥æ–‡æ¡£æ¥è‡ªè¡Œè¿è¡Œè¿™ä¸ªè¿‡ç¨‹ã€‚<br>

```python
from langchain_core.documents import Document

document_chain.invoke({
    "input": "how can langsmith help with testing?",
    "context": [Document(page_content="langsmith can let you visualize test results")]
})
```

However, we want the documents to first come from the retriever we just set up. That way, for a given question we can use the retriever to dynamically(åŠ¨æ€çš„) select the most relevant documents and pass those in.<br>

ç„¶è€Œï¼Œæˆ‘ä»¬å¸Œæœ›æ–‡æ¡£é¦–å…ˆæ¥è‡ªæˆ‘ä»¬åˆšåˆšè®¾ç½®çš„æ£€ç´¢å™¨ã€‚è¿™æ ·ï¼Œå¯¹äºç»™å®šçš„é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ£€ç´¢å™¨åŠ¨æ€åœ°é€‰æ‹©æœ€ç›¸å…³çš„æ–‡æ¡£å¹¶ä¼ é€’ç»™æ¨¡å‹ã€‚<br>

```python
from langchain.chains import create_retrieval_chain

retriever = vector.as_retriever()
retrieval_chain = create_retrieval_chain(retriever, document_chain)
```

We can now invoke this chain. This returns a dictionary - the response from the LLM is in the answer key.<br>

æˆ‘ä»¬ç°åœ¨å¯ä»¥è°ƒç”¨è¿™ä¸ªé“¾æ¡äº†ã€‚è¿™å°†è¿”å›ä¸€ä¸ªå­—å…¸ - LLMçš„å“åº”ä½äº `answer` é”®ä¸­ã€‚<br>

```python
response = retrieval_chain.invoke({"input": "how can langsmith help with testing?"})
print(response["answer"])

# LangSmith offers several features that can help with testing:...
```

This answer should be much more accurate!<br>

è¿™ä¸ªç­”æ¡ˆåº”è¯¥æ›´å‡†ç¡®ï¼<br>

#### ä½•æ—¶ä½¿ç”¨æ£€ç´¢é“¾:

LangChain æ”¯æŒæ£€ç´¢é“¾çš„åŠŸèƒ½æ˜¯ä¸ºäº†åœ¨å¤„ç†é—®é¢˜æ—¶æä¾›æ›´åŠ ä¸°å¯Œå’Œå‡†ç¡®çš„ä¿¡æ¯æ¥æºã€‚æ£€ç´¢é“¾å…è®¸ç³»ç»Ÿé€šè¿‡æ£€ç´¢å¤–éƒ¨æ•°æ®æºæ¥å¢å¼ºå…¶å›ç­”çš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚ä½ å†³å®šä½•æ—¶ä½¿ç”¨æ£€ç´¢é“¾è€Œä¸æ˜¯ç›´æ¥å›å¤ï¼Œå¯ä»¥åŸºäºä»¥ä¸‹å‡ ä¸ªè€ƒè™‘å› ç´ ï¼š<br>

1. **é—®é¢˜çš„å¤æ‚æ€§å’Œç‰¹å®šæ€§**ï¼šå¦‚æœé—®é¢˜éå¸¸å¤æ‚æˆ–éœ€è¦ç‰¹å®šé¢†åŸŸçš„æ·±å…¥çŸ¥è¯†ï¼Œä½¿ç”¨æ£€ç´¢é“¾å¯èƒ½ä¼šæ›´å¥½ï¼Œå› ä¸ºå®ƒå¯ä»¥è®¿é—®æœ€æ–°çš„æˆ–æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚

2. **ä¿¡æ¯çš„æ—¶æ•ˆæ€§**ï¼šå¯¹äºéœ€è¦æœ€æ–°ä¿¡æ¯çš„é—®é¢˜ï¼ˆå¦‚æ–°é—»äº‹ä»¶ã€æœ€è¿‘çš„ç§‘å­¦å‘ç°ç­‰ï¼‰ï¼Œä½¿ç”¨æ£€ç´¢é“¾å¯ä»¥å¸®åŠ©è·å–æœ€æ–°æ•°æ®ã€‚

3. **å¯ç”¨æ•°æ®çš„é™åˆ¶**ï¼šå¦‚æœé—®é¢˜æ¶‰åŠåˆ°çš„ä¿¡æ¯å¯èƒ½ä¸åŒ…å«åœ¨æ¨¡å‹è®­ç»ƒæ•°æ®ä¸­ï¼Œæˆ–è€…ä¿¡æ¯å·²ç»è¿‡æ—¶ï¼Œé‚£ä¹ˆä½¿ç”¨æ£€ç´¢é“¾å¯ä»¥è®¿é—®æœ€æ–°å’Œæœ€ç›¸å…³çš„å¤–éƒ¨èµ„æºã€‚

4. **ç²¾ç¡®åº¦å’Œå¯é æ€§çš„è¦æ±‚**ï¼šå¯¹äºéœ€è¦é«˜åº¦å‡†ç¡®å’Œå¯é ç­”æ¡ˆçš„æƒ…å†µï¼ˆæ¯”å¦‚åŒ»ç–—ã€æ³•å¾‹å’¨è¯¢ç­‰ï¼‰ï¼Œæ£€ç´¢é“¾å¯ä»¥é€šè¿‡è®¿é—®æƒå¨æ•°æ®æºæ¥æä¾›æ”¯æŒã€‚

5. **ç”¨æˆ·çš„åå¥½**ï¼šæœ‰æ—¶å€™ï¼Œç”¨æˆ·å¯èƒ½æ›´å€¾å‘äºè·å–ç»è¿‡éªŒè¯çš„ä¿¡æ¯æºæä¾›çš„ç­”æ¡ˆï¼Œè€Œä¸æ˜¯ç›´æ¥ä»æ¨¡å‹ç”Ÿæˆçš„å›ç­”ã€‚

6. **ç”¨æˆ·æ˜¯å¦ä¸Šä¼ è¶…å¤§å‹æ–‡ä»¶**: å¦‚æœç”¨æˆ·ä¸Šä¼ è¶…å¤§å‹æ–‡ä»¶ï¼Œå¯ä»¥é‡‡ç”¨å…ˆå¯¹è¯¥æ–‡ä»¶åˆ‡åˆ†ã€å‘é‡åŒ–å¹¶å­˜å‚¨åˆ°å‘é‡åº“æ“ä½œï¼Œåç»­è°ƒç”¨æ£€ç´¢é“¾æ£€ç´¢ã€‚

åœ¨å†³å®šæ˜¯å¦ä½¿ç”¨æ£€ç´¢é“¾æ—¶ï¼Œè¿˜è¦è€ƒè™‘æ£€ç´¢è¿‡ç¨‹å¯èƒ½å¢åŠ çš„æ—¶é—´å»¶è¿Ÿå’Œèµ„æºæ¶ˆè€—ã€‚ç›´æ¥å›å¤é€šå¸¸æ›´å¿«ï¼Œä½†å¯èƒ½ä¸å¦‚æ£€ç´¢é“¾å›å¤çš„ä¿¡æ¯å…¨é¢æˆ–å‡†ç¡®ã€‚<br>

ğŸš€ğŸš€ğŸš€ **æœ€å¥½çš„æƒ…å†µæ˜¯ï¼Œæ™®é€šçŠ¶æ€ä¸‹è®©å¤§æ¨¡å‹ç›´æ¥å›å¤ï¼Œç”¨æˆ·ä¸Šä¼ è¶…å¤§å‹æ–‡ä»¶(å¯ä»¥é€šè¿‡è®¡ç®—tokenåˆ¤æ–­)åé‡‡ç”¨æ£€ç´¢é“¾ã€‚** <br>

æ€»çš„æ¥è¯´ï¼Œå½“é¢å¯¹éœ€è¦é«˜åº¦ä¸“ä¸šçŸ¥è¯†ã€æœ€æ–°ä¿¡æ¯ã€æˆ–è€…å¯¹ç­”æ¡ˆå‡†ç¡®æ€§å’Œå¯é æ€§æœ‰é«˜è¦æ±‚çš„é—®é¢˜æ—¶ï¼Œä½¿ç”¨æ£€ç´¢é“¾ä¼šæ˜¯ä¸€ä¸ªå¥½é€‰æ‹©ã€‚å¯¹äºä¸€èˆ¬æ€§çš„é—®é¢˜ï¼Œç›´æ¥å›å¤å¯èƒ½æ›´ä¸ºé«˜æ•ˆå’Œåˆé€‚ã€‚<br>

#### æ£€ç´¢é“¾å®Œæ•´ä»£ç :

```python
"""
@author:PeilongChen(peilongchencc@163.com)
@description:å®ç°åŸºäºLangChainçš„æ–‡æ¡£æ£€ç´¢é“¾ã€‚
"""
import os
from dotenv import load_dotenv
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from load_file_split_documents import ChineseRecursiveTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_openai import ChatOpenAI
from langchain.chains import create_retrieval_chain

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv_path = '.env.local'
load_dotenv(dotenv_path=dotenv_path)

# è®¾ç½®ç½‘ç»œä»£ç†ç¯å¢ƒå˜é‡
os.environ['http_proxy'] = 'http://127.0.0.1:7890'
os.environ['https_proxy'] = 'http://127.0.0.1:7890'


########################################################################
# æ–‡æ¡£åˆ‡åˆ†ï¼Œè¿›è¡Œå‘é‡åŒ–ï¼Œå¹¶å­˜å…¥FAISS
########################################################################

chunk_overlap = 50
chunk_size = 500
# æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„
filepath = 'example_data.txt'
# ä½¿ç”¨LangChainå†…ç½®txtæ–‡ä»¶åŠ è½½å™¨
loader = TextLoader(filepath)
# ä½¿ç”¨åŠ è½½å™¨åŠ è½½æ–‡æ¡£
docs = loader.load()    # æ•°æ®ç±»å‹ä¸ºlist [(page_content='ï¼ˆä¸€ï¼‰ç›´æ¥æ‰“å‹å¼\næ´—ç›˜\nç›´æ¥æ‰“å‹è¾ƒå¤šå‡ºç°åœ¨ åº„å®¶ å¸è´§åŒºåŸŸï¼Œç›®çš„æ˜¯... metadata={'source': 'example_data.txt'}' metadata={'source': 'example_data.txt'})]
# è°ƒç”¨OpenAIçš„Embeddings API
embeddings = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
# å®ä¾‹åŒ–è‡ªå®šä¹‰çš„æ–‡æœ¬åˆ†å‰²å™¨
text_splitter = ChineseRecursiveTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
# è¿›è¡Œæ–‡æœ¬åˆ†å‰²
documents = text_splitter.split_documents(docs)
vector = FAISS.from_documents(documents, embeddings)

########################################################################
# è°ƒç”¨å¤§æ¨¡å‹(æ¥å£)ï¼Œæ„å»ºprompt
########################################################################

llm = ChatOpenAI(
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    # model_name="gpt-4-0125-preview"
    )
# è®©æ¨¡å‹ "ä»…æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”ä»¥ä¸‹é—®é¢˜"
prompt = ChatPromptTemplate.from_template("""Answer the following question based only on the provided context:

<context>
{context}
</context>

Question: {input}""")

document_chain = create_stuff_documents_chain(llm, prompt)  # "input" å’Œ "context" å‚æ•°ä¼šä» `.invoke()` çš„å‚æ•°ä¸­è·å–ã€‚

########################################################################
# æ„å»ºæ£€ç´¢é“¾
########################################################################

retriever = vector.as_retriever()
retrieval_chain = create_retrieval_chain(retriever, document_chain)

response = retrieval_chain.invoke({"input": "æ¨ªç›˜æ•´ç†çš„å½¢æ€æ˜¯ä»€ä¹ˆæ ·å­?"})

# print(response)

print(response["answer"])

# ç”±äºç»“æœå«æœ‰éšæœºæ€§ï¼Œç¬”è€…ç»ˆç«¯è§åˆ°çš„2ç§å›å¤å¦‚ä¸‹:
# å›å¤1: æ¨ªç›˜æ•´ç†çš„å½¢æ€åœ¨Kçº¿ä¸Šçš„è¡¨ç°å¸¸å¸¸æ˜¯ä¸€æ¡æ¨ªçº¿æˆ–è€…é•¿æœŸçš„å¹³å°ã€‚
# å›å¤2: æ¨ªç›˜æ•´ç†çš„å½¢æ€åœ¨Kçº¿ä¸Šçš„è¡¨ç°å¸¸å¸¸æ˜¯ä¸€æ¡æ¨ªçº¿æˆ–è€…é•¿æœŸçš„å¹³å°ï¼Œä»æˆäº¤é‡ä¸Šæ¥çœ‹ï¼Œåœ¨å¹³å°æ•´ç†çš„è¿‡ç¨‹ä¸­æˆäº¤é‡å‘ˆé€’å‡çš„çŠ¶æ€ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨å¹³å°ä¸Šæ²¡æœ‰æˆ–å¾ˆå°‘æœ‰æˆäº¤é‡æ”¾å‡ºã€‚æˆäº¤æ¸…æ·¡ï¼Œæˆäº¤ä»·æ ¼ä¹Ÿæåº¦ä¸æ´»è·ƒã€‚
```

`invoke()`å‡½æ•°çš„å†…éƒ¨è§£é‡Šå¦‚ä¸‹:<br>

```txt
"""Transform a single input into an output. Override to implement.

Args:
    input: The input to the runnable.
    config: A config to use when invoking the runnable.
        The config supports standard keys like 'tags', 'metadata' for tracing
        purposes, 'max_concurrency' for controlling how much work to do
        in parallel, and other keys. Please refer to the RunnableConfig
        for more details.

Returns:
    The output of the runnable.
"""
```

æ„æ€æ˜¯:<br>

```txt
å°†å•ä¸€è¾“å…¥è½¬æ¢ä¸ºè¾“å‡ºã€‚é‡å†™ä»¥å®ç°ã€‚

å‚æ•°ï¼š
- inputï¼šè¿è¡Œå¯¹è±¡çš„è¾“å…¥ã€‚
- configï¼šè°ƒç”¨è¿è¡Œå¯¹è±¡æ—¶ä½¿ç”¨çš„é…ç½®ã€‚è¯¥é…ç½®æ”¯æŒå¦‚'tags'ã€'metadata'ç­‰æ ‡å‡†é”®ç”¨äºè¿½è¸ªç›®çš„ï¼Œ'max_concurrency'ç”¨äºæ§åˆ¶å¹¶è¡Œå·¥ä½œçš„é‡ï¼Œä»¥åŠå…¶ä»–é”®ã€‚è¯·å‚é˜…RunnableConfigä»¥è·å–æ›´å¤šè¯¦æƒ…ã€‚

è¿”å›å€¼ï¼š
- è¿è¡Œå¯¹è±¡çš„è¾“å‡ºã€‚
```