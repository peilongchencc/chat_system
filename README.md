# LangChain

æœ¬é¡¹ç›®åŸºäºLangChainå®˜ç½‘ç¿»è¯‘ã€æ·»åŠ è‡ªå·±çš„ç†è§£ï¼Œä¸»è¦ç”¨äºä¸ªäººç ”ç©¶ã€å­¦ä¹ ã€‚<br>

- [LangChain](#langchain)
  - [Quickstart(å¿«é€Ÿå…¥é—¨):](#quickstartå¿«é€Ÿå…¥é—¨)
    - [Setup(å®‰è£…):](#setupå®‰è£…)
      - [Jupyter Notebook:](#jupyter-notebook)
      - [Installation:](#installation)
    - [LangSmith:](#langsmith)
    - [Building with LangChain(ä½¿ç”¨LangChainæ„å»ºåº”ç”¨):](#building-with-langchainä½¿ç”¨langchainæ„å»ºåº”ç”¨)
    - [LLM Chain:](#llm-chain)
      - [OpenAI:](#openai)
      - [Local:](#local)
    - [æ‹“å±•-pythonæ“ä½œç¬¦| çš„ç”¨æ³•:](#æ‹“å±•-pythonæ“ä½œç¬¦-çš„ç”¨æ³•)
    - [LangChainä½¿ç”¨ | æ“ä½œç¬¦çš„åŸç†:](#langchainä½¿ç”¨--æ“ä½œç¬¦çš„åŸç†)
    - [Chain Debug:](#chain-debug)
    - [Retrieval Chain(æ£€ç´¢é“¾--æŸ¥æ‰¾å¹¶æå–çš„è¿‡ç¨‹):](#retrieval-chainæ£€ç´¢é“¾--æŸ¥æ‰¾å¹¶æå–çš„è¿‡ç¨‹)
      - [ä½•æ—¶ä½¿ç”¨æ£€ç´¢é“¾:](#ä½•æ—¶ä½¿ç”¨æ£€ç´¢é“¾)
      - [æ£€ç´¢é“¾å®Œæ•´ä»£ç :](#æ£€ç´¢é“¾å®Œæ•´ä»£ç )
      - [å…³äºè¿”å›å€¼çš„é‡è¦æé†’:](#å…³äºè¿”å›å€¼çš„é‡è¦æé†’)
      - [Diving Deeper(æ·±å…¥äº†è§£):](#diving-deeperæ·±å…¥äº†è§£)
    - [Conversation Retrieval Chain(å¯¹è¯æ£€ç´¢é“¾):](#conversation-retrieval-chainå¯¹è¯æ£€ç´¢é“¾)
      - [Updating Retrieval(æ›´æ–°æ£€ç´¢):](#updating-retrievalæ›´æ–°æ£€ç´¢)
      - [æµ‹è¯•ä»£ç å¦‚ä¸‹:](#æµ‹è¯•ä»£ç å¦‚ä¸‹)
      - [å¯¹è¯æ£€ç´¢é“¾å®Œæ•´ä»£ç :](#å¯¹è¯æ£€ç´¢é“¾å®Œæ•´ä»£ç )
      - [ç‰¹åˆ«æ³¨æ„:](#ç‰¹åˆ«æ³¨æ„)
    - [Agent(ä»£ç†):](#agentä»£ç†)
    - [Agent å®Œæ•´ä»£ç ç¤ºä¾‹:](#agent-å®Œæ•´ä»£ç ç¤ºä¾‹)
  - [Document loaders:](#document-loaders)
    - [PDF:](#pdf)
      - [Using PyPDF:](#using-pypdf)
    - [Extracting images(æå–å›¾åƒ):](#extracting-imagesæå–å›¾åƒ)
    - [Using MathPix:](#using-mathpix)
    - [Using Unstructured:](#using-unstructured)
      - [ModuleNotFoundError: No module named 'pdf2image':](#modulenotfounderror-no-module-named-pdf2image)
      - [ModuleNotFoundError: No module named 'pdfminer':](#modulenotfounderror-no-module-named-pdfminer)
      - [ModuleNotFoundError: No module named 'pillow\_heif':](#modulenotfounderror-no-module-named-pillow_heif)
      - [ModuleNotFoundError: No module named 'unstructured\_inference':](#modulenotfounderror-no-module-named-unstructured_inference)


## Quickstart(å¿«é€Ÿå…¥é—¨):

In this quickstart we'll show you how to:<br>

åœ¨è¿™ä¸ªå¿«é€Ÿå…¥é—¨ä¸­ï¼Œæˆ‘ä»¬å°†å‘ä½ å±•ç¤ºå¦‚ä½•ï¼š<br>

- Get setup with LangChain, LangSmith and LangServe(å¼€å§‹å®‰è£…å’Œé…ç½® LangChainã€LangSmith å’Œ LangServe)

- Use the most basic and common components(ç»„ä»¶) of LangChain: prompt templates, models, and output parsers(ä½¿ç”¨ LangChain æœ€åŸºç¡€å’Œå¸¸è§çš„ç»„ä»¶ï¼šæç¤ºæ¨¡æ¿ã€æ¨¡å‹å’Œè¾“å‡ºè§£æå™¨)

- Use LangChain Expression Language(æ¨æµ‹åº”è¯¥æŒ‡çš„æ˜¯langchainè¯­è¨€è§„åˆ™), the protocol(åè®®) that LangChain is built on and which facilitates(ä¿ƒè¿›ï¼›ä½¿å˜å¾—æ›´ç®€æ˜“) component chaining(é“¾æ¥ï¼›è¿æ¥)(ä½¿ç”¨LangChainè¡¨è¾¾å¼è¯­è¨€ï¼Œè¿™æ˜¯LangChainæ„å»ºçš„åè®®ï¼Œæœ‰åŠ©äºç»„ä»¶é“¾æ¥)

- Build a simple application with LangChain(ä½¿ç”¨ LangChain æ„å»ºä¸€ä¸ªç®€å•çš„åº”ç”¨ç¨‹åº)

- Trace your application with LangSmith(ç”¨ LangSmith è·Ÿè¸ªä½ çš„åº”ç”¨ç¨‹åº)

- Serve your application with LangServe(ä½¿ç”¨ LangServe ä¸ºä½ çš„åº”ç”¨ç¨‹åºæä¾›æœåŠ¡ï¼ˆæ”¯æ’‘ï¼‰ï¼Œæˆ–è€…ç¿»è¯‘ä¸º "åˆ©ç”¨ LangServe éƒ¨ç½²å¹¶è¿è¡Œä½ çš„åº”ç”¨ç¨‹åºã€‚")

That's a fair amount to cover! Let's dive in.<br>

å†…å®¹ç›¸å½“å¤šï¼è®©æˆ‘ä»¬å¼€å§‹å§ã€‚<br>

### Setup(å®‰è£…):

#### Jupyter Notebook:

This guide (and most of the other guides in the documentation) use Jupyter notebooks and assume(å‡è®¾) the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because often times things can go wrong (unexpected output, API down, etc) and going through guides in an interactive(äº¤äº’çš„) environment is a great way to better understand them.<br>

æœ¬æŒ‡å—ï¼ˆä»¥åŠæ–‡æ¡£ä¸­çš„å¤§å¤šæ•°å…¶ä»–æŒ‡å—ï¼‰éƒ½ä½¿ç”¨ Jupyter ç¬”è®°æœ¬ï¼Œå¹¶å‡è®¾è¯»è€…ä¹Ÿåœ¨ä½¿ç”¨ã€‚Jupyter ç¬”è®°æœ¬éå¸¸é€‚åˆå­¦ä¹ å¦‚ä½•ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç³»ç»Ÿï¼Œå› ä¸ºåœ¨æ“ä½œè¿‡ç¨‹ä¸­ç»å¸¸ä¼šå‡ºç°é—®é¢˜ï¼ˆæ„å¤–è¾“å‡ºã€API æ•…éšœç­‰ï¼‰ï¼Œè€Œåœ¨äº¤äº’å¼ç¯å¢ƒä¸­æµè§ˆæŒ‡å—æ˜¯æ›´å¥½åœ°ç†è§£è¿™äº›é—®é¢˜çš„ç»ä½³æ–¹å¼ã€‚<br>

> "åœ¨äº¤äº’å¼ç¯å¢ƒä¸­æµè§ˆæŒ‡å—æ˜¯æ›´å¥½åœ°ç†è§£è¿™äº›é—®é¢˜çš„ç»ä½³æ–¹å¼" åº”è¯¥æŒ‡çš„æ˜¯äº¤äº’å¼ç¯å¢ƒèƒ½æ›´å¥½è§‚å¯Ÿè¾“å‡ºã€‚æ¯•ç«ŸIDEèƒ½DEBUGçš„ä¼˜åŠ¿å¤ªå¤§hhhã€‚

#### Installation:

åˆ©ç”¨condaåˆ›å»ºè™šæ‹Ÿç¯å¢ƒ:<br>

```bash
conda create --name langchain python=3.10.11
```

æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ:<br>

```bash
conda activate langchain
```

å®‰è£…`langchain`:<br>

```bash
pip install langchain
# or
conda install langchain -c conda-forge
```

> ç¬”è€…ä½¿ç”¨çš„æ˜¯condaå®‰è£…æ–¹å¼ã€‚

### LangSmith:

Many of the applications you build with LangChain will contain multiple steps with multiple invocations(è°ƒç”¨) of LLM calls(è¿™é‡Œåº”è¯¥æŒ‡çš„æ˜¯è°ƒç”¨å¤§æ¨¡å‹çš„æ¥å£). As these applications get more and more complex, it becomes crucial(è‡³å…³é‡è¦çš„) to be able to inspect(æ£€æŸ¥ï¼›ä»”ç»†è§‚å¯Ÿ) what exactly is going on inside your chain(é“¾) or agent(ä»£ç†). The best way to do this is with LangSmith.<br>

ä½¿ç”¨LangChainæ„å»ºåº”ç”¨ç¨‹åºåŒ…å«å¤šä¸ªæ­¥éª¤ï¼Œæ¶‰åŠå¤šæ¬¡è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å£ã€‚éšç€è¿™äº›åº”ç”¨ç¨‹åºå˜å¾—è¶Šæ¥è¶Šå¤æ‚ï¼Œèƒ½å¤Ÿæ£€æŸ¥é“¾æ¡æˆ–ä»£ç†ä¸­ç©¶ç«Ÿå‘ç”Ÿäº†ä»€ä¹ˆå˜å¾—è‡³å…³é‡è¦ã€‚åšè¿™äº‹æœ€ä½³çš„æ–¹æ³•å°±æ˜¯ä½¿ç”¨LangSmithã€‚<br>

> multiple invocations(è°ƒç”¨) of è¡¨ç¤ºå¤šæ¬¡è°ƒç”¨ã€‚

Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:<br>

è¯·æ³¨æ„ï¼Œè™½ç„¶LangSmithå¹¶éå¿…éœ€ï¼Œä½†å®ƒéå¸¸æœ‰å¸®åŠ©ã€‚å¦‚æœæ‚¨æƒ³ä½¿ç”¨LangSmithï¼Œåœ¨é€šè¿‡ä¸Šè¿°é“¾æ¥æ³¨å†Œåï¼Œç¡®ä¿è®¾ç½®æ‚¨çš„ç¯å¢ƒå˜é‡ä»¥å¼€å§‹è®°å½•è¿½è¸ªæ—¥å¿—ï¼š<br>

```bash
export LANGCHAIN_TRACING_V2="true"
export LANGCHAIN_API_KEY="..."
```

### Building with LangChain(ä½¿ç”¨LangChainæ„å»ºåº”ç”¨):

LangChain enables building application that connect external(å¤–éƒ¨çš„) sources of data and computation(è®¡ç®—ï¼›è®¡ç®—è¿‡ç¨‹) to LLMs. <br>

LangChainæ”¯æŒæ„å»ºè¿æ¥å¤–éƒ¨æ•°æ®æºå’Œè®¡ç®—èµ„æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨ã€‚<br>

In this quickstart, we will walk through(ä¸€æ­¥ä¸€æ­¥åœ°å±•ç¤ºæˆ–è§£é‡Š) a few different ways of doing that. We will start with a simple LLM chain, which just relies on(ä¾èµ–) information in the prompt template to respond. <br>

åœ¨è¿™ä¸ªå¿«é€Ÿå…¥é—¨ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»å‡ ç§ä¸åŒçš„å®ç°æ–¹å¼ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä»ä¸€ä¸ªç®€å•çš„LLMé“¾å¼€å§‹ï¼Œå®ƒä»…ä¾èµ–äºæç¤ºæ¨¡æ¿ä¸­çš„ä¿¡æ¯æ¥è¿›è¡Œå›åº”ã€‚<br>

Next, we will build a retrieval(æ£€ç´¢) chain, which fetches data from a separate(ç‹¬ç«‹çš„) database and passes that into the prompt template. We will then add in chat history, to create a conversation retrieval chain(å¯¹è¯æ£€ç´¢é“¾). <br>

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªæ£€ç´¢é“¾ï¼Œå®ƒä¼šä»ç‹¬ç«‹çš„æ•°æ®åº“ä¸­è·å–æ•°æ®ï¼Œå¹¶å°†è¿™äº›æ•°æ®ä¼ é€’ç»™æç¤ºæ¨¡æ¿ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†æ·»åŠ èŠå¤©å†å²è®°å½•ï¼Œä»¥åˆ›å»ºä¸€ä¸ªå¯¹è¯æ£€ç´¢é“¾ã€‚<br>

This allows you interact(äº¤äº’) in a chat manner with this LLM, **so it remembers previous questions**. <br>

è¿™ä½¿æ‚¨èƒ½å¤Ÿä»¥èŠå¤©æ–¹å¼ä¸è¿™ä¸ªLLMäº¤äº’ï¼Œ**å› æ­¤å®ƒèƒ½è®°ä½ä¹‹å‰çš„é—®é¢˜**ã€‚<br>

Finally, we will build an agent - which utilizes(åˆ©ç”¨;ä½¿ç”¨;è¿ç”¨) an LLM to determine whether or not it needs to fetch data to answer questions. <br>

æœ€åï¼Œæˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªä»£ç† - å®ƒåˆ©ç”¨LLMæ¥åˆ¤æ–­æ˜¯å¦éœ€è¦è·å–æ•°æ®æ¥å›ç­”é—®é¢˜ã€‚<br>

We will cover these at a high level, but there are lot of details to all of these! We will link to relevant docs.<br>

> æ¨æµ‹å¤§æ¦‚æ˜¯æŒ‡ç«™åœ¨é«˜å¤„æ›´å®¹æ˜“çœ‹æ¸…äº‹ç‰©æœ¬è´¨ã€‚

æˆ‘ä»¬å°†ä»é«˜å±‚æ¬¡ä¸Šä»‹ç»è¿™äº›å†…å®¹ï¼Œä½†è¿™äº›éƒ½æœ‰å¾ˆå¤šç»†èŠ‚ï¼æˆ‘ä»¬å°†é“¾æ¥åˆ°ç›¸å…³æ–‡æ¡£ã€‚<br>

### LLM Chain:

For this getting started guide, we will provide two options: using OpenAI (a popular model available via API) or using a local open source model.<br>

å¯¹äºè¿™ä¸ªå…¥é—¨æŒ‡å—ï¼Œæˆ‘ä»¬å°†æä¾›ä¸¤ç§é€‰æ‹©ï¼šä½¿ç”¨ OpenAIï¼ˆä¸€ç§å¯ä»¥é€šè¿‡ API è°ƒç”¨çš„æµè¡Œæ¨¡å‹ï¼‰æˆ–ä½¿ç”¨æœ¬åœ°å¼€æºæ¨¡å‹ã€‚<br>

#### OpenAI:

First we'll need to import the LangChain x OpenAI integration(é›†æˆ) package.<br>

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å¯¼å…¥ LangChain ä¸ OpenAI çš„é›†æˆåŒ…ã€‚<br>

```bash
pip install langchain-openai
```

Accessing the API requires an API key, which you can get by creating an account(è´¦æˆ·) and heading(å‰å¾€) [here](https://platform.openai.com/api-keys). Once we have a key we'll want to set it as an environment variable by running:<br>

è¦è®¿é—® APIï¼Œä½ éœ€è¦ä¸€ä¸ª API å¯†é’¥ï¼Œå¯ä»¥é€šè¿‡åˆ›å»ºä¸€ä¸ªè´¦æˆ·å¹¶å‰å¾€è¿™é‡Œæ¥è·å–ã€‚ä¸€æ—¦æˆ‘ä»¬æ‹¥æœ‰äº†å¯†é’¥ï¼Œæˆ‘ä»¬ä¼šæƒ³è¦é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤å°†å…¶è®¾ç½®ä¸ºä¸€ä¸ªç¯å¢ƒå˜é‡ï¼š<br>

```bash
export OPENAI_API_KEY="your-api-key-here"
```

We can then initialize the model:<br>

ç„¶åæˆ‘ä»¬å¯ä»¥åˆå§‹åŒ–æ¨¡å‹ï¼š<br>

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()
```

If you'd prefer not to set an environment variable you can pass the key in directly(ç›´æ¥åœ°) via the `openai_api_key` named parameter when initiating the OpenAI LLM class:<br>

å¦‚æœä½ ä¸æƒ³è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œä½ å¯ä»¥åœ¨åˆå§‹åŒ– OpenAI LLM ç±»æ—¶ï¼Œç›´æ¥é€šè¿‡ openai_api_key è¿™ä¸ªå‘½åå‚æ•°ä¼ å…¥å¯†é’¥ï¼š<br>

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(openai_api_key="...")
```

Once you've installed and initialized the LLM of your choice, we can try using it!<br>

ä¸€æ—¦ä½ å®‰è£…å¹¶åˆå§‹åŒ–äº†ä½ é€‰æ‹©çš„ LLMï¼Œæˆ‘ä»¬å°±å¯ä»¥å°è¯•ä½¿ç”¨å®ƒäº†ï¼<br>

Let's ask it what LangSmith is - this is something that wasn't present in the training data so it shouldn't have a very good response.<br>

è®©æˆ‘ä»¬é—®é—®å®ƒ LangSmith æ˜¯ä»€ä¹ˆ - è¿™æ˜¯è®­ç»ƒæ•°æ®ä¸­æ²¡æœ‰çš„å†…å®¹ï¼Œæ‰€ä»¥å®ƒåº”è¯¥ä¸ä¼šæœ‰å¾ˆå¥½çš„å›ç­”ã€‚<br>

```python
llm.invoke("how can langsmith help with testing?")
```

> åœ¨ç¼–ç¨‹é¢†åŸŸï¼Œ"invoke a function" è¡¨ç¤º "è°ƒç”¨ä¸€ä¸ªå‡½æ•°"ï¼Œæ‰€ä»¥ä¸Šè¿°ä»£ç ä¸­ "invoke" çš„æ„æ€å¤§æ¦‚ç‡æ˜¯ "è°ƒç”¨"ã€‚

ç¬”è€…åœ¨IDEä¸­æµ‹è¯•çš„æ•ˆæœå¦‚ä¸‹:<br>

> å¿…é¡»ä¿è¯ç»ˆç«¯èƒ½å¤Ÿè¿æ¥openaiæœåŠ¡ï¼Œæ‰å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç ã€‚
> ç»æµ‹è¯•ï¼Œè¿è¡Œ`llm.invoke("...")`åç»ˆç«¯ä»¥éæµå¼è¾“å‡ºå½¢å¼è¿”å›ã€‚

```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv_path = '.env.local'
load_dotenv(dotenv_path=dotenv_path)

llm = ChatOpenAI(openai_api_key=os.getenv("OPENAI_API_KEY"))

llm_response = llm.invoke("ä½ å¥½")

print(llm_response)     # content='ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ'
print(type(llm_response))   # <class 'langchain_core.messages.ai.AIMessage'>
print(llm_response.content) # ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ
```

We can also guide(æŒ‡å¯¼ï¼›å¼•å¯¼) it's response with a prompt template(æ¨¡æ¿). Prompt templates are used to convert(è½¬æ¢) raw user input to a better input to the LLM.<br>

æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨æç¤ºæ¨¡æ¿æ¥å¼•å¯¼å…¶å›åº”ã€‚æç¤ºæ¨¡æ¿ç”¨äºå°†åŸå§‹ç”¨æˆ·è¾“å…¥è½¬æ¢ä¸ºæ›´é€‚åˆè¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å…¥ã€‚

```python
from langchain_core.prompts import ChatPromptTemplate
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are world class technical documentation writer."),  # è‹±æ–‡çš„æ„æ€æ˜¯: ä½ æ˜¯ä¸–ç•Œçº§çš„æŠ€æœ¯æ–‡æ¡£æ’°å†™ä¸“å®¶(å†™æ‰‹)ã€‚
    ("user", "{input}")
])
```

We can now combine these into a simple LLM chain:<br>

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™äº›ç»„åˆæˆä¸€ä¸ªç®€å•çš„LLMé“¾ï¼š<br>

```python
chain = prompt | llm 
```

We can now invoke(è°ƒç”¨) it and ask the same question. It still won't know the answer, but it should respond(å›åº”) in a more proper tone for a technical writer!<br>

ç°åœ¨æˆ‘ä»¬å¯ä»¥è°ƒç”¨å®ƒå¹¶è¯¢é—®ç›¸åŒçš„é—®é¢˜ã€‚å®ƒä»ç„¶ä¸ä¼šçŸ¥é“ç­”æ¡ˆï¼Œä½†åº”è¯¥ä»¥æ›´é€‚åˆæŠ€æœ¯å†™æ‰‹çš„è¯­æ°”å›åº”ï¼<br>

```python
chain.invoke({"input": "how can langsmith help with testing?"})
```

ç¬”è€…æ‰€ç”¨çš„å®Œæ•´ä»£ç å¦‚ä¸‹:<br>

```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv_path = '.env.local'
load_dotenv(dotenv_path=dotenv_path)

llm = ChatOpenAI(openai_api_key=os.getenv("OPENAI_API_KEY"))

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are world class technical documentation writer."),
    ("user", "{input}")
])

chain = prompt | llm 

chain_response = chain.invoke({"input": "how can langsmith help with testing?"})

print(chain_response.content)
```

The output of a ChatModel (and therefore, of this chain) is a message. However, it's often much more convenient to work with strings. Let's add a simple output parser to convert the chat message to a string.<br>

> "and therefore, of this chain" è¿™ä¸ªçŸ­è¯­çš„æ„æ€æ˜¯ "å› æ­¤ï¼Œå¯¹äºè¿™ä¸ªé“¾ä¹Ÿæ˜¯å¦‚æ­¤"ã€‚æä¸æ‡‚ "å› æ­¤ï¼Œå¯¹äºè¿™ä¸ªé“¾ä¹Ÿæ˜¯å¦‚æ­¤" åœ¨è¿™å¥è¯ä¸­çš„å«ä¹‰ï¼Œä½†ä¸å¦¨ç¢æ•´å¥è¯çš„ç†è§£ã€‚

ChatModelï¼ˆå› æ­¤ï¼Œå¯¹äºè¿™ä¸ªé“¾ä¹Ÿæ˜¯å¦‚æ­¤ï¼‰çš„è¾“å‡ºæ˜¯ä¸€æ¡æ¶ˆæ¯ã€‚ç„¶è€Œï¼Œä½¿ç”¨å­—ç¬¦ä¸²é€šå¸¸æ›´æ–¹ä¾¿ã€‚è®©æˆ‘ä»¬æ·»åŠ ä¸€ä¸ªç®€å•çš„è¾“å‡ºè§£æå™¨ï¼Œå°†èŠå¤©æ¶ˆæ¯è½¬æ¢ä¸ºå­—ç¬¦ä¸²ã€‚<br>

```python
from langchain_core.output_parsers import StrOutputParser

output_parser = StrOutputParser()
```

We can now add this to the previous(ä¹‹å‰çš„) chain:<br>

ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªæ·»åŠ åˆ°ä¹‹å‰çš„é“¾ä¸­ï¼š<br>

```python
chain = prompt | llm | output_parser
```

We can now invoke(è°ƒç”¨) it and ask the same question. The answer will now be a string (rather than a ChatMessage).<br>

ç°åœ¨æˆ‘ä»¬å¯ä»¥è°ƒç”¨å®ƒå¹¶æå‡ºåŒæ ·çš„é—®é¢˜ã€‚ç°åœ¨çš„ç­”æ¡ˆå°†æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ˆè€Œä¸æ˜¯ChatMessageï¼‰ã€‚<br>

```python
chain.invoke({"input": "how can langsmith help with testing?"})
```

ç¬”è€…æ‰€ç”¨å®Œæ•´ä»£ç :<br>

```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv_path = '.env.local'
load_dotenv(dotenv_path=dotenv_path)

llm = ChatOpenAI(openai_api_key=os.getenv("OPENAI_API_KEY"))

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are world class technical documentation writer."),
    ("user", "{input}")
])

output_parser = StrOutputParser()

chain = prompt | llm | output_parser

chain_response = chain.invoke({"input": "how can langsmith help with testing?"})

print(chain_response)
print(type(chain_response))     # <class 'str'>
```

è¿™æ¬¡æ·»åŠ äº†ç»“æœè§£æå™¨ï¼Œè¾“å‡ºçš„ç›´æ¥æ˜¯ `string` ç±»å‹çš„ç»“æœï¼Œå…¶å®å’Œä¹‹å‰ç¬”è€…ä½¿ç”¨çš„ `print(chain_response.content)` æ•ˆæœç›¸åŒã€‚<br>

#### Local:

Ollama allows you to run open-source large language models, such as Llama 2, locally.<br>

Ollama ä½¿ä½ èƒ½å¤Ÿåœ¨æœ¬åœ°è¿è¡Œå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ¯”å¦‚ Llama 2ã€‚<br>

First, follow these instructions(æŒ‡ç¤º) to set up and run a local Ollama instance(å®ä¾‹):<br>

é¦–å…ˆï¼ŒæŒ‰ç…§ä»¥ä¸‹æŒ‡ç¤ºæ¥è®¾ç½®å¹¶è¿è¡Œä¸€ä¸ªæœ¬åœ°çš„ Ollama å®ä¾‹ï¼š<br>

- Download
- Fetch a model via `ollama pull llama2`(é€šè¿‡ `ollama pull llama2` å‘½ä»¤è·å–æ¨¡å‹)

Then, make sure the Ollama server is running. After that, you can do:<br>

ç„¶åï¼Œç¡®ä¿ Ollama æœåŠ¡å™¨æ­£åœ¨è¿è¡Œã€‚ä¹‹åï¼Œä½ å¯ä»¥è¿›è¡Œï¼š<br>


### æ‹“å±•-pythonæ“ä½œç¬¦| çš„ç”¨æ³•:

Python ä¸­çš„ `|` æ“ä½œç¬¦ä¸»è¦æœ‰ä¸¤ç§ç”¨é€”ï¼Œå…·ä½“ç”¨æ³•å–å†³äºå®ƒè¢«ç”¨åœ¨ä»€ä¹ˆä¸Šä¸‹æ–‡ä¸­ï¼š<br>

1. **æŒ‰ä½æˆ–ï¼ˆBitwise ORï¼‰**ï¼šå½“ `|` ç”¨äºæ•´æ•°æ—¶ï¼Œå®ƒæ‰§è¡ŒæŒ‰ä½æˆ–æ“ä½œã€‚è¿™æ„å‘³ç€å¯¹ä¸¤ä¸ªæ•°çš„äºŒè¿›åˆ¶è¡¨ç¤ºè¿›è¡Œæ“ä½œï¼Œåªè¦å¯¹åº”ä½ä¸­è‡³å°‘æœ‰ä¸€ä¸ªä¸º1ï¼Œåˆ™ç»“æœçš„è¯¥ä½ä¹Ÿä¸º1ã€‚

ä¾‹å¦‚ï¼š<br>

```python
a = 0b1010  # äºŒè¿›åˆ¶è¡¨ç¤ºï¼Œç­‰äºåè¿›åˆ¶ä¸­çš„10
b = 0b0101  # äºŒè¿›åˆ¶è¡¨ç¤ºï¼Œç­‰äºåè¿›åˆ¶ä¸­çš„5
result = a | b  # ç»“æœå°†æ˜¯0b1111ï¼Œå³åè¿›åˆ¶ä¸­çš„15
```

2. **é›†åˆçš„å¹¶é›†ï¼ˆSet Unionï¼‰**ï¼šå½“ `|` ç”¨åœ¨ä¸¤ä¸ªé›†åˆä¸Šæ—¶ï¼Œå®ƒè¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰åœ¨ä¸¤ä¸ªé›†åˆä¸­çš„å…ƒç´ çš„æ–°é›†åˆï¼Œç›¸å½“äºä¸¤ä¸ªé›†åˆçš„å¹¶é›†ã€‚

ä¾‹å¦‚ï¼š<br>

```python
set1 = {1, 2, 3}
set2 = {3, 4, 5}
result = set1 | set2  # ç»“æœå°†æ˜¯{1, 2, 3, 4, 5}
```

ä» Python 3.9 å¼€å§‹ï¼Œ`|` è¿˜å¯ä»¥ç”¨äºå­—å…¸ï¼Œè¡¨ç¤ºåˆå¹¶ä¸¤ä¸ªå­—å…¸ï¼š<br>

3. **å­—å…¸çš„åˆå¹¶ï¼ˆDictionary Mergeï¼‰**ï¼šä½¿ç”¨ `|` æ“ä½œç¬¦å¯ä»¥åˆå¹¶ä¸¤ä¸ªå­—å…¸ï¼Œå¦‚æœä¸¤ä¸ªå­—å…¸æœ‰ç›¸åŒçš„é”®ï¼Œåˆ™ç¬¬äºŒä¸ªå­—å…¸ä¸­çš„é”®å€¼å¯¹ä¼šè¦†ç›–ç¬¬ä¸€ä¸ªå­—å…¸ä¸­çš„é”®å€¼å¯¹ã€‚

ä¾‹å¦‚ï¼š<br>

```python
dict1 = {'a': 1, 'b': 2}
dict2 = {'b': 3, 'c': 4}
result = dict1 | dict2  # ç»“æœå°†æ˜¯{'a': 1, 'b': 3, 'c': 4}
```

è¿™äº›æ˜¯ `|` æ“ä½œç¬¦åœ¨ Python ä¸­çš„ä¸»è¦ç”¨é€”ã€‚å®ƒçš„ç¡®åˆ‡è¡Œä¸ºå–å†³äºæ“ä½œæ•°çš„ç±»å‹ã€‚<br>

### LangChainä½¿ç”¨ | æ“ä½œç¬¦çš„åŸç†:

`LangChain` ä½¿ç”¨ `|` æ“ä½œç¬¦çš„èƒ½åŠ›ä¸æ˜¯ Python è¯­è¨€å†…å»ºçš„åŠŸèƒ½ï¼Œè€Œæ˜¯é€šè¿‡è‡ªå®šä¹‰ç±»å’Œé‡è½½è¿ç®—ç¬¦å®ç°çš„ã€‚Python å…è®¸ç±»é€šè¿‡å®šä¹‰ç‰¹æ®Šæ–¹æ³•ï¼ˆä¹Ÿç§°ä¸ºé­”æœ¯æ–¹æ³•æˆ–åŒä¸‹åˆ’çº¿æ–¹æ³•ï¼‰æ¥é‡è½½ï¼ˆoverrideï¼‰æˆ–å®ç°ç‰¹å®šçš„è¿ç®—ç¬¦è¡Œä¸ºã€‚å¯¹äº `|` æ“ä½œç¬¦ï¼Œç›¸å…³çš„é­”æœ¯æ–¹æ³•æ˜¯ `__or__`ã€‚<br>

> ç»DebugéªŒè¯ï¼ŒLangChainä½¿ç”¨çš„ç¡®å®æ˜¯ `__or__`æ–¹æ³•ã€‚

å½“ä½  `LangChain` ä½¿ç”¨ `|` æ“ä½œç¬¦ä»¥éæ ‡å‡†æ–¹å¼ï¼ˆä¾‹å¦‚ï¼Œä¸æ˜¯æŒ‰ä½æˆ–ã€é›†åˆå¹¶é›†æˆ–å­—å…¸åˆå¹¶ï¼‰ï¼Œè¿™é€šå¸¸æ„å‘³ç€è¯¥ç±»å®šä¹‰äº†ä¸€ä¸ª `__or__` æ–¹æ³•æ¥è‡ªå®šä¹‰ `|` æ“ä½œç¬¦çš„è¡Œä¸ºã€‚<br>

è¿™ç§æŠ€æœ¯å…è®¸å¼€å‘è€…åˆ›å»ºæ›´å…·è¡¨è¾¾æ€§å’Œé¢†åŸŸç‰¹å®šçš„è¯­æ³•ï¼Œæé«˜ä»£ç çš„å¯è¯»æ€§å’Œæ˜“ç”¨æ€§ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªæ•°æ®å¤„ç†åº“å¯èƒ½å…è®¸ä½ å°†æ•°æ®è½¬æ¢æˆ–è¿‡æ»¤æ“ä½œä»¥é“¾å¼æ–¹å¼è¿æ¥èµ·æ¥ï¼Œä½¿ç”¨ `|` æ“ä½œç¬¦æ¥è¡¨ç¤ºè¿™ç§è¿æ¥ï¼Œä½¿å¾—ä»£ç æ›´åŠ ç›´è§‚ã€‚<br>

è¿™é‡Œæ˜¯ä¸€ä¸ªç®€åŒ–çš„ä¾‹å­ï¼Œæ¼”ç¤ºå¦‚ä½•ä¸ºä¸€ä¸ªç±»é‡è½½ `|` æ“ä½œç¬¦ï¼š<br>

```python
class CustomClass:
    def __init__(self, value):
        self.value = value

    # å®šä¹‰ | æ“ä½œç¬¦çš„è¡Œä¸º
    def __or__(self, other):
        # è¿™é‡Œå¯ä»¥å®šä¹‰ä»»ä½•è‡ªå®šä¹‰é€»è¾‘
        # ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°åˆå¹¶ä¸¤ä¸ªå¯¹è±¡çš„å€¼
        return CustomClass(self.value + other.value)

# ä½¿ç”¨è‡ªå®šä¹‰çš„ | æ“ä½œç¬¦
a = CustomClass(5)
b = CustomClass(10)
result = a | b
print(result.value)  # è¾“å‡ºï¼š15
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ`CustomClass` çš„å®ä¾‹ `a` å’Œ `b` èƒ½å¤Ÿä½¿ç”¨ `|` æ“ä½œç¬¦ï¼Œå…¶è¡Œä¸ºæ˜¯å°†ä¸¤ä¸ªå®ä¾‹çš„ `value` å±æ€§å€¼ç›¸åŠ ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨ `CustomClass` ä¸­å®šä¹‰äº† `__or__` æ–¹æ³•æ¥å®ç°è¿™ä¸€è¡Œä¸ºã€‚<br>

æœ¬è´¨ä¸Šæ‰§è¡Œçš„æ˜¯ `a | b` æ‰§è¡Œçš„æ˜¯ `a.__or__(b)` ï¼ŒæŠŠ `b` ä½œä¸ºäº† `other` ã€‚<br>

å…·ä½“é€»è¾‘å›¾å¦‚ä¸‹:<br>

![](./materials/operator_logic.jpg)

å½“è¿è¡Œåˆ° `chain = prompt | llm | output_parser` æ—¶ï¼ŒDebugå‘ç°ï¼Œå…¶å®æ˜¯è·³è½¬åˆ°äº†LangChainä¸­ `Runnable` ç±»çš„ `__or__` æ–¹æ³•ã€‚<br>

```python
class Runnable(Generic[Input, Output], ABC):
  
  # çœç•¥å…¶ä»–å‡½æ•°

  def __or__(
      self,
      other: Union[
          Runnable[Any, Other],
          Callable[[Any], Other],
          Callable[[Iterator[Any]], Iterator[Other]],
          Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]],
      ],
  ) -> RunnableSerializable[Input, Other]:
      """Compose this runnable with another object to create a RunnableSequence.
      è¯‘: å°†æ­¤ runnable ä¸å¦ä¸€ä¸ªå¯¹è±¡ç»„åˆï¼Œä»¥åˆ›å»ºä¸€ä¸ª RunnableSequenceã€‚
      """
      return RunnableSequence(self, coerce_to_runnable(other))
```

```python
def coerce_to_runnable(thing: RunnableLike) -> Runnable[Input, Output]:
    """Coerce(å¼ºåˆ¶;å¼ºè¿«) a runnable-like object into a Runnable.

    Args:
        thing: A runnable-like object.

    Returns:
        A Runnable.
    """
    if isinstance(thing, Runnable):
        return thing
    elif inspect.isasyncgenfunction(thing) or inspect.isgeneratorfunction(thing):
        return RunnableGenerator(thing)
    elif callable(thing):
        return RunnableLambda(cast(Callable[[Input], Output], thing))
    elif isinstance(thing, dict):
        return cast(Runnable[Input, Output], RunnableParallel(thing))
    else:
        raise TypeError(
            f"Expected a Runnable, callable or dict."
            f"Instead got an unsupported type: {type(thing)}"
        )
```


```python
class RunnableSequence(RunnableSerializable[Input, Output]):
    """A sequence of runnables, where the output of each is the input of the next.

    RunnableSequence is the most important composition operator in LangChain as it is
    used in virtually every chain.

    A RunnableSequence can be instantiated directly or more commonly by using the `|`
    operator where either the left or right operands (or both) must be a Runnable.

    Any RunnableSequence automatically supports sync, async, batch.

    The default implementations of `batch` and `abatch` utilize threadpools and
    asyncio gather and will be faster than naive invocation of invoke or ainvoke
    for IO bound runnables.

    Batching is implemented by invoking the batch method on each component of the
    RunnableSequence in order.

    A RunnableSequence preserves the streaming properties of its components, so if all
    components of the sequence implement a `transform` method -- which
    is the method that implements the logic to map a streaming input to a streaming
    output -- then the sequence will be able to stream input to output!

    If any component of the sequence does not implement transform then the
    streaming will only begin after this component is run. If there are
    multiple blocking components, streaming begins after the last one.

    Please note: RunnableLambdas do not support `transform` by default! So if
        you need to use a RunnableLambdas be careful about where you place them in a
        RunnableSequence (if you need to use the .stream()/.astream() methods).

        If you need arbitrary logic and need streaming, you can subclass
        Runnable, and implement `transform` for whatever logic you need.

    Here is a simple example that uses simple functions to illustrate the use of
    RunnableSequence:

        .. code-block:: python

            from langchain_core.runnables import RunnableLambda

            def add_one(x: int) -> int:
                return x + 1

            def mul_two(x: int) -> int:
                return x * 2

            runnable_1 = RunnableLambda(add_one)
            runnable_2 = RunnableLambda(mul_two)
            sequence = runnable_1 | runnable_2
            # Or equivalently:
            # sequence = RunnableSequence(first=runnable_1, last=runnable_2)
            sequence.invoke(1)
            await sequence.ainvoke(1)

            sequence.batch([1, 2, 3])
            await sequence.abatch([1, 2, 3])

    Here's an example that uses streams JSON output generated by an LLM:

        .. code-block:: python

            from langchain_core.output_parsers.json import SimpleJsonOutputParser
            from langchain_openai import ChatOpenAI

            prompt = PromptTemplate.from_template(
                'In JSON format, give me a list of {topic} and their '
                'corresponding names in French, Spanish and in a '
                'Cat Language.'
            )

            model = ChatOpenAI()
            chain = prompt | model | SimpleJsonOutputParser()

            async for chunk in chain.astream({'topic': 'colors'}):
                print('-')
                print(chunk, sep='', flush=True)
    """

    # The steps are broken into first, middle and last, solely for type checking
    # purposes. It allows specifying the `Input` on the first type, the `Output` of
    # the last type.
    first: Runnable[Input, Any]
    """The first runnable in the sequence."""
    middle: List[Runnable[Any, Any]] = Field(default_factory=list)
    """The middle runnables in the sequence."""
    last: Runnable[Any, Output]
    """The last runnable in the sequence."""

    def __init__(
        self,
        *steps: RunnableLike,
        name: Optional[str] = None,
        first: Optional[Runnable[Any, Any]] = None,
        middle: Optional[List[Runnable[Any, Any]]] = None,
        last: Optional[Runnable[Any, Any]] = None,
    ) -> None:
        """Create a new RunnableSequence.

        Args:
            steps: The steps to include in the sequence.
        """
        steps_flat: List[Runnable] = []
        if not steps:
            if first is not None and last is not None:
                steps_flat = [first] + (middle or []) + [last]
        for step in steps:
            if isinstance(step, RunnableSequence):
                steps_flat.extend(step.steps)
            else:
                steps_flat.append(coerce_to_runnable(step))
        if len(steps_flat) < 2:
            raise ValueError(
                f"RunnableSequence must have at least 2 steps, got {len(steps_flat)}"
            )
        super().__init__(
            first=steps_flat[0],
            middle=list(steps_flat[1:-1]),
            last=steps_flat[-1],
            name=name,
        )
```

### Chain Debug:

![](./materials/chain_debug.jpg)

### Retrieval Chain(æ£€ç´¢é“¾--æŸ¥æ‰¾å¹¶æå–çš„è¿‡ç¨‹):

In order to properly(æ°å½“çš„) answer the original(åŸå§‹çš„) question ("how can langsmith help with testing?"), we need to provide additional(é¢å¤–çš„) context to the LLM.<br>

ä¸ºäº†æ°å½“å›ç­”åŸå§‹é—®é¢˜ï¼ˆ"langsmith å¦‚ä½•å¸®åŠ©è¿›è¡Œæµ‹è¯•ï¼Ÿ"ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦å‘ LLM æä¾›é¢å¤–çš„ä¸Šä¸‹æ–‡ã€‚<br>

 We can do this via retrieval. Retrieval is useful when you have too much data to pass to the LLM directly(ç›´æ¥åœ°)(too..to... å¤ª...è€Œä¸èƒ½...). You can then use a retriever to fetch only the most relevant pieces(æœ€ç›¸å…³çš„ç‰‡æ®µ) and pass those in.<br>

è¿™å¯ä»¥é€šè¿‡æ£€ç´¢æ¥å®ç°ã€‚å½“ä½ æœ‰å¤ªå¤šæ•°æ®ä¸èƒ½ç›´æ¥ä¼ é€’ç»™ LLM æ—¶ï¼Œæ£€ç´¢éå¸¸æœ‰ç”¨ã€‚ä½ å¯ä»¥ä½¿ç”¨æ£€ç´¢å™¨åªè·å–æœ€ç›¸å…³çš„æ•°æ®ç‰‡æ®µç„¶åä¼ å…¥ã€‚<br>

![](./materials/retrieval_resolve.jpg)

In this process, we will look up(æŸ¥è¯¢) relevant documents from a Retriever and then pass them into the prompt. A Retriever can be backed(å¯ä»¥ä¾æ®...) by anything - a SQL table, the internet, etc - but in this instance(å®ä¾‹) we will populate(ç”Ÿæ´»äºï¼›å¡«å……) a vector store and use that as a retriever. For more information on vectorstores, see this [documentation](https://python.langchain.com/docs/modules/data_connection/vectorstores).<br>

åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä»æ£€ç´¢å™¨ä¸­æŸ¥æ‰¾ç›¸å…³æ–‡æ¡£ï¼Œç„¶åå°†å®ƒä»¬ä¼ å…¥æç¤ºä¸­ã€‚æ£€ç´¢å™¨å¯ä»¥ç”±ä»»ä½•ä¸œè¥¿æ”¯æŒâ€”â€”ä¸€ä¸ª SQL è¡¨ï¼Œäº’è”ç½‘ç­‰â€”â€”ä½†åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†å¡«å……ä¸€ä¸ªå‘é‡å­˜å‚¨å¹¶ä½¿ç”¨å®ƒä½œä¸ºæ£€ç´¢å™¨ã€‚æœ‰å…³å‘é‡å­˜å‚¨çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤æ–‡æ¡£ã€‚<br>

First, we need to load(åŠ è½½) the data that we want to index(æ£€ç´¢). In order to do this, we will use the `WebBaseLoader`. This requires installing `BeautifulSoup`:<br>

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦åŠ è½½æˆ‘ä»¬æƒ³è¦ç´¢å¼•çš„æ•°æ®ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `WebBaseLoader` ã€‚è¿™éœ€è¦å®‰è£… `BeautifulSoup`ï¼š<br>

```bash
pip install beautifulsoup4
```

After that, we can import and use WebBaseLoader.<br>

ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥å¯¼å…¥å¹¶ä½¿ç”¨ WebBaseLoaderã€‚<br>

```python
from langchain_community.document_loaders import WebBaseLoader
loader = WebBaseLoader("https://docs.smith.langchain.com/overview")

docs = loader.load()
```

ä»£ç è§£é‡Š:<br>

ä¸Šè¿°ä»£ç ä½¿ç”¨äº†`langchain_community.document_loaders`æ¨¡å—ä¸­çš„`WebBaseLoader`ç±»ï¼Œç›®çš„æ˜¯ä»æŒ‡å®šçš„URLåŠ è½½æ–‡æ¡£ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒåˆ›å»ºäº†ä¸€ä¸ªæŒ‡å‘URL "https://docs.smith.langchain.com/overview" çš„`WebBaseLoader`å®ä¾‹ã€‚ç„¶åï¼Œè°ƒç”¨æ­¤å®ä¾‹ä¸Šçš„`load`æ–¹æ³•æ¥ä»æŒ‡å®šçš„URLè·å–å’ŒåŠ è½½å†…å®¹ã€‚<br>

ä»¥ä¸‹æ˜¯`WebBaseLoader`å·¥ä½œæ–¹å¼åŠå…¶åŠŸèƒ½çš„è¯¦ç»†è§£é‡Šï¼Œè¿™äº›ä¿¡æ¯æ ¹æ®LangChainæ–‡æ¡£æä¾›ï¼š<br>

- **åˆå§‹åŒ–**ï¼š`WebBaseLoader`åœ¨åˆå§‹åŒ–æ—¶å¯ä»¥è®¾ç½®å¤šä¸ªå‚æ•°ï¼Œå¦‚`web_path`ï¼ˆè¦ä»ä¸­åŠ è½½çš„URLsï¼‰ã€`header_template`ï¼ˆè¯·æ±‚çš„HTTPå¤´ï¼‰å’Œ`verify_ssl`ï¼ˆæ˜¯å¦éªŒè¯SSLè¯ä¹¦ï¼‰ç­‰ã€‚æ­¤ç±»æ—¨åœ¨ä½¿ç”¨`urllib`åŠ è½½HTMLé¡µé¢ï¼Œå¹¶ç”¨`BeautifulSoup`è§£æï¼Œ`BeautifulSoup`æ˜¯ä¸€ä¸ªç”¨äºä»HTMLå’ŒXMLæ–‡ä»¶ä¸­æå–æ•°æ®çš„åº“ã€7â€ æ¥æºã€‘ã€‚

- **åŠ è½½æ–¹æ³•**ï¼š`load`æ–¹æ³•ç‰¹åˆ«ç”¨äºä»åˆå§‹åŒ–æ—¶æŒ‡å®šçš„URL(s)è·å–å’Œè§£æHTMLå†…å®¹ã€‚å®ƒä½¿ç”¨`BeautifulSoup`è¿›è¡Œè§£æï¼Œå¦‚æœæä¾›äº†å¤šä¸ªURLsï¼Œä¹Ÿå¯ä»¥å¤„ç†ã€‚åŠ è½½å™¨æ”¯æŒå¼‚æ­¥æ“ä½œï¼Œè¯·æ±‚çš„é€Ÿç‡é™åˆ¶ï¼Œä»¥åŠé€šè¿‡BeautifulSoupè¿›è¡Œè‡ªå®šä¹‰è§£æé€‰é¡¹ã€‚

- **åŠŸèƒ½**ï¼šè¯¥ç±»é™¤äº†`load`æ–¹æ³•å¤–ï¼Œè¿˜æä¾›äº†å¦‚`aload`ï¼ˆå¼‚æ­¥åŠ è½½ï¼‰ã€`fetch_all`ï¼ˆå¹¶å‘å¸¦é€Ÿç‡é™åˆ¶åœ°è·å–å¤šä¸ªURLsï¼‰ã€`lazy_load`ï¼ˆæŒ‰éœ€åŠ è½½å†…å®¹ï¼‰ä»¥åŠ`scrape`æˆ–`scrape_all`ï¼ˆç”¨BeautifulSoupè·å–å’Œè§£æå†…å®¹ï¼‰ç­‰å¤šç§æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•å…è®¸åœ¨çµæ´»çš„ç½‘é¡µæŠ“å–å’Œå†…å®¹å¤„ç†åœºæ™¯ä¸­ä½¿ç”¨ã€‚

è¿™ä¸ªåŠ è½½å™¨æ˜¯LangChainæ–‡æ¡£åŠ è½½åŸºç¡€è®¾æ–½çš„ä¸€éƒ¨åˆ†ï¼Œæ—¨åœ¨ä¿ƒè¿›ç½‘é¡µå†…å®¹çš„æ£€ç´¢å’Œå¤„ç†ï¼Œä»¥ä¾›ä¸‹æ¸¸åº”ç”¨ä½¿ç”¨ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€æ•°æ®åˆ†ææˆ–è¾“å…¥åˆ°æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­ã€‚<br>

Next, we need to index it into a vectorstore. This requires a few components, namely an [embedding model](https://python.langchain.com/docs/modules/data_connection/text_embedding) and [a vectorstore](https://python.langchain.com/docs/modules/data_connection/vectorstores).<br>

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶ç´¢å¼•åˆ°ä¸€ä¸ªå‘é‡å­˜å‚¨ä¸­ã€‚è¿™éœ€è¦å‡ ä¸ªç»„ä»¶ï¼Œå³ä¸€ä¸ªåµŒå…¥æ¨¡å‹å’Œä¸€ä¸ªå‘é‡å­˜å‚¨ã€‚<br>

For embedding models, we once again provide examples for accessing via OpenAI or via local models.<br>

å¯¹äºåµŒå…¥æ¨¡å‹ï¼Œæˆ‘ä»¬å†æ¬¡æä¾›äº†é€šè¿‡ OpenAI æˆ–é€šè¿‡æœ¬åœ°æ¨¡å‹è®¿é—®çš„ä¾‹å­ã€‚<br>

Make sure you have the `langchain_openai` package installed an the appropriate environment variables set (these are the same as needed for the LLM).<br>

è¯·ç¡®ä¿ä½ å·²å®‰è£… `langchain_openai` åŒ…ï¼Œå¹¶è®¾ç½®äº†é€‚å½“çš„ç¯å¢ƒå˜é‡ï¼ˆè¿™äº›ä¸ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰€éœ€çš„ç¯å¢ƒå˜é‡ç›¸åŒï¼‰ã€‚<br>

```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()
```

Now, we can use this embedding model to ingest(æ‘„å…¥ï¼›å¯¼å…¥) documents into a vectorstore. We will use a simple local vectorstore, [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss), for simplicity's sake.<br>

> sake é€šå¸¸ç”¨äºè¡¨è¾¾åšæŸäº‹çš„åŸå› æˆ–ç›®çš„ã€‚ä¾‹å¦‚ï¼Œ"for simplicity's sake" æ„å‘³ç€ä¸ºäº†ç®€å•èµ·è§ï¼›"for the sake of clarity" æ„å‘³ç€ä¸ºäº†æ¸…æ™°èµ·è§ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªåµŒå…¥æ¨¡å‹å°†æ–‡æ¡£æ‘„å…¥åˆ°ä¸€ä¸ªå‘é‡å­˜å‚¨ä¸­ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªç®€å•çš„æœ¬åœ°å‘é‡å­˜å‚¨ï¼ŒFAISSã€‚<br>

First we need to install the required packages for that:<br>

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…æ‰€éœ€çš„åŒ…ï¼š<br>

```bash
pip install faiss-cpu
```

> faisså‘é‡åº“ç»è¿‡ `pip install faiss-cpu` åå°±å¯ä»¥ç”¨ï¼Œä¸éœ€è¦é¢å¤–å®‰è£…ã€‚

Then we can build our index:<br>

ç„¶åæˆ‘ä»¬å¯ä»¥æ„å»ºæˆ‘ä»¬çš„ç´¢å¼•ï¼š<br>

```python
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter

# "RecursiveCharacterTextSplitter"å¯ä»¥æ‹†è§£ä¸ºå‡ ä¸ªéƒ¨åˆ†æ¥ç†è§£ï¼š

# Recursiveï¼ˆé€’å½’çš„ï¼‰ã€Characterï¼ˆå­—ç¬¦ï¼‰ã€Textï¼ˆæ–‡æœ¬ï¼‰ã€Splitterï¼ˆåˆ†å‰²å™¨ï¼‰

# "RecursiveCharacterTextSplitter"å¤§è‡´æ„å‘³é€’å½’åœ°å°†æ–‡æœ¬æŒ‰å­—ç¬¦åˆ†å‰²ã€‚ä»ä¸€ä¸ªè¾ƒå¤§çš„æ–‡æœ¬å—å¼€å§‹ï¼Œå¹¶é€’å½’åœ°å°†å…¶åˆ†è§£ä¸ºå•ä¸ªå­—ç¬¦ï¼Œæ¯æ¬¡è¿­ä»£å¤„ç†æ›´å°çš„æ–‡æœ¬ç‰‡æ®µï¼Œç›´åˆ°æ•´ä¸ªæ–‡æœ¬è¢«ç»†åˆ†ä¸ºå•ç‹¬çš„å­—ç¬¦ã€‚

text_splitter = RecursiveCharacterTextSplitter()
documents = text_splitter.split_documents(docs)
vector = FAISS.from_documents(documents, embeddings)
```

ç‚¹å‡» `langchain_community.vectorstores` åå†…å®¹è¯¦è§£:<br>

**Vector store** stores embedded data and performs vector search.ï¼ˆ**å‘é‡å­˜å‚¨**ç”¨äºå­˜å‚¨åµŒå…¥æ•°æ®å¹¶æ‰§è¡Œå‘é‡æœç´¢ã€‚ï¼‰<br>

One of the most common(å¸¸è§çš„) ways to store and search over unstructured(éç»“æ„åŒ–) data is to embed it and store the resulting embedding vectors, and then query the store and retrieve(æ£€ç´¢) the data that are 'most similar'(æœ€ç›¸ä¼¼) to the embedded query.<br>

å­˜å‚¨å’Œæœç´¢éç»“æ„åŒ–æ•°æ®çš„æœ€å¸¸è§æ–¹æ³•ä¹‹ä¸€æ˜¯å°†å…¶å‘é‡åŒ–ï¼Œå­˜å‚¨è½¬åŒ–åçš„è¯å‘é‡ï¼Œç„¶åæŸ¥è¯¢å¹¶æ£€ç´¢ä¸åµŒå…¥æŸ¥è¯¢â€œæœ€ç›¸ä¼¼â€çš„æ•°æ®ã€‚<br>

**Class hierarchy(ç±»å±‚æ¬¡ç»“æ„):**

.. code-block(ä»£ç å—)::<br>

```python
VectorStore --> <name>  # Examples: Annoy, FAISS, Milvus

BaseRetriever --> VectorStoreRetriever --> <name>Retriever  # Example: VespaRetriever
```

**Main helpers(ä¸»è¦åŠ©æ‰‹):**<br>

.. code-block(ä»£ç å—)::<br>

```python
Embeddings, Document
```

Now that we have this data indexed(ç´¢å¼•åˆ°) in a vectorstore, we will create a retrieval chain. <br>

æ—¢ç„¶æˆ‘ä»¬å·²ç»å°†è¿™äº›æ•°æ®ç´¢å¼•åˆ°å‘é‡å­˜å‚¨ä¸­ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªæ£€ç´¢é“¾ã€‚<br>

This chain will take an incoming question, look up relevant documents, then pass those documents along with the original question into an LLM and ask it to answer the original question.<br>

è¯¥é“¾å°†æ¥æ”¶ä¸€ä¸ªè¾“å…¥çš„é—®é¢˜ï¼ŒæŸ¥æ‰¾ç›¸å…³æ–‡æ¡£ï¼Œç„¶åå°†è¿™äº›æ–‡æ¡£è¿åŒåŸå§‹é—®é¢˜ä¼ é€’ç»™ä¸€ä¸ªLLMï¼Œå¹¶è¦æ±‚å…¶å›ç­”åŸå§‹é—®é¢˜ã€‚<br>

First, let's set up the chain that takes a question and the retrieved documents and generates an answer.<br>

é¦–å…ˆï¼Œè®©æˆ‘ä»¬è®¾ç½®ä¸€ä¸ªé“¾ï¼Œè¯¥é“¾æ¥æ”¶ä¸€ä¸ªé—®é¢˜å’Œæ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªç­”æ¡ˆã€‚<br>

```python
from langchain.chains.combine_documents import create_stuff_documents_chain

prompt = ChatPromptTemplate.from_template("""Answer the following question based only on the provided context:

<context>
{context}
</context>

Question: {input}""")

document_chain = create_stuff_documents_chain(llm, prompt)
```

If we wanted to, we could run this ourselves by passing in documents directly:<br>

å¦‚æœæˆ‘ä»¬æ„¿æ„çš„è¯ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥é€šè¿‡ä¼ å…¥æ–‡æ¡£æ¥è‡ªè¡Œè¿è¡Œè¿™ä¸ªè¿‡ç¨‹ã€‚<br>

```python
from langchain_core.documents import Document

document_chain.invoke({
    "input": "how can langsmith help with testing?",
    "context": [Document(page_content="langsmith can let you visualize test results")]
})
```

However, we want the documents to first come from the retriever we just set up. That way, for a given question we can use the retriever to dynamically(åŠ¨æ€çš„) select the most relevant documents and pass those in.<br>

ç„¶è€Œï¼Œæˆ‘ä»¬å¸Œæœ›æ–‡æ¡£é¦–å…ˆæ¥è‡ªæˆ‘ä»¬åˆšåˆšè®¾ç½®çš„æ£€ç´¢å™¨ã€‚è¿™æ ·ï¼Œå¯¹äºç»™å®šçš„é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ£€ç´¢å™¨åŠ¨æ€åœ°é€‰æ‹©æœ€ç›¸å…³çš„æ–‡æ¡£å¹¶ä¼ é€’ç»™æ¨¡å‹ã€‚<br>

```python
from langchain.chains import create_retrieval_chain

retriever = vector.as_retriever()
retrieval_chain = create_retrieval_chain(retriever, document_chain)
```

We can now invoke this chain. This returns a dictionary - the response from the LLM is in the answer key.<br>

æˆ‘ä»¬ç°åœ¨å¯ä»¥è°ƒç”¨è¿™ä¸ªé“¾æ¡äº†ã€‚è¿™å°†è¿”å›ä¸€ä¸ªå­—å…¸ - LLMçš„å“åº”ä½äº `answer` é”®ä¸­ã€‚<br>

```python
response = retrieval_chain.invoke({"input": "how can langsmith help with testing?"})
print(response["answer"])

# LangSmith offers several features that can help with testing:...
```

This answer should be much more accurate!<br>

è¿™ä¸ªç­”æ¡ˆåº”è¯¥æ›´å‡†ç¡®ï¼<br>

#### ä½•æ—¶ä½¿ç”¨æ£€ç´¢é“¾:

LangChain æ”¯æŒæ£€ç´¢é“¾çš„åŠŸèƒ½æ˜¯ä¸ºäº†åœ¨å¤„ç†é—®é¢˜æ—¶æä¾›æ›´åŠ ä¸°å¯Œå’Œå‡†ç¡®çš„ä¿¡æ¯æ¥æºã€‚æ£€ç´¢é“¾å…è®¸ç³»ç»Ÿé€šè¿‡æ£€ç´¢å¤–éƒ¨æ•°æ®æºæ¥å¢å¼ºå…¶å›ç­”çš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚ä½ å†³å®šä½•æ—¶ä½¿ç”¨æ£€ç´¢é“¾è€Œä¸æ˜¯ç›´æ¥å›å¤ï¼Œå¯ä»¥åŸºäºä»¥ä¸‹å‡ ä¸ªè€ƒè™‘å› ç´ ï¼š<br>

1. **é—®é¢˜çš„å¤æ‚æ€§å’Œç‰¹å®šæ€§**ï¼šå¦‚æœé—®é¢˜éå¸¸å¤æ‚æˆ–éœ€è¦ç‰¹å®šé¢†åŸŸçš„æ·±å…¥çŸ¥è¯†ï¼Œä½¿ç”¨æ£€ç´¢é“¾å¯èƒ½ä¼šæ›´å¥½ï¼Œå› ä¸ºå®ƒå¯ä»¥è®¿é—®æœ€æ–°çš„æˆ–æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚

2. **ä¿¡æ¯çš„æ—¶æ•ˆæ€§**ï¼šå¯¹äºéœ€è¦æœ€æ–°ä¿¡æ¯çš„é—®é¢˜ï¼ˆå¦‚æ–°é—»äº‹ä»¶ã€æœ€è¿‘çš„ç§‘å­¦å‘ç°ç­‰ï¼‰ï¼Œä½¿ç”¨æ£€ç´¢é“¾å¯ä»¥å¸®åŠ©è·å–æœ€æ–°æ•°æ®ã€‚

3. **å¯ç”¨æ•°æ®çš„é™åˆ¶**ï¼šå¦‚æœé—®é¢˜æ¶‰åŠåˆ°çš„ä¿¡æ¯å¯èƒ½ä¸åŒ…å«åœ¨æ¨¡å‹è®­ç»ƒæ•°æ®ä¸­ï¼Œæˆ–è€…ä¿¡æ¯å·²ç»è¿‡æ—¶ï¼Œé‚£ä¹ˆä½¿ç”¨æ£€ç´¢é“¾å¯ä»¥è®¿é—®æœ€æ–°å’Œæœ€ç›¸å…³çš„å¤–éƒ¨èµ„æºã€‚

4. **ç²¾ç¡®åº¦å’Œå¯é æ€§çš„è¦æ±‚**ï¼šå¯¹äºéœ€è¦é«˜åº¦å‡†ç¡®å’Œå¯é ç­”æ¡ˆçš„æƒ…å†µï¼ˆæ¯”å¦‚åŒ»ç–—ã€æ³•å¾‹å’¨è¯¢ç­‰ï¼‰ï¼Œæ£€ç´¢é“¾å¯ä»¥é€šè¿‡è®¿é—®æƒå¨æ•°æ®æºæ¥æä¾›æ”¯æŒã€‚

5. **ç”¨æˆ·çš„åå¥½**ï¼šæœ‰æ—¶å€™ï¼Œç”¨æˆ·å¯èƒ½æ›´å€¾å‘äºè·å–ç»è¿‡éªŒè¯çš„ä¿¡æ¯æºæä¾›çš„ç­”æ¡ˆï¼Œè€Œä¸æ˜¯ç›´æ¥ä»æ¨¡å‹ç”Ÿæˆçš„å›ç­”ã€‚

6. **ç”¨æˆ·æ˜¯å¦ä¸Šä¼ è¶…å¤§å‹æ–‡ä»¶**: å¦‚æœç”¨æˆ·ä¸Šä¼ è¶…å¤§å‹æ–‡ä»¶ï¼Œå¯ä»¥é‡‡ç”¨å…ˆå¯¹è¯¥æ–‡ä»¶åˆ‡åˆ†ã€å‘é‡åŒ–å¹¶å­˜å‚¨åˆ°å‘é‡åº“æ“ä½œï¼Œåç»­è°ƒç”¨æ£€ç´¢é“¾æ£€ç´¢ã€‚

åœ¨å†³å®šæ˜¯å¦ä½¿ç”¨æ£€ç´¢é“¾æ—¶ï¼Œè¿˜è¦è€ƒè™‘æ£€ç´¢è¿‡ç¨‹å¯èƒ½å¢åŠ çš„æ—¶é—´å»¶è¿Ÿå’Œèµ„æºæ¶ˆè€—ã€‚ç›´æ¥å›å¤é€šå¸¸æ›´å¿«ï¼Œä½†å¯èƒ½ä¸å¦‚æ£€ç´¢é“¾å›å¤çš„ä¿¡æ¯å…¨é¢æˆ–å‡†ç¡®ã€‚<br>

ğŸš€ğŸš€ğŸš€ **æœ€å¥½çš„æƒ…å†µæ˜¯ï¼Œæ™®é€šçŠ¶æ€ä¸‹è®©å¤§æ¨¡å‹ç›´æ¥å›å¤ï¼Œç”¨æˆ·ä¸Šä¼ è¶…å¤§å‹æ–‡ä»¶(å¯ä»¥é€šè¿‡è®¡ç®—tokenåˆ¤æ–­)åé‡‡ç”¨æ£€ç´¢é“¾ã€‚** <br>

æ€»çš„æ¥è¯´ï¼Œå½“é¢å¯¹éœ€è¦é«˜åº¦ä¸“ä¸šçŸ¥è¯†ã€æœ€æ–°ä¿¡æ¯ã€æˆ–è€…å¯¹ç­”æ¡ˆå‡†ç¡®æ€§å’Œå¯é æ€§æœ‰é«˜è¦æ±‚çš„é—®é¢˜æ—¶ï¼Œä½¿ç”¨æ£€ç´¢é“¾ä¼šæ˜¯ä¸€ä¸ªå¥½é€‰æ‹©ã€‚å¯¹äºä¸€èˆ¬æ€§çš„é—®é¢˜ï¼Œç›´æ¥å›å¤å¯èƒ½æ›´ä¸ºé«˜æ•ˆå’Œåˆé€‚ã€‚<br>

#### æ£€ç´¢é“¾å®Œæ•´ä»£ç :

```python
"""
@author:PeilongChen(peilongchencc@163.com)
@description:å®ç°åŸºäºLangChainçš„æ–‡æ¡£æ£€ç´¢é“¾ã€‚
"""
import os
from dotenv import load_dotenv
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from load_file_split_documents import ChineseRecursiveTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_openai import ChatOpenAI
from langchain.chains import create_retrieval_chain

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv_path = '.env.local'
load_dotenv(dotenv_path=dotenv_path)

# è®¾ç½®ç½‘ç»œä»£ç†ç¯å¢ƒå˜é‡
os.environ['http_proxy'] = 'http://127.0.0.1:7890'
os.environ['https_proxy'] = 'http://127.0.0.1:7890'


########################################################################
# æ–‡æ¡£åˆ‡åˆ†ï¼Œè¿›è¡Œå‘é‡åŒ–ï¼Œå¹¶å­˜å…¥FAISS
########################################################################

chunk_overlap = 50
chunk_size = 500
# æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„
filepath = 'example_data.txt'
# ä½¿ç”¨LangChainå†…ç½®txtæ–‡ä»¶åŠ è½½å™¨
loader = TextLoader(filepath)
# ä½¿ç”¨åŠ è½½å™¨åŠ è½½æ–‡æ¡£
docs = loader.load()    # æ•°æ®ç±»å‹ä¸ºlist [(page_content='ï¼ˆä¸€ï¼‰ç›´æ¥æ‰“å‹å¼\næ´—ç›˜\nç›´æ¥æ‰“å‹è¾ƒå¤šå‡ºç°åœ¨ åº„å®¶ å¸è´§åŒºåŸŸï¼Œç›®çš„æ˜¯... metadata={'source': 'example_data.txt'}' metadata={'source': 'example_data.txt'})]
# è°ƒç”¨OpenAIçš„Embeddings API
embeddings = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
# å®ä¾‹åŒ–è‡ªå®šä¹‰çš„æ–‡æœ¬åˆ†å‰²å™¨
text_splitter = ChineseRecursiveTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
# è¿›è¡Œæ–‡æœ¬åˆ†å‰²
documents = text_splitter.split_documents(docs)
vector = FAISS.from_documents(documents, embeddings)

########################################################################
# è°ƒç”¨å¤§æ¨¡å‹(æ¥å£)ï¼Œæ„å»ºprompt
########################################################################

llm = ChatOpenAI(
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    # model_name="gpt-4-0125-preview"
    )
# è®©æ¨¡å‹ "ä»…æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”ä»¥ä¸‹é—®é¢˜"
prompt = ChatPromptTemplate.from_template("""Answer the following question based only on the provided context:

<context>
{context}
</context>

Question: {input}""")

document_chain = create_stuff_documents_chain(llm, prompt)  # "input" å’Œ "context" å‚æ•°ä¼šä» `.invoke()` çš„å‚æ•°ä¸­è·å–ã€‚

########################################################################
# æ„å»ºæ£€ç´¢é“¾
########################################################################

retriever = vector.as_retriever()
retrieval_chain = create_retrieval_chain(retriever, document_chain)

response = retrieval_chain.invoke({"input": "æ¨ªç›˜æ•´ç†çš„å½¢æ€æ˜¯ä»€ä¹ˆæ ·å­?"})

# print(response)

print(response["answer"])

# ç”±äºç»“æœå«æœ‰éšæœºæ€§ï¼Œç¬”è€…ç»ˆç«¯è§åˆ°çš„2ç§å›å¤å¦‚ä¸‹:
# å›å¤1: æ¨ªç›˜æ•´ç†çš„å½¢æ€åœ¨Kçº¿ä¸Šçš„è¡¨ç°å¸¸å¸¸æ˜¯ä¸€æ¡æ¨ªçº¿æˆ–è€…é•¿æœŸçš„å¹³å°ã€‚
# å›å¤2: æ¨ªç›˜æ•´ç†çš„å½¢æ€åœ¨Kçº¿ä¸Šçš„è¡¨ç°å¸¸å¸¸æ˜¯ä¸€æ¡æ¨ªçº¿æˆ–è€…é•¿æœŸçš„å¹³å°ï¼Œä»æˆäº¤é‡ä¸Šæ¥çœ‹ï¼Œåœ¨å¹³å°æ•´ç†çš„è¿‡ç¨‹ä¸­æˆäº¤é‡å‘ˆé€’å‡çš„çŠ¶æ€ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨å¹³å°ä¸Šæ²¡æœ‰æˆ–å¾ˆå°‘æœ‰æˆäº¤é‡æ”¾å‡ºã€‚æˆäº¤æ¸…æ·¡ï¼Œæˆäº¤ä»·æ ¼ä¹Ÿæåº¦ä¸æ´»è·ƒã€‚
```

`invoke()`å‡½æ•°çš„å†…éƒ¨è§£é‡Šå¦‚ä¸‹:<br>

```txt
"""Transform a single input into an output. Override to implement.

Args:
    input: The input to the runnable.
    config: A config to use when invoking the runnable.
        The config supports standard keys like 'tags', 'metadata' for tracing
        purposes, 'max_concurrency' for controlling how much work to do
        in parallel, and other keys. Please refer to the RunnableConfig
        for more details.

Returns:
    The output of the runnable.
"""
```

æ„æ€æ˜¯:<br>

```txt
å°†å•ä¸€è¾“å…¥è½¬æ¢ä¸ºè¾“å‡ºã€‚é‡å†™ä»¥å®ç°ã€‚

å‚æ•°ï¼š
- inputï¼šè¿è¡Œå¯¹è±¡çš„è¾“å…¥ã€‚
- configï¼šè°ƒç”¨è¿è¡Œå¯¹è±¡æ—¶ä½¿ç”¨çš„é…ç½®ã€‚è¯¥é…ç½®æ”¯æŒå¦‚'tags'ã€'metadata'ç­‰æ ‡å‡†é”®ç”¨äºè¿½è¸ªç›®çš„ï¼Œ'max_concurrency'ç”¨äºæ§åˆ¶å¹¶è¡Œå·¥ä½œçš„é‡ï¼Œä»¥åŠå…¶ä»–é”®ã€‚è¯·å‚é˜…RunnableConfigä»¥è·å–æ›´å¤šè¯¦æƒ…ã€‚

è¿”å›å€¼ï¼š
- è¿è¡Œå¯¹è±¡çš„è¾“å‡ºã€‚
```

`as_retriever`çš„ç”¨æ³•å¦‚ä¸‹:<br>

```python
def as_retriever(self, **kwargs: Any) -> VectorStoreRetriever:
    """Return VectorStoreRetriever initialized from this VectorStore.
    Args:
        search_type (Optional[str]): Defines the type of search that
            the Retriever should perform.
            Can be "similarity" (default), "mmr", or
            "similarity_score_threshold".
        search_kwargs (Optional[Dict]): Keyword arguments to pass to the
            search function. Can include things like:
                k: Amount of documents to return (Default: 4)
                score_threshold: Minimum relevance threshold
                    for similarity_score_threshold
                fetch_k: Amount of documents to pass to MMR algorithm (Default: 20)
                lambda_mult: Diversity of results returned by MMR;
                    1 for minimum diversity and 0 for maximum. (Default: 0.5)
                filter: Filter by document metadata

    Returns:
        VectorStoreRetriever: Retriever class for VectorStore.

    Examples:

    .. code-block:: python

        # Retrieve more documents with higher diversity
        # Useful if your dataset has many similar documents
        docsearch.as_retriever(
            search_type="mmr",
            search_kwargs={'k': 6, 'lambda_mult': 0.25}
        )

        # Fetch more documents for the MMR algorithm to consider
        # But only return the top 5
        docsearch.as_retriever(
            search_type="mmr",
            search_kwargs={'k': 5, 'fetch_k': 50}
        )

        # Only retrieve documents that have a relevance score
        # Above a certain threshold
        docsearch.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={'score_threshold': 0.8}
        )

        # Only get the single most similar document from the dataset
        docsearch.as_retriever(search_kwargs={'k': 1})

        # Use a filter to only retrieve documents from a specific paper
        docsearch.as_retriever(
            search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}
        )
    """
    tags = kwargs.pop("tags", None) or []
    tags.extend(self._get_retriever_tags())
    return VectorStoreRetriever(vectorstore=self, **kwargs, tags=tags)
```

#### å…³äºè¿”å›å€¼çš„é‡è¦æé†’:

è°ƒç”¨ä¸Šè¿°æ£€ç´¢é“¾ä»£ç æ—¶ï¼Œæ˜¯æ— æ³•è¿”å›åˆ†æ•°çš„(LangChainé»˜è®¤ä½¿ç”¨çš„L2æ¬§æ°è·ç¦»)ã€‚å¦‚æœä½ æƒ³è¿”å›åˆ†æ•°ï¼Œéœ€è¦è‡ªå·±ä¿®æ”¹LangChainä¸­çš„å†…å®¹ï¼Œç›¸å…³ä»£ç å¦‚ä¸‹:<br>

```python
class FAISS(VectorStore):
    """`Meta Faiss` vector store.

    To use, you must have the ``faiss`` python package installed.

    Example:
        .. code-block:: python

            from langchain_community.embeddings.openai import OpenAIEmbeddings
            from langchain_community.vectorstores import FAISS

            embeddings = OpenAIEmbeddings()
            texts = ["FAISS is an important library", "LangChain supports FAISS"]
            faiss = FAISS.from_texts(texts, embeddings)

    """

    def __init__(
        self,
        embedding_function: Union[
            Callable[[str], List[float]],
            Embeddings,
        ],
        index: Any,
        docstore: Docstore,
        index_to_docstore_id: Dict[int, str],
        relevance_score_fn: Optional[Callable[[float], float]] = None,
        normalize_L2: bool = False,
        distance_strategy: DistanceStrategy = DistanceStrategy.EUCLIDEAN_DISTANCE,
    ):
        """Initialize with necessary components."""
        if not isinstance(embedding_function, Embeddings):
            logger.warning(
                "`embedding_function` is expected to be an Embeddings object, support "
                "for passing in a function will soon be removed."
            )
        self.embedding_function = embedding_function
        self.index = index
        self.docstore = docstore
        self.index_to_docstore_id = index_to_docstore_id
        self.distance_strategy = distance_strategy
        self.override_relevance_score_fn = relevance_score_fn
        self._normalize_L2 = normalize_L2
        if (
            self.distance_strategy != DistanceStrategy.EUCLIDEAN_DISTANCE
            and self._normalize_L2
        ):
            warnings.warn(
                "Normalizing L2 is not applicable for metric type: {strategy}".format(
                    strategy=self.distance_strategy
                )
            )

    # å…¶ä»–ä»£ç çœç•¥

    def similarity_search(
        self,
        query: str,
        k: int = 4,
        filter: Optional[Union[Callable, Dict[str, Any]]] = None,
        fetch_k: int = 20,
        **kwargs: Any,
    ) -> List[Document]:
        """Return docs most similar to query.

        Args:
            query: Text to look up documents similar to.
            k: Number of Documents to return. Defaults to 4.
            filter: (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.
            fetch_k: (Optional[int]) Number of Documents to fetch before filtering.
                      Defaults to 20.

        Returns:
            List of Documents most similar to the query.
        """
        docs_and_scores = self.similarity_search_with_score(
            query, k, filter=filter, fetch_k=fetch_k, **kwargs
        )
        return [doc for doc, _ in docs_and_scores]
```

å…·ä½“çš„æ–‡ä»¶è·¯å¾„ç±»ä¼¼å¦‚ä¸‹:<br>

```bash
/root/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py
```

ç”±ä¸Šè¿°ä»£ç å¯ä»¥çœ‹å‡ºï¼Œ`FAISS` ç±»ä¸­å‡½æ•° `similarity_search` çš„è¿”å›å€¼åªè¿”å›äº†æ–‡æ¡£docã€‚å¦‚æœä½ æƒ³ä¿®æ”¹è¿”å›å€¼ï¼Œå¯ä»¥ä»è¿™é‡Œä¸‹æ‰‹ã€‚<br>

ä¹Ÿå¯é‡‡ç”¨ä¸‹åˆ—æ–¹å¼ç»§æ‰¿ `FAISS` ç±»ï¼Œé‡å†™å‡½æ•° `similarity_search`:<br>

```python
class ExtendedFAISS(FAISS):
    def similarity_search(
        self,
        query: str,
        k: int = 4,
        filter: Optional[Union[Callable, Dict[str, Any]]] = None,
        fetch_k: int = 20,
        **kwargs: Any,
    ) -> List[Tuple[Document, float]]:
        """
        é‡å†™similarity_searchæ–¹æ³•ä»¥è¿”å›æ–‡æ¡£åŠå…¶åˆ†æ•°ã€‚

        å‚æ•°:
            query: è¦æŸ¥æ‰¾ç›¸ä¼¼æ–‡æ¡£çš„æ–‡æœ¬ã€‚
            k: è¿”å›çš„æ–‡æ¡£æ•°é‡ï¼Œé»˜è®¤ä¸º4ã€‚
            filter: å¯é€‰çš„ï¼Œé€šè¿‡å…ƒæ•°æ®è¿‡æ»¤ï¼Œé»˜è®¤ä¸ºNoneã€‚
            fetch_k: å¯é€‰çš„ï¼Œåœ¨è¿‡æ»¤å‰è·å–çš„æ–‡æ¡£æ•°é‡ï¼Œé»˜è®¤ä¸º20ã€‚
            **kwargs: é¢å¤–çš„å…³é”®å­—å‚æ•°ã€‚

        è¿”å›:
            ä¸€ä¸ªå…ƒç»„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å«ä¸€ä¸ªæ–‡æ¡£å’Œå…¶L2è·ç¦»åˆ†æ•°ã€‚
            åˆ†æ•°è¶Šä½è¡¨ç¤ºç›¸ä¼¼åº¦è¶Šé«˜ã€‚
        """
        # è°ƒç”¨ç°æœ‰çš„similarity_search_with_scoreæ–¹æ³•è·å–æ–‡æ¡£åŠå…¶åˆ†æ•°ã€‚
        return self.similarity_search_with_score(
            query, k, filter=filter, fetch_k=fetch_k, **kwargs
        )
```

ğŸš¨ğŸš¨ğŸš¨æ³¨æ„: **è¿™ä¸¤ç§æ–¹å¼éƒ½éœ€è¦è€ƒè™‘æ•°æ®çš„ä¼ é€’ï¼Œéœ€è¦ä½ è‡ªå·±debugåç»­ä»£ç ï¼Œä¿è¯ç¨‹åºæ­£å¸¸è¿è¡Œã€æ–‡æ¡£å¾—åˆ†å¯ä»¥æ­£å¸¸è¿”å›ã€‚** <br>

#### Diving Deeper(æ·±å…¥äº†è§£):

We've now successfully set up a basic retrieval chain. We only touched on the basics of retrieval - for a deeper dive into everything mentioned here, see [this section of documentation.](https://python.langchain.com/docs/modules/data_connection)<br>

æˆ‘ä»¬å·²ç»æˆåŠŸå»ºç«‹äº†ä¸€ä¸ªåŸºæœ¬çš„æ£€ç´¢é“¾ã€‚æˆ‘ä»¬åªæ˜¯ç®€å•ä»‹ç»äº†æ£€ç´¢çš„åŸºç¡€çŸ¥è¯† - æ¬²æ·±å…¥äº†è§£æœ¬æ–‡ä¸­æåˆ°çš„æ‰€æœ‰å†…å®¹ï¼Œè¯·å‚é˜…æ–‡æ¡£ä¸­çš„è¿™ä¸€éƒ¨åˆ†ã€‚<br>


### Conversation Retrieval Chain(å¯¹è¯æ£€ç´¢é“¾):

The chain we've created so far can only answer single questions. One of the main types of LLM applications that people are building are chat bots. So how do we turn this chain into one that **can answer follow up questions?** <br>

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬åˆ›å»ºçš„é“¾åªèƒ½å›ç­”å•ä¸ªé—®é¢˜ã€‚äººä»¬æ­£åœ¨æ„å»ºçš„LLMåº”ç”¨çš„ä¸»è¦ç±»å‹ä¹‹ä¸€æ˜¯èŠå¤©æœºå™¨äººã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•å°†è¿™ä¸ªé“¾è½¬å˜ä¸ºä¸€ä¸ªå¯ä»¥ **å›ç­”åç»­é—®é¢˜çš„é“¾å‘¢ï¼Ÿ** <br>

We can still use the `create_retrieval_chain` function, but we need to change two things:<br>

æˆ‘ä»¬ä»ç„¶å¯ä»¥ä½¿ç”¨ `create_retrieval_chain` å‡½æ•°ï¼Œä½†æˆ‘ä»¬éœ€è¦æ”¹å˜ä¸¤ä»¶äº‹ï¼š<br>

1. The retrieval method should now not just work on the most recent input, but rather should take the whole history into account.(æ£€ç´¢æ–¹æ³•ç°åœ¨ä¸ä»…ä»…åº”è¯¥åœ¨æœ€è¿‘çš„è¾“å…¥ä¸Šå·¥ä½œï¼Œè€Œæ˜¯åº”è¯¥è€ƒè™‘æ•´ä¸ªå†å²è®°å½•ã€‚)

2. The final LLM chain should likewise(adv. åŒæ ·åœ°) take the whole history into account.(æœ€ç»ˆçš„LLMé“¾åŒæ ·åº”è¯¥è€ƒè™‘æ•´ä¸ªå†å²è®°å½•ã€‚)

> åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­ï¼Œâ€œaccountâ€å¹¶ä¸æ˜¯æŒ‡â€œè´¦æˆ·â€çš„æ„æ€ï¼Œè€Œæ˜¯æŒ‡â€œè€ƒè™‘åˆ°ï¼›å°†...çº³å…¥è€ƒè™‘èŒƒå›´â€çš„æ„æ€ã€‚æ¢å¥è¯è¯´ï¼Œå°†æ•´ä¸ªå†å²è®°å½•è€ƒè™‘åœ¨å†…ï¼Œå°†å…¶çº³å…¥åˆ°ç›¸å…³å¤„ç†æˆ–å†³ç­–ä¸­ã€‚

#### Updating Retrieval(æ›´æ–°æ£€ç´¢):

In order to update retrieval, we will create a new chain. This chain will take in the most recent input (`input`) and the conversation history (`chat_history`) and use an LLM to generate a search query.<br>

ä¸ºäº†æ›´æ–°æ£€ç´¢ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªæ–°çš„é“¾ã€‚è¿™ä¸ªé“¾å°†æ¥æ”¶æœ€è¿‘çš„è¾“å…¥ï¼ˆ`input`ï¼‰å’Œå¯¹è¯å†å²è®°å½•ï¼ˆ`chat_history`ï¼‰ï¼Œå¹¶ä½¿ç”¨LLMç”Ÿæˆä¸€ä¸ªæœç´¢æŸ¥è¯¢ã€‚<br>

```python
from langchain.chains import create_history_aware_retriever
from langchain_core.prompts import MessagesPlaceholder

# First we need a prompt that we can pass into an LLM to generate this search query
# é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå¯ä»¥ä¼ é€’ç»™å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥ç”Ÿæˆè¿™ä¸ªæœç´¢æŸ¥è¯¢çš„æç¤º(prompt)ã€‚

prompt = ChatPromptTemplate.from_messages([
    MessagesPlaceholder(variable_name="chat_history"),
    ("user", "{input}"),
    ("user", "Given the above conversation, generate a search query to look up in order to get information relevant to the conversation")
])

# Given the above conversation, generate a search query to look up in order to get information relevant to the conversation
# æ ¹æ®ä»¥ä¸Šå¯¹è¯ï¼Œç”Ÿæˆä¸€ä¸ªæœç´¢æŸ¥è¯¢ä»¥è·å–ä¸å¯¹è¯ç›¸å…³çš„ä¿¡æ¯

retriever_chain = create_history_aware_retriever(llm, retriever, prompt)
```

We can test this out by passing in an instance where the user is asking a follow up question.<br>

æˆ‘ä»¬å¯ä»¥é€šè¿‡æä¾›ä¸€ä¸ªç”¨æˆ·æ­£åœ¨è¯¢é—®åç»­é—®é¢˜çš„å®ä¾‹æ¥è¿›è¡Œæµ‹è¯•ã€‚<br>

```python
from langchain_core.messages import HumanMessage, AIMessage

chat_history = [HumanMessage(content="Can LangSmith help test my LLM applications?"), AIMessage(content="Yes!")]
retriever_chain.invoke({
    "chat_history": chat_history,
    "input": "Tell me how"
})
```

You should see that this returns documents about testing in LangSmith. This is because the LLM generated a new query, combining the chat history with the follow up question.<br>

ä½ åº”è¯¥ä¼šçœ‹åˆ°ï¼Œè¿™å°†è¿”å› "å…³äºåœ¨LangSmithè¿›è¡Œæµ‹è¯•" çš„æ–‡ä»¶(æ•°æ®ç±»å‹ç±»ä¼¼ `Document(page_content='æ¨ªç›˜æ•´ç†)` )ã€‚è¿™æ˜¯å› ä¸ºLLMç”Ÿæˆäº†ä¸€ä¸ªæ–°çš„æŸ¥è¯¢ï¼Œå°†èŠå¤©è®°å½•ä¸åç»­é—®é¢˜ç»“åˆèµ·æ¥ã€‚<br>

#### æµ‹è¯•ä»£ç å¦‚ä¸‹:

```python
"""
@author:PeilongChen(peilongchencc@163.com)
@description:å®ç°åŸºäºLangChainçš„æ–‡æ¡£æ£€ç´¢é“¾ã€‚
"""
import os
from dotenv import load_dotenv
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from load_file_split_documents import ChineseRecursiveTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_openai import ChatOpenAI
from langchain.chains import create_retrieval_chain
from langchain_core.messages import HumanMessage, AIMessage
from langchain.chains import create_history_aware_retriever
from langchain_core.prompts import MessagesPlaceholder

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv_path = '.env.local'
load_dotenv(dotenv_path=dotenv_path)

# è®¾ç½®ç½‘ç»œä»£ç†ç¯å¢ƒå˜é‡
os.environ['http_proxy'] = 'http://127.0.0.1:7890'
os.environ['https_proxy'] = 'http://127.0.0.1:7890'


########################################################################
# æ–‡æ¡£åˆ‡åˆ†ï¼Œè¿›è¡Œå‘é‡åŒ–ï¼Œå¹¶å­˜å…¥FAISS
########################################################################

chunk_overlap = 50
chunk_size = 500
# æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„
filepath = 'example_data.txt'
# ä½¿ç”¨LangChainå†…ç½®txtæ–‡ä»¶åŠ è½½å™¨
loader = TextLoader(filepath)
# ä½¿ç”¨åŠ è½½å™¨åŠ è½½æ–‡æ¡£
docs = loader.load()    # æ•°æ®ç±»å‹ä¸ºlist [(page_content='ï¼ˆä¸€ï¼‰ç›´æ¥æ‰“å‹å¼\næ´—ç›˜\nç›´æ¥æ‰“å‹è¾ƒå¤šå‡ºç°åœ¨ åº„å®¶ å¸è´§åŒºåŸŸï¼Œç›®çš„æ˜¯... metadata={'source': 'example_data.txt'}' metadata={'source': 'example_data.txt'})]
# è°ƒç”¨OpenAIçš„Embeddings API
embeddings = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
# å®ä¾‹åŒ–è‡ªå®šä¹‰çš„æ–‡æœ¬åˆ†å‰²å™¨
text_splitter = ChineseRecursiveTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
# è¿›è¡Œæ–‡æœ¬åˆ†å‰²
documents = text_splitter.split_documents(docs)
vector = FAISS.from_documents(documents, embeddings)

########################################################################
# è°ƒç”¨å¤§æ¨¡å‹(æ¥å£)ï¼Œæ„å»ºprompt
########################################################################

llm = ChatOpenAI(
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    # model_name="gpt-4-0125-preview"
    )

prompt = ChatPromptTemplate.from_messages([
    MessagesPlaceholder(variable_name="chat_history"),
    ("user", "{input}"),
    ("user", "Given the above conversation, generate a search query to look up in order to get information relevant to the conversation")
])

chat_history = [HumanMessage(content="æ¨ªç›˜æ•´ç†çš„å½¢æ€æ˜¯ä»€ä¹ˆæ ·å­?"), AIMessage(content="Yes!")]

########################################################################
# æ„å»ºæ£€ç´¢é“¾
########################################################################

retriever = vector.as_retriever()

retriever_chain = create_history_aware_retriever(llm, retriever, prompt)

response = retriever_chain.invoke({
    "chat_history": chat_history,
    "input": "ä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ç§æƒ…å†µå‘¢ï¼Ÿ"
})

for each_res in response:
    print(each_res.page_content)    # ç›¸å…³æ–‡æ¡£çš„å…·ä½“å†…å®¹
```

Now that we have this new retriever, we can create a new chain to continue the conversation with these retrieved documents in mind.<br>

ç°åœ¨æˆ‘ä»¬æœ‰äº†è¿™ä¸ªæ–°çš„æ£€ç´¢å™¨ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªæ–°çš„é“¾ï¼Œç»§ç»­è€ƒè™‘è¿™äº›æ£€ç´¢åˆ°çš„æ–‡ä»¶æ¥ç»§ç»­å¯¹è¯ã€‚<br>

```python
prompt = ChatPromptTemplate.from_messages([
    ("system", "Answer the user's questions based on the below context:\n\n{context}"),
    MessagesPlaceholder(variable_name="chat_history"),
    ("user", "{input}"),
])
document_chain = create_stuff_documents_chain(llm, prompt)

retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)
```

We can now test this out end-to-end:<br>

ç°åœ¨æˆ‘ä»¬å¯ä»¥è¿›è¡Œç«¯åˆ°ç«¯çš„æµ‹è¯•ï¼š<br>

```python
chat_history = [HumanMessage(content="Can LangSmith help test my LLM applications?"), AIMessage(content="Yes!")]
retrieval_chain.invoke({
    "chat_history": chat_history,
    "input": "Tell me how"
})
```

We can see that this gives a coherent answer - we've successfully turned our retrieval chain into a chatbot!<br>

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™æä¾›äº†ä¸€ä¸ªè¿è´¯çš„ç­”æ¡ˆ - æˆ‘ä»¬æˆåŠŸåœ°å°†æˆ‘ä»¬çš„æ£€ç´¢é“¾è½¬åŒ–ä¸ºäº†ä¸€ä¸ªèŠå¤©æœºå™¨äººï¼<br>

#### å¯¹è¯æ£€ç´¢é“¾å®Œæ•´ä»£ç :

```python
"""
@author:PeilongChen(peilongchencc@163.com)
@description:å®ç°åŸºäºLangChainçš„æ–‡æ¡£æ£€ç´¢é“¾ã€‚
"""
import os
from dotenv import load_dotenv
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from load_file_split_documents import ChineseRecursiveTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_openai import ChatOpenAI
from langchain.chains import create_retrieval_chain
from langchain_core.messages import HumanMessage, AIMessage
from langchain.chains import create_history_aware_retriever
from langchain_core.prompts import MessagesPlaceholder

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv_path = '.env.local'
load_dotenv(dotenv_path=dotenv_path)

# è®¾ç½®ç½‘ç»œä»£ç†ç¯å¢ƒå˜é‡
os.environ['http_proxy'] = 'http://127.0.0.1:7890'
os.environ['https_proxy'] = 'http://127.0.0.1:7890'


########################################################################
# æ–‡æ¡£åˆ‡åˆ†ï¼Œè¿›è¡Œå‘é‡åŒ–ï¼Œå¹¶å­˜å…¥FAISS
########################################################################

chunk_overlap = 50
chunk_size = 500
# æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„
filepath = 'example_data.txt'
# ä½¿ç”¨LangChainå†…ç½®txtæ–‡ä»¶åŠ è½½å™¨
loader = TextLoader(filepath)
# ä½¿ç”¨åŠ è½½å™¨åŠ è½½æ–‡æ¡£
docs = loader.load()    # æ•°æ®ç±»å‹ä¸ºlist [(page_content='ï¼ˆä¸€ï¼‰ç›´æ¥æ‰“å‹å¼\næ´—ç›˜\nç›´æ¥æ‰“å‹è¾ƒå¤šå‡ºç°åœ¨ åº„å®¶ å¸è´§åŒºåŸŸï¼Œç›®çš„æ˜¯... metadata={'source': 'example_data.txt'}' metadata={'source': 'example_data.txt'})]
# è°ƒç”¨OpenAIçš„Embeddings API
embeddings = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
# å®ä¾‹åŒ–è‡ªå®šä¹‰çš„æ–‡æœ¬åˆ†å‰²å™¨
text_splitter = ChineseRecursiveTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
# è¿›è¡Œæ–‡æœ¬åˆ†å‰²
documents = text_splitter.split_documents(docs)
vector = FAISS.from_documents(documents, embeddings)
retriever = vector.as_retriever()

########################################################################
# è°ƒç”¨å¤§æ¨¡å‹(æ¥å£)ï¼Œæ„å»ºprompt
########################################################################

llm = ChatOpenAI(
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    # model_name="gpt-4-0125-preview"
    )

########################################################################
# ç¬¬ä¸€æ¬¡è°ƒç”¨llmï¼Œå°†å¤šè½®å¯¹è¯ä¸­å½“å‰é—®å¥çš„ä¿¡æ¯è¡¥å……å®Œæ•´(å¦‚æœå½“å‰é—®å¥ç¼ºå°‘ä¿¡æ¯çš„è¯ï¼Œä¾‹å¦‚ç¼ºä¸»è¯­ç­‰ç­‰ã€‚)
########################################################################

prompt = ChatPromptTemplate.from_messages([
    MessagesPlaceholder(variable_name="chat_history"),
    ("user", "{input}"),
    ("user", "Given the above conversation, generate a search query to look up in order to get information relevant to the conversation")
])
# æ„å»ºæ£€ç´¢å™¨é“¾, "create history aware retriever" è¡¨ç¤ºåˆ›å»ºä¸€ä¸ªå…·æœ‰å†å²æ„è¯†çš„æ£€ç´¢å™¨,awareè¡¨ç¤ºæ„è¯†åˆ°æˆ–çŸ¥é“æŸç§æƒ…å†µã€‚
retriever_chain = create_history_aware_retriever(llm, retriever, prompt)

########################################################################
# ç¬¬äºŒæ¬¡è°ƒç”¨llmï¼Œåˆ©ç”¨åˆšåˆšç”Ÿæˆçš„å«å®Œæ•´ä¿¡æ¯çš„é—®å¥ä»æ–‡æ¡£ä¸­æ£€ç´¢éœ€è¦çš„å†…å®¹
########################################################################

prompt = ChatPromptTemplate.from_messages([
    ("system", "Answer the user's questions based on the below context:\n\n{context}"),
    MessagesPlaceholder(variable_name="chat_history"),
    ("user", "{input}"),
])
document_chain = create_stuff_documents_chain(llm, prompt)
# æ„å»ºæ£€ç´¢é“¾
retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)

chat_history_content = "æ¨ªç›˜æ•´ç†çš„å½¢æ€æ˜¯ä»€ä¹ˆæ ·å­?"
chat_history = [HumanMessage(content=chat_history_content), AIMessage(content="Yes!")]

response = retrieval_chain.invoke({
    "chat_history": chat_history,
    "input": "ä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ç§æƒ…å†µå‘¢ï¼Ÿ"
})

print(response["answer"])

# ç»ˆç«¯è¾“å‡º:
# è¿™ç§æƒ…å†µå‡ºç°çš„åŸå› æ˜¯ä¸»åŠ›åœ¨è‚¡ä»·ä¸Šå‡åˆ°æ•æ„Ÿä»·ä½æˆ–è€…å¸‚åœºèƒŒæ™¯æœ‰æ‰€è½¬æ¢æ—¶ï¼Œä¼šé€‚æ—¶æŠ›å‡ºä¸€éƒ¨åˆ†ç­¹ç ï¼Œæ‰“å‹ä½è‚¡ä»·çš„ä¸Šå‡è¶‹åŠ¿ï¼Œå½¢æˆå¹³å°æ•´ç†çš„æ ¼å±€ã€‚åœ¨è¿™ä¸ªé˜¶æ®µå†…ï¼Œæˆäº¤é‡ç¨æ˜¾æ´»è·ƒï¼Œä¸€æ—¦å¹³å°æ•´ç†æ ¼å±€å½¢æˆï¼Œæˆäº¤é‡ä¼šè¿…é€Ÿèç¼©ã€‚

# åŸæ–‡:
# æ¨ªç›˜æ•´ç†çš„å½¢æ€åœ¨Kçº¿ä¸Šçš„è¡¨ç°å¸¸å¸¸æ˜¯ä¸€æ¡æ¨ªçº¿æˆ–è€…é•¿æœŸçš„å¹³å°ï¼Œä»æˆäº¤é‡ä¸Šæ¥çœ‹ï¼Œåœ¨å¹³å°æ•´ç†çš„è¿‡ç¨‹ä¸­æˆäº¤é‡å‘ˆé€’å‡çš„çŠ¶æ€ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨å¹³å°ä¸Šæ²¡æœ‰æˆ–å¾ˆå°‘æœ‰æˆäº¤é‡æ”¾å‡ºã€‚æˆäº¤æ¸…æ·¡ï¼Œæˆäº¤ä»·æ ¼ä¹Ÿæåº¦ä¸æ´»è·ƒã€‚ä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ç§æƒ…å†µå‘¢ï¼Ÿå…¶å†…åœ¨çš„æœºç†å°±æ˜¯ï¼šå½“è‚¡ä»·ä¸Šå‡åˆ°æ•æ„Ÿä»·ä½æˆ–æµ®ç æ¶ŒåŠ¨äº¦æˆ–å¸‚åœºèƒŒæ™¯æœ‰æ‰€è½¬æ¢çš„æ—¶å€™ï¼Œä¸»åŠ›ä¼šé€‚æ—¶æŠ›å‡ºä¸€éƒ¨åˆ†ç­¹ç ï¼Œæ‰“å‹ä½è‚¡ä»·çš„å‡åŠ¿ï¼Œç”¨ä¸€éƒ¨åˆ†èµ„é‡‘é¡¶ä½è·åˆ©æŠ›ç›˜ï¼Œå¼ºåˆ¶è‚¡ä»·å½¢æˆå¹³å°æ•´ç†çš„æ ¼å±€ï¼Œåœ¨è¿™ä¸ªé˜¶æ®µå†…ï¼Œæˆäº¤é‡ç¨æ˜¾æ´»è·ƒï¼Œä¸€æ—¦å¹³å°æ•´ç†æ ¼å±€å½¢æˆï¼Œæˆäº¤é‡åº”è¿…é€Ÿåœ°èç¼©ä¸‹æ¥ã€‚ä¸»åŠ›ä¸€èˆ¬ä¼šè®©æ•£æˆ·æŠ•èµ„è€…å’Œå°èµ„é‡‘æŒæœ‰è€…æ‰€æŒç­¹ç åœ¨å¹³å°å†…å……åˆ†è‡ªç”±æ¢æ‰‹ï¼Œåªæ˜¯åœ¨å¤§åŠ¿ä¸å¥½è‚¡ä»·ä¸‹æ»‘çš„æƒ…å†µä¸‹ï¼Œé€‚æ—¶æ§åˆ¶è‚¡ä»·ä¸‹è·Œçš„å†²åŠ¨ã€‚æ­¤é˜¶æ®µæ—¶é—´å†…çš„æˆäº¤é‡ç”±äºä¸»åŠ›æ´»åŠ¨æå°‘ï¼Œæˆäº¤é‡åº”è¯¥æ˜¯æ¸…æ·¡çš„ã€‚
```

#### ç‰¹åˆ«æ³¨æ„:

å¯¹è¯æ£€ç´¢é“¾è™½ç„¶è§£å†³äº†ä¸Šä¸‹æ–‡å…³è”é—®é¢˜ï¼Œä½† **LangChainå†…éƒ¨æ²¡æœ‰å…³äº "å†å²å¯¹è¯æ•°æ®(`chat_history_content`)" æœ€å¤§é•¿åº¦çš„å¤„ç†é€»è¾‘** ğŸ’¢ğŸ’¢ğŸ’¢ã€‚<br>

ç»æµ‹è¯•ï¼Œå½“ `chat_history_content` è¿‡å¤§æ—¶ï¼Œè™½ç„¶ä¸‹åˆ—ä»£ç å¯ä»¥æ­£å¸¸è¿è¡Œï¼Œä¾‹å¦‚:<br>

```python
# å¤§å‹é—®ç­”è¯­æ–™æ•°æ®
file_path = "financial_data.txt"

with open(file_path, "r") as file:
    chat_history_content = file.read()

chat_history = [HumanMessage(content=chat_history_content), AIMessage(content="Yes!")]
```

ä½†å½“è¿è¡Œä¸‹åˆ—ä»£ç æ—¶æŠ¥é”™:<br>

```python
response = retrieval_chain.invoke({
    "chat_history": chat_history,
    "input": "å¯è½¬å€ºå…·å¤‡å€ºæ€§å—ï¼Ÿ"
})
```

ç»ˆç«¯æç¤º:<br>

```txt
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 16385 tokens. However, your messages resulted in 185680 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
```

ç¤ºæ„å›¾å¦‚ä¸‹:<br>

![](./materials/exceed_max_length.jpg)


### Agent(ä»£ç†):

We've so far create examples of chains - where each step is known ahead of time. The final thing we will create is an agent - where the LLM decides what steps to take.<br>

åˆ°ç›®å‰ä¸ºæ­¢æˆ‘ä»¬å·²ç»åˆ›å»ºäº†é“¾å¼ç¤ºä¾‹â€”â€”å…¶ä¸­æ¯ä¸€æ­¥éƒ½æ˜¯é¢„å…ˆçŸ¥é“çš„ã€‚æˆ‘ä»¬æ¥ä¸‹æ¥è¦åˆ›å»ºçš„æ˜¯ä¸€ä¸ªä»£ç†â€”â€”ç”±LLMå†³å®šè¦é‡‡å–ä»€ä¹ˆæ­¥éª¤ã€‚<br>

**NOTE: for this example we will only show how to create an agent using OpenAI models, as local models are not reliable enough yet.** <br>

**æ³¨æ„ï¼šå¯¹äºè¿™ä¸ªç¤ºä¾‹ï¼Œæˆ‘ä»¬åªä¼šå±•ç¤ºå¦‚ä½•ä½¿ç”¨OpenAIæ¨¡å‹æ¥åˆ›å»ºä¸€ä¸ªä»£ç†ï¼Œå› ä¸ºæœ¬åœ°æ¨¡å‹ç›®å‰è¿˜ä¸å¤Ÿå¯é ã€‚** <br>

One of the first things to do when building an agent is to decide what tools it should have access to. For this example, we will give the agent access to two tools:<br>

æ„å»ºä¸€ä¸ªä»£ç†çš„ç¬¬ä¸€æ­¥æ˜¯å†³å®šå®ƒåº”è¯¥æœ‰æƒè®¿é—®å“ªäº›å·¥å…·ã€‚å¯¹äºè¿™ä¸ªç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†ç»™ä»£ç†è®¿é—®ä¸¤ä¸ªå·¥å…·çš„æƒé™ï¼š<br>

1. The retriever we just created. This will let it easily answer questions about LangSmith.(æˆ‘ä»¬åˆšåˆšåˆ›å»ºçš„æ£€ç´¢å™¨ã€‚è¿™å°†ä½¿å®ƒèƒ½å¤Ÿè½»æ¾å›ç­”å…³äºLangSmithçš„é—®é¢˜ã€‚)

2. A search tool. This will let it easily answer questions that require up to date information.(ä¸€ä¸ªæœç´¢å·¥å…·ã€‚è¿™å°†ä½¿å®ƒèƒ½å¤Ÿè½»æ¾å›ç­”éœ€è¦æœ€æ–°ä¿¡æ¯çš„é—®é¢˜ã€‚)

> up to date: æœ€æ–°çš„

First, let's set up(è®¾ç½®) a tool for the retriever we just created:<br>

é¦–å…ˆï¼Œè®©æˆ‘ä»¬ä¸ºæˆ‘ä»¬åˆšåˆšåˆ›å»ºçš„æ£€ç´¢å™¨è®¾ç½®ä¸€ä¸ªå·¥å…·ï¼š<br>

```python
from langchain.tools.retriever import create_retriever_tool

retriever_tool = create_retriever_tool(
    retriever,
    "langsmith_search",
    "Search for information about LangSmith. For any questions about LangSmith, you must use this tool!",
)
# åŸæ–‡: "Search for information about LangSmith. For any questions about LangSmith, you must use this tool!"
# ç¿»è¯‘: "æœç´¢å…³äºLangSmithçš„ä¿¡æ¯ã€‚å¯¹äºä»»ä½•å…³äºLangSmithçš„é—®é¢˜ï¼Œä½ å¿…é¡»ä½¿ç”¨è¿™ä¸ªå·¥å…·ï¼"
```

The search tool that we will use is [Tavily](https://python.langchain.com/docs/integrations/retrievers/tavily). This will require an API key (they have generous free tier). After creating it on their platform, you need to set it as an environment variable:<br>

æˆ‘ä»¬å°†ä½¿ç”¨çš„æœç´¢å·¥å…·æ˜¯ Tavilyã€‚è¿™å°†éœ€è¦ä¸€ä¸ª API å¯†é’¥ï¼ˆä»–ä»¬æä¾›æ…·æ…¨çš„å…è´¹å¥—é¤ï¼‰ã€‚åœ¨ä»–ä»¬çš„å¹³å°ä¸Šåˆ›å»ºä¹‹åï¼Œä½ éœ€è¦å°†å…¶è®¾ç½®ä¸ºç¯å¢ƒå˜é‡ï¼š<br>

```bash
export TAVILY_API_KEY=...
```

If you do not want to set up an API key, you can skip creating this tool.<br>

å¦‚æœä½ ä¸æƒ³è®¾ç½® API å¯†é’¥ï¼Œä½ å¯ä»¥è·³è¿‡åˆ›å»ºè¿™ä¸ªå·¥å…·ã€‚<br>

> å› ä¸ºæ„å»ºçš„ tools æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå°‘ä¸€é¡¹ä¹Ÿæ²¡æœ‰å…³ç³»ã€‚

```python
from langchain_community.tools.tavily_search import TavilySearchResults

search = TavilySearchResults()
```

We can now create a list of the tools we want to work with:<br>

æˆ‘ä»¬ç°åœ¨å¯ä»¥åˆ—å‡ºæˆ‘ä»¬æƒ³è¦ä½¿ç”¨çš„å·¥å…·ï¼š<br>

```python
tools = [retriever_tool, search]
```

Now that we have the tools, we can create an agent to use them. We will go over this pretty quickly - for a deeper dive into what exactly is going on, check out the [Agent's Getting Started documentation](https://python.langchain.com/docs/modules/agents)<br>

æ—¢ç„¶æˆ‘ä»¬å·²ç»æœ‰äº†å·¥å…·ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªä»£ç†æ¥ä½¿ç”¨å®ƒä»¬ã€‚æˆ‘ä»¬ä¼šå¾ˆå¿«åœ°è®²è§£è¿™ä¸€ç‚¹ - å¦‚æœæƒ³æ·±å…¥äº†è§£æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…ï¼Œè¯·æŸ¥é˜…ä»£ç†çš„å…¥é—¨æ–‡æ¡£ã€‚<br>

Install langchain hub first(é¦–å…ˆå®‰è£… Langchain Hub):<br>

```bash
pip install langchainhub
```

Now we can use it to get a predefined prompt:<br>

ç°åœ¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥è·å–ä¸€ä¸ªé¢„å®šä¹‰çš„æç¤º:<br>

```python
from langchain_openai import ChatOpenAI
from langchain import hub
from langchain.agents import create_openai_functions_agent
from langchain.agents import AgentExecutor

# Get the prompt to use - you can modify this!
prompt = hub.pull("hwchase17/openai-functions-agent")
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
agent = create_openai_functions_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
```

We can now invoke the agent and see how it responds! We can ask it questions about LangSmith:<br>

ç°åœ¨æˆ‘ä»¬å¯ä»¥è°ƒç”¨ä»£ç†å¹¶æŸ¥çœ‹å®ƒçš„å“åº”ï¼æˆ‘ä»¬å¯ä»¥é—®å®ƒå…³äº LangSmith çš„é—®é¢˜ï¼š<br>

```python
agent_executor.invoke({"input": "how can langsmith help with testing?"})
```

We can ask it about the weather:<br>

æˆ‘ä»¬å¯ä»¥é—®å®ƒå…³äºå¤©æ°”çš„é—®é¢˜:<br>

```python
agent_executor.invoke({"input": "what is the weather in SF?"})
```

We can have conversations with it:<br>

æˆ‘ä»¬å¯ä»¥ä¸å®ƒè¿›è¡Œå¯¹è¯:<br>

```python
chat_history = [HumanMessage(content="Can LangSmith help test my LLM applications?"), AIMessage(content="Yes!")]
agent_executor.invoke({
    "chat_history": chat_history,
    "input": "Tell me how"
})
```

### Agent å®Œæ•´ä»£ç ç¤ºä¾‹:

æš‚æ— ï¼Œç¬”è€…å…ˆå»ç ”ç©¶ä¸‹Agentçš„å…·ä½“é€»è¾‘ã€‚<br>


## Document loaders:

### PDF:

Portable(ä¾¿æºå¼) Document Format (PDF), standardized(æ ‡å‡†åŒ–) as ISO 32000, is a file format developed by Adobe in 1992 to present(å±•ç¤º;å‘ˆç°) documents, including text formatting and images, in a manner independent(ç‹¬ç«‹çš„;æ— å…³çš„) of application software(åº”ç”¨è½¯ä»¶), hardware(ç¡¬ä»¶), and operating systems(æ“ä½œç³»ç»Ÿ).<br>

ä¾¿æºå¼æ–‡æ¡£æ ¼å¼ï¼ˆPDFï¼‰ï¼Œæ ‡å‡†åŒ–ä¸ºISO 32000ï¼Œæ˜¯ç”±Adobeäº1992å¹´å¼€å‘çš„æ–‡ä»¶æ ¼å¼ï¼Œç”¨äºä»¥ä¸åº”ç”¨è½¯ä»¶ã€ç¡¬ä»¶å’Œæ“ä½œç³»ç»Ÿæ— å…³çš„æ–¹å¼å‘ˆç°æ–‡æ¡£ï¼ŒåŒ…æ‹¬æ–‡æœ¬æ ¼å¼å’Œå›¾åƒã€‚<br>

This covers how to load PDF documents into the Document format that we use downstream.<br>

è¿™(éƒ¨åˆ†å†…å®¹)æ¶µç›–äº†å¦‚ä½•å°†PDFæ–‡æ¡£åŠ è½½åˆ°æˆ‘ä»¬ä¸‹æ¸¸ä½¿ç”¨çš„æ–‡æ¡£æ ¼å¼ä¸­ã€‚<br>

#### Using PyPDF:

Load PDF using `pypdf` into array of documents, where each document contains the page content and metadata with page number.<br>

ä½¿ç”¨ `pypdf` å°† PDF åŠ è½½ä¸ºæ–‡æ¡£æ•°ç»„ï¼Œå…¶ä¸­æ¯ä¸ªæ–‡æ¡£åŒ…å«é¡µé¢å†…å®¹å’Œå¸¦æœ‰é¡µç çš„å…ƒæ•°æ®ã€‚<br>

```bash
pip install pypdf
```

```python
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("example_data/layout-parser-paper.pdf")
pages = loader.load_and_split()
print(pages[0])

# ç»ˆç«¯è¾“å‡º:
# Document(page_content='LayoutParser : A Uni\x0ced Toolkit for Deep\nLearning Based Document Image Analysis\nZejiang Shen1( \x00), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\nLee4, Jacob Carlson3, and Weining Li5\n1Allen Institute for AI\nshannons@allenai.org\n2Brown University\nruochen zhang@brown.edu\n3Harvard University\nfmelissadell,jacob carlson g@fas.harvard.edu\n4University of Washington\nbcgl@cs.washington.edu\n5University of Waterloo\nw422li@uwaterloo.ca\nAbstract. Recent advances in document image analysis (DIA) have been\nprimarily driven by the application of neural networks. Ideally, research\noutcomes could be easily deployed in production and extended for further\ninvestigation. However, various factors like loosely organized codebases\nand sophisticated model con\x0cgurations complicate the easy reuse of im-\nportant innovations by a wide audience. Though there have been on-going\ne\x0borts to improve reusability and simplify deep learning (DL) model\ndevelopment in disciplines like natural language processing and computer\nvision, none of them are optimized for challenges in the domain of DIA.\nThis represents a major gap in the existing toolkit, as DIA is central to\nacademic research across a wide range of disciplines in the social sciences\nand humanities. This paper introduces LayoutParser , an open-source\nlibrary for streamlining the usage of DL in DIA research and applica-\ntions. The core LayoutParser library comes with a set of simple and\nintuitive interfaces for applying and customizing DL models for layout de-\ntection, character recognition, and many other document processing tasks.\nTo promote extensibility, LayoutParser also incorporates a community\nplatform for sharing both pre-trained models and full document digiti-\nzation pipelines. We demonstrate that LayoutParser is helpful for both\nlightweight and large-scale digitization pipelines in real-word use cases.\nThe library is publicly available at https://layout-parser.github.io .\nKeywords: Document Image Analysis Â·Deep Learning Â·Layout Analysis\nÂ·Character Recognition Â·Open Source library Â·Toolkit.\n1 Introduction\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\ndocument image analysis (DIA) tasks including document image classi\x0ccation [ 11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021', metadata={'source': 'example_data/layout-parser-paper.pdf', 'page': 0})
```

ç»æµ‹è¯•,ä¸Šè¿°ä»£ç å¯ä»¥æ­£ç¡®è¯»å–åŒæ å½¢å¼çš„PDFï¼Œå®Œæ•´ç‰ˆä»£ç å¯å‚è€ƒ `example_code/parse_pdf_with_langchain.py` æ–‡ä»¶ã€‚<br>

ğŸ”¥ğŸ”¥ğŸ”¥An advantage of this approach is that documents can be retrieved with page numbers.<br>

ğŸ”¥ğŸ”¥ğŸ”¥è¿™ç§æ–¹æ³•çš„ä¸€ä¸ªä¼˜åŠ¿æ˜¯å¯ä»¥æ ¹æ®é¡µç æ£€ç´¢æ–‡æ¡£ã€‚<br>

We want to use `OpenAIEmbeddings` so we have to get the OpenAI API Key.<br>

æˆ‘ä»¬æƒ³è¦ä½¿ç”¨ `OpenAIEmbeddings`ï¼Œæ‰€ä»¥æˆ‘ä»¬å¿…é¡»è·å– OpenAI API å¯†é’¥ã€‚<br>

```python
import os
import getpass

os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')
# OpenAI API Key: Â·Â·Â·Â·Â·Â·Â·Â·

from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings())
docs = faiss_index.similarity_search("How will the community be engaged?", k=2) # ç¤¾åŒºå°†å¦‚ä½•å‚ä¸ï¼Ÿ
for doc in docs:
    print(str(doc.metadata["page"]) + ":", doc.page_content[:300])
```

ç»ˆç«¯è¾“å‡ºå¦‚ä¸‹:<br>

```txt
9: 10 Z. Shen et al.
Fig. 4: Illustration of (a) the original historical Japanese document with layout
detection results and (b) a recreated version of the document image that achieves
much better character recognition recall. The reorganization algorithm rearranges
the tokens based on the their detect
3: 4 Z. Shen et al.
Efficient Data AnnotationC u s t o m i z e d  M o d e l  T r a i n i n gModel Cust omizationDI A Model HubDI A Pipeline SharingCommunity PlatformLa y out Detection ModelsDocument Images
T h e  C o r e  L a y o u t P a r s e r  L i b r a r yOCR ModuleSt or age & VisualizationLa y ou
```

ä¸­æ–‡å«ä¹‰:<br>

```txt
9:10 æ²ˆç­‰äºº
å›¾4ï¼šï¼ˆaï¼‰åŸå§‹å†å²æ—¥æ–‡æ–‡æ¡£åŠå…¶å¸ƒå±€æ£€æµ‹ç»“æœçš„ç¤ºæ„å›¾ï¼›ï¼ˆbï¼‰é‡æ–°åˆ›å»ºçš„æ–‡æ¡£å›¾åƒï¼Œå®ç°äº†æ›´å¥½çš„å­—ç¬¦è¯†åˆ«å¬å›ç‡ã€‚é‡æ–°ç»„ç»‡ç®—æ³•åŸºäºå®ƒä»¬çš„æ£€æµ‹ç»“æœé‡æ–°æ’åˆ—ä»¤ç‰Œã€‚
3:4 æ²ˆç­‰äºº
é«˜æ•ˆæ•°æ®æ ‡æ³¨è‡ªå®šä¹‰æ¨¡å‹è®­ç»ƒæ¨¡å‹å®šåˆ¶DIAæ¨¡å‹ä¸­å¿ƒDIAç®¡é“å…±äº«ç¤¾åŒºå¹³å°å¸ƒå±€æ£€æµ‹æ¨¡å‹æ–‡æ¡£å›¾åƒæ ¸å¿ƒå¸ƒå±€è§£æåº“OCRæ¨¡å—å­˜å‚¨ä¸å¯è§†åŒ–å¸ƒå±€
```

### Extracting images(æå–å›¾åƒ):

Using the `rapidocr-onnxruntime` package we can extract images as text as well:<br>

åˆ©ç”¨ `rapidocr-onnxruntime` åŒ…ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å°†å›¾åƒæå–ä¸ºæ–‡æœ¬ï¼š<br>

```bash
pip install rapidocr-onnxruntime
```

```python
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("example_data/LayoutParser_åŸºäºæ·±åº¦å­¦ä¹ çš„æ–‡æ¡£å›¾åƒåˆ†æçš„ç»Ÿä¸€å·¥å…·åŒ….pdf", extract_images=True)
pages = loader.load()
print(pages[3].page_content)    # The page index starts counting from 0.
```

åŸå›¾åŠè¯†åˆ«æ•ˆæœå¦‚ä¸‹:<br>

![](./materials/parse_result_pdf_contain_images.jpg)

ğŸš¨ğŸš¨ğŸš¨æ³¨æ„:ç”±äºå›¾ç‰‡çš„æ ¼å¼å¤šæ ·ï¼Œæå–å‡ºçš„å†…å®¹çš„æ’ç‰ˆå¯èƒ½ä¸æ˜¯ä½ æœŸæœ›çš„æ ·å­ï¼Œè¿™éƒ¨åˆ†ä»£ç åªèƒ½ä½œä¸ºé€šç”¨ç‰ˆä½¿ç”¨ã€‚é’ˆå¯¹è‡ªå·±çš„ä»»åŠ¡ï¼Œå¯èƒ½éœ€è¦ä¸“é—¨è®¾è®¡ä¸€ä¸ªOCRæ¨¡å—ã€‚<br>

è¯·å°†ä¸‹åˆ—å†…å®¹ç¿»è¯‘ä¸ºåœ°é“çš„ä¸­æ–‡ï¼š

### Using MathPix:

Inspired by Daniel Gross's(å—...å¯å‘) https://gist.github.com/danielgross/3ab4104e14faccc12b49200843adab21 <br>

```python
from langchain_community.document_loaders import MathpixPDFLoader
loader = MathpixPDFLoader("example_data/LayoutParser_åŸºäºæ·±åº¦å­¦ä¹ çš„æ–‡æ¡£å›¾åƒåˆ†æçš„ç»Ÿä¸€å·¥å…·åŒ….pdf")   # å®˜ç½‘çš„"example_data/layout-parser-paper.pdf"å°±æ˜¯ç¬”è€…å½“å‰ä½¿ç”¨çš„æ–‡ä»¶ï¼›
data = loader.load()
print(data) # ç¬”è€…æ²¡æœ‰æµ‹è¯•ï¼Œç¬”è€…æ²¡æœ‰æ³¨å†ŒMathpixã€‚
```

MathPix æ˜¯ä¸€æ¬¾èƒ½å¤Ÿè¯†åˆ«å’Œè§£ææ•°å­¦å…¬å¼çš„è½¯ä»¶å·¥å…·ï¼Œç‰¹åˆ«é€‚ç”¨äºå­¦ç”Ÿã€æ•™å¸ˆå’Œç§‘ç ”äººå‘˜ç­‰éœ€è¦å¤„ç†å¤æ‚æ•°å­¦å…¬å¼çš„ç”¨æˆ·ã€‚<br>

MathPix ä¼šä½¿ç”¨å…¶é«˜çº§çš„OCRï¼ˆOptical Character Recognitionï¼Œå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼‰æŠ€æœ¯æ¥è¯†åˆ«å›¾ç‰‡ä¸­çš„æ•°å­¦å…¬å¼ï¼Œå¹¶å°†å…¶è½¬æ¢æˆå¯ç¼–è¾‘çš„æ–‡æœ¬æ ¼å¼ï¼Œæ¯”å¦‚ LaTeX æˆ– Markdown ç­‰ã€‚è¿™ä¸€è¿‡ç¨‹ä¸ä»…æé«˜äº†è¾“å…¥å¤æ‚æ•°å­¦å…¬å¼çš„æ•ˆç‡ï¼Œä¹Ÿæå¤§åœ°ç®€åŒ–äº†åœ¨æ•°å­—æ–‡æ¡£ä¸­ä½¿ç”¨å’Œå…±äº«è¿™äº›å…¬å¼çš„è¿‡ç¨‹ã€‚MathPix æ”¯æŒå¤šç§æ•°å­¦ç¬¦å·å’Œè¡¨è¾¾å¼çš„è¯†åˆ«ï¼ŒåŒ…æ‹¬ç§¯åˆ†ã€å¯¼æ•°ã€çŸ©é˜µã€æé™ç­‰ï¼Œæ˜¯å­¦æœ¯å†™ä½œå’Œç§‘ç ”å·¥ä½œä¸­çš„ä¸€ä¸ªæœ‰ç”¨å·¥å…·ã€‚<br>

âš ï¸âš ï¸âš ï¸æ³¨æ„: ä½¿ç”¨Mathpixéœ€è¦å…ˆæœ‰`MATHPIX_API_KEY`ï¼Œå¹¶å°†`MATHPIX_API_KEY`æ·»åŠ åˆ°ç¯å¢ƒå˜é‡ã€‚<br>

### Using Unstructured:

âŒâŒâŒå®˜ç½‘ç¤ºä¾‹--æ— æ³•è¿è¡Œ:<br>

```python
from langchain_community.document_loaders import UnstructuredPDFLoader
loader = UnstructuredPDFLoader("example_data/layout-parser-paper.pdf")
data = loader.load()
print(data)
```

ç‚¹å‡»`UnstructuredPDFLoader`è·³è½¬åå¾—åˆ°çš„ä»£ç ç¤ºä¾‹:<br>

```python

```

ä¾èµ–é¡¹å®‰è£…:<br>

```bash
pip install pdf2image, pdfminer.six, pillow_heif
```

#### ModuleNotFoundError: No module named 'pdf2image':

è¿™ä¸ªé”™è¯¯ä¿¡æ¯è¡¨æ˜ä½ çš„Pythonç¯å¢ƒä¸­æ²¡æœ‰å®‰è£…`pdf2image`è¿™ä¸ªæ¨¡å—ã€‚`pdf2image`æ˜¯ä¸€ä¸ªåº“ï¼Œç”¨äºå°†PDFæ–‡ä»¶è½¬æ¢æˆå›¾åƒã€‚è¿™ä¸ªåº“éå¸¸æœ‰ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¯¹PDFæ–‡ä»¶è¿›è¡Œè§†è§‰å¤„ç†æ—¶ã€‚<br>

è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½ éœ€è¦å®‰è£…`pdf2image`æ¨¡å—ã€‚ä½ å¯ä»¥ä½¿ç”¨pipæ¥å®‰è£…å®ƒï¼Œpipæ˜¯Pythonçš„åŒ…ç®¡ç†å·¥å…·ã€‚<br>

æ‰“å¼€ä½ çš„å‘½ä»¤è¡Œå·¥å…·ï¼ˆåœ¨Windowsä¸Šæ˜¯å‘½ä»¤æç¤ºç¬¦æˆ–PowerShellï¼Œåœ¨MacOSæˆ–Linuxä¸Šæ˜¯ç»ˆç«¯ï¼‰ï¼Œç„¶åè¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®‰è£…`pdf2image`ï¼š<br>

```sh
pip install pdf2image
```

å®‰è£…`pdf2image`ä¹‹å‰ï¼Œè¯·ç¡®ä¿ä½ çš„ç³»ç»Ÿä¸Šå·²ç»å®‰è£…äº†Popplerã€‚`pdf2image`ä¾èµ–äºPopplerè¿™ä¸ªå·¥å…·ï¼ŒPoppleræ˜¯ä¸€ä¸ªPDFæ¸²æŸ“åº“ï¼Œç”¨äºæ”¯æŒPDFè½¬æ¢æˆå›¾åƒçš„è¿‡ç¨‹ã€‚å¦‚æœä½ æ²¡æœ‰å®‰è£…Popplerï¼Œ`pdf2image`å°†æ— æ³•æ­£å¸¸å·¥ä½œã€‚<br>

- åœ¨**Linux**ä¸Šï¼Œä½ å¯ä»¥é€šè¿‡åŒ…ç®¡ç†å™¨å®‰è£…Popplerã€‚ä¾‹å¦‚ï¼Œåœ¨Ubuntuæˆ–Debianä¸Šï¼Œä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š<br>

```sh
sudo apt-get install poppler-utils
```

- åœ¨**MacOS**ä¸Šï¼Œä½ å¯ä»¥ä½¿ç”¨Homebrewæ¥å®‰è£…Popplerï¼š

```sh
brew install poppler
```

- åœ¨**Windows**ä¸Šï¼Œå®‰è£…Popplerå¯èƒ½ç¨å¾®å¤æ‚ä¸€äº›ï¼Œå› ä¸ºä½ éœ€è¦ä¸‹è½½Popplerçš„äºŒè¿›åˆ¶æ–‡ä»¶å¹¶å°†å…¶æ·»åŠ åˆ°ç³»ç»Ÿçš„PATHç¯å¢ƒå˜é‡ä¸­ã€‚ä½ å¯ä»¥ä»[Popplerçš„å®˜æ–¹é¡µé¢](https://poppler.freedesktop.org/)æˆ–å…¶ä»–å¯ä¿¡çš„æºä¸‹è½½Windowsç‰ˆæœ¬çš„Popplerã€‚

å®‰è£…å®Œæˆåï¼Œå†æ¬¡è¿è¡Œä½ çš„ä»£ç ï¼Œçœ‹çœ‹é—®é¢˜æ˜¯å¦è§£å†³ã€‚<br>

#### ModuleNotFoundError: No module named 'pdfminer':

è¿™ä¸ªé”™è¯¯ä¿¡æ¯è¡¨æ˜ä½ çš„Pythonç¯å¢ƒä¸­æ²¡æœ‰å®‰è£…`pdfminer`è¿™ä¸ªæ¨¡å—ã€‚`pdfminer`æ˜¯ä¸€ä¸ªç”¨äºä»PDFæ–‡æ¡£ä¸­æå–ä¿¡æ¯çš„å·¥å…·ï¼Œå®ƒå¯ä»¥è§£æPDFæ–‡æœ¬ã€å›¾è¡¨å’Œå…ƒæ•°æ®ã€‚<br>

è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½ éœ€è¦å®‰è£…`pdfminer`æ¨¡å—ã€‚å¦‚æœä½ ä½¿ç”¨çš„æ˜¯Python 3ï¼Œé‚£ä¹ˆåº”è¯¥å®‰è£…`pdfminer.six`ï¼Œè¿™æ˜¯`pdfminer`çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä¸“é—¨ä¸ºPython 3æ›´æ–°å’Œç»´æŠ¤ã€‚ä½ å¯ä»¥ä½¿ç”¨pipæ¥å®‰è£…å®ƒï¼Œpipæ˜¯Pythonçš„åŒ…ç®¡ç†å·¥å…·ã€‚<br>

æ‰“å¼€ä½ çš„å‘½ä»¤è¡Œå·¥å…·ï¼ˆåœ¨Windowsä¸Šæ˜¯å‘½ä»¤æç¤ºç¬¦æˆ–PowerShellï¼Œåœ¨MacOSæˆ–Linuxä¸Šæ˜¯ç»ˆç«¯ï¼‰ï¼Œç„¶åè¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®‰è£…`pdfminer.six`ï¼š<br>

```sh
pip install pdfminer.six
```

å¦‚æœä½ å·²ç»å®‰è£…äº†`pdfminer.six`ä½†ä»ç„¶é‡åˆ°è¿™ä¸ªé”™è¯¯ï¼Œå¯èƒ½æ˜¯å› ä¸ºä½ çš„è„šæœ¬è¿è¡Œåœ¨ä¸€ä¸ªæ²¡æœ‰å®‰è£…è¯¥æ¨¡å—çš„ç¯å¢ƒä¸­ã€‚ç¡®ä¿ä½ çš„è„šæœ¬è¿è¡Œåœ¨æ­£ç¡®çš„Pythonç¯å¢ƒä¸­ï¼Œç‰¹åˆ«æ˜¯å¦‚æœä½ ä½¿ç”¨äº†è™šæ‹Ÿç¯å¢ƒçš„è¯ã€‚<br>

å¦‚æœä½ ä½¿ç”¨çš„æ˜¯å¦ä¸€ä¸ªç‰ˆæœ¬çš„`pdfminer`æˆ–æœ‰ç‰¹å®šçš„å®‰è£…éœ€æ±‚ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´å®‰è£…å‘½ä»¤ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ ç¡®å®éœ€è¦åŸå§‹çš„`pdfminer`ï¼ˆé€‚ç”¨äºPython 2ï¼‰ï¼Œä½ å¯ä»¥å°è¯•å®‰è£…å®ƒï¼ˆå°½ç®¡ä¸æ¨èä½¿ç”¨Python 2ï¼Œå› ä¸ºå®ƒå·²ç»ä¸å†è·å¾—å®˜æ–¹æ”¯æŒï¼‰ï¼š<br>

```sh
pip install pdfminer
```

å®‰è£…å®Œæˆåï¼Œå†æ¬¡è¿è¡Œä½ çš„ä»£ç ï¼Œçœ‹çœ‹é—®é¢˜æ˜¯å¦è§£å†³ã€‚<br>

#### ModuleNotFoundError: No module named 'pillow_heif':

è¿™ä¸ªé”™è¯¯ä¿¡æ¯è¡¨æ˜ä½ çš„Pythonç¯å¢ƒä¸­æ²¡æœ‰å®‰è£…`pillow_heif`è¿™ä¸ªæ¨¡å—ã€‚`pillow_heif`æ˜¯ä¸€ä¸ªå¤„ç†HEIFï¼ˆHigh Efficiency Image File Formatï¼Œé«˜æ•ˆç‡å›¾åƒæ–‡ä»¶æ ¼å¼ï¼‰å›¾ç‰‡çš„åº“ï¼Œå®ƒä¾èµ–äºPillowï¼ŒPillowæ˜¯Pythonä¸­ä¸€ä¸ªå¹¿æ³›ä½¿ç”¨çš„å›¾åƒå¤„ç†åº“ã€‚<br>

è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½ éœ€è¦å®‰è£…`pillow_heif`æ¨¡å—ã€‚ä½ å¯ä»¥ä½¿ç”¨pipæ¥å®‰è£…å®ƒï¼Œpipæ˜¯Pythonçš„åŒ…ç®¡ç†å·¥å…·ã€‚<br>

æ‰“å¼€ä½ çš„å‘½ä»¤è¡Œå·¥å…·ï¼ˆåœ¨Windowsä¸Šæ˜¯å‘½ä»¤æç¤ºç¬¦æˆ–PowerShellï¼Œåœ¨MacOSæˆ–Linuxä¸Šæ˜¯ç»ˆç«¯ï¼‰ï¼Œç„¶åè¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®‰è£…`pillow_heif`ï¼š<br>

```sh
pip install pillow_heif
```

å®‰è£…`pillow_heif`æ—¶ï¼Œå¦‚æœä½ è¿˜æ²¡æœ‰å®‰è£…Pillowï¼Œå®ƒåº”è¯¥ä¼šè‡ªåŠ¨å®‰è£…Pillowä½œä¸ºä¾èµ–ã€‚å¦‚æœå‡ºäºæŸç§åŸå› Pillowæ²¡æœ‰è¢«è‡ªåŠ¨å®‰è£…ï¼Œä½ å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤æ‰‹åŠ¨å®‰è£…Pillowï¼š<br>

```sh
pip install Pillow
```

å®‰è£…å®Œæˆåï¼Œå†æ¬¡è¿è¡Œä½ çš„ä»£ç ï¼Œçœ‹çœ‹é—®é¢˜æ˜¯å¦è§£å†³ã€‚å¦‚æœä½ çš„é¡¹ç›®ä¾èµ–äºç‰¹å®šç‰ˆæœ¬çš„`pillow_heif`æˆ–Pillowï¼Œè¯·ç¡®ä¿å®‰è£…äº†æ­£ç¡®çš„ç‰ˆæœ¬ä»¥é¿å…å…¼å®¹æ€§é—®é¢˜ã€‚<br>

#### ModuleNotFoundError: No module named 'unstructured_inference':

è¿™ä¸ªé”™è¯¯ä¿¡æ¯è¡¨æ˜ä½ çš„Pythonç¯å¢ƒä¸­æ²¡æœ‰å®‰è£…`unstructured_inference`è¿™ä¸ªæ¨¡å—ã€‚è¿™ä¸ªæ¨¡å—å¯èƒ½æ˜¯`langchain`æˆ–å…¶ä¾èµ–åº“çš„ä¸€éƒ¨åˆ†ï¼Œç”¨äºå¤„ç†éç»“æ„åŒ–æ•°æ®çš„æ¨ç†ã€‚<br>

è§£å†³è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•æ˜¯å®‰è£…ç¼ºå¤±çš„æ¨¡å—ã€‚ç„¶è€Œï¼Œæ ¹æ®é”™è¯¯ä¿¡æ¯å’Œæä¾›çš„ä¸Šä¸‹æ–‡ï¼Œ`unstructured_inference`ä¼¼ä¹ä¸æ˜¯ä¸€ä¸ªå¹¿æ³›è®¤çŸ¥çš„PythonåŒ…ï¼Œè¿™å¯èƒ½æ„å‘³ç€å®ƒæ˜¯ä¸€ä¸ªç‰¹å®šé¡¹ç›®çš„ä¸€éƒ¨åˆ†ï¼Œæˆ–è€…æ˜¯ä¸€ä¸ªè¾ƒæ–°çš„ã€å°šæœªå¹¿æ³›é‡‡ç”¨çš„åº“ã€‚<br>

é¦–å…ˆï¼Œå°è¯•ä½¿ç”¨pipç›´æ¥å®‰è£…`unstructured_inference`ï¼š

```sh
pip install unstructured_inference
```

å¦‚æœè¿™ä¸ªå‘½ä»¤å¤±è´¥ï¼Œè¯´æ˜è¿™ä¸ªåŒ…å¯èƒ½ä¸åœ¨PyPIä¸Šï¼Œæˆ–è€…æœ‰ä¸€ä¸ªä¸åŒçš„å®‰è£…æ–¹æ³•ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ éœ€è¦æŸ¥æ‰¾`unstructured_inference`æ¨¡å—çš„æ¥æºã€‚è¿™å¯èƒ½æ¶‰åŠåˆ°ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š

1. **æ£€æŸ¥`langchain`æ–‡æ¡£æˆ–GitHubä»“åº“**ï¼šæ—¢ç„¶é”™è¯¯å‘ç”Ÿåœ¨ä½¿ç”¨`langchain`æ—¶ï¼Œ`unstructured_inference`å¯èƒ½æ˜¯`langchain`æˆ–å…¶ä¾èµ–ä¹‹ä¸€çš„ä¸€éƒ¨åˆ†ã€‚æ£€æŸ¥`langchain`çš„å®˜æ–¹æ–‡æ¡£æˆ–GitHubä»“åº“ï¼Œçœ‹æ˜¯å¦æœ‰å…³äºå¦‚ä½•å®‰è£…æˆ–è§£å†³ä¾èµ–é—®é¢˜çš„æŒ‡å¯¼ã€‚

2. **æœç´¢`unstructured_inference`**ï¼šå°è¯•åœ¨ç½‘ä¸Šæœç´¢`unstructured_inference`ï¼Œçœ‹æ˜¯å¦æœ‰å…¶ä»–äººé‡åˆ°è¿‡ç±»ä¼¼çš„é—®é¢˜æˆ–è€…æ˜¯å¦æœ‰ä¸“é—¨çš„å®‰è£…æŒ‡å—ã€‚

3. **ç¯å¢ƒé—®é¢˜**ï¼šç¡®è®¤ä½ æ­£åœ¨ä½¿ç”¨çš„Pythonç¯å¢ƒæ˜¯æ­£ç¡®çš„ã€‚æœ‰æ—¶å€™ï¼Œå¦‚æœä½ åœ¨è™šæ‹Ÿç¯å¢ƒä¸­å·¥ä½œï¼Œå¯èƒ½ä¼šå› ä¸ºç¯å¢ƒé…ç½®ä¸å½“è€Œå¯¼è‡´æ¨¡å—æ‰¾ä¸åˆ°ã€‚

å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½æ— æ³•è§£å†³é—®é¢˜ï¼Œä½ å¯èƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥`unstructured_inference`æ¨¡å—çš„ç¡®åˆ‡æ¥æºå’Œå®‰è£…æ–¹æ³•ã€‚è¿™å¯èƒ½æ¶‰åŠåˆ°é˜…è¯»æ›´å¤šçš„æ–‡æ¡£ï¼Œæˆ–è€…åœ¨ç›¸å…³çš„å¼€å‘è€…ç¤¾åŒºä¸­å¯»æ±‚å¸®åŠ©ã€‚